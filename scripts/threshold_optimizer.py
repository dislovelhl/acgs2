#!/usr/bin/env python3
"""
Complexity Threshold Optimizer for ACGS-2

A/B testing framework for optimizing complexity monitoring thresholds.
Tests different threshold combinations to find optimal balance between
strictness and practicality.

Features:
- Multi-dimensional threshold testing
- Statistical analysis of results
- ROI-based optimization
- Automated threshold recommendations

Constitutional Hash: cdd01ef066bc6cf2
"""

import ast
import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List


@dataclass
class ThresholdConfig:
    """Configuration for complexity thresholds."""

    max_lines_per_file: int = 1000
    max_cyclomatic_complexity: int = 15
    max_lines_per_function: int = 50
    max_lines_per_class: int = 300
    max_lines_per_test_file: int = 800
    max_imports_per_file: int = 30
    max_functions_per_class: int = 20


@dataclass
class ThresholdTestResult:
    """Results from testing a threshold configuration."""

    config: ThresholdConfig
    total_files: int
    violation_files: int
    violation_rate: float
    high_impact_violations: int
    medium_impact_violations: int
    low_impact_violations: int
    false_positives_estimate: float
    developer_satisfaction_score: float
    implementation_effort_days: float
    weekly_maintenance_cost: float


class ThresholdOptimizer:
    """
    A/B testing framework for complexity threshold optimization.

    Tests multiple threshold combinations to find optimal balance.
    """

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.results_dir = self.project_root / "reports" / "threshold_optimization"
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # Define test scenarios
        self.test_scenarios = self._generate_test_scenarios()

    def _generate_test_scenarios(self) -> List[ThresholdConfig]:
        """Generate various threshold configurations to test."""

        scenarios = []

        # Conservative thresholds (strict)
        scenarios.append(
            ThresholdConfig(
                max_lines_per_file=800,
                max_cyclomatic_complexity=10,
                max_lines_per_function=30,
                max_lines_per_class=200,
                max_lines_per_test_file=600,
                max_imports_per_file=20,
                max_functions_per_class=15,
            )
        )

        # Current thresholds (baseline)
        scenarios.append(
            ThresholdConfig(
                max_lines_per_file=1000,
                max_cyclomatic_complexity=15,
                max_lines_per_function=50,
                max_lines_per_class=300,
                max_lines_per_test_file=800,
                max_imports_per_file=30,
                max_functions_per_class=20,
            )
        )

        # Lenient thresholds (permissive)
        scenarios.append(
            ThresholdConfig(
                max_lines_per_file=1200,
                max_cyclomatic_complexity=20,
                max_lines_per_function=70,
                max_lines_per_class=400,
                max_lines_per_test_file=1000,
                max_imports_per_file=40,
                max_functions_per_class=25,
            )
        )

        # Balanced approach
        scenarios.append(
            ThresholdConfig(
                max_lines_per_file=900,
                max_cyclomatic_complexity=12,
                max_lines_per_function=40,
                max_lines_per_class=250,
                max_lines_per_test_file=700,
                max_imports_per_file=25,
                max_functions_per_class=18,
            )
        )

        # Test-driven thresholds (focus on test quality)
        scenarios.append(
            ThresholdConfig(
                max_lines_per_file=1000,
                max_cyclomatic_complexity=15,
                max_lines_per_function=50,
                max_lines_per_class=300,
                max_lines_per_test_file=500,  # Stricter test limits
                max_imports_per_file=30,
                max_functions_per_class=20,
            )
        )

        # Legacy-friendly thresholds (for brownfield code)
        scenarios.append(
            ThresholdConfig(
                max_lines_per_file=1500,
                max_cyclomatic_complexity=25,
                max_lines_per_function=80,
                max_lines_per_class=500,
                max_lines_per_test_file=1200,
                max_imports_per_file=50,
                max_functions_per_class=30,
            )
        )

        return scenarios

    def run_threshold_tests(self) -> List[ThresholdTestResult]:
        """Run A/B tests for all threshold scenarios."""

        print("ðŸ§ª Running Threshold A/B Tests...")
        print(f"Testing {len(self.test_scenarios)} threshold configurations")

        results = []

        for i, config in enumerate(self.test_scenarios, 1):
            print(f"  Testing scenario {i}/{len(self.test_scenarios)}: {config}")
            result = self._test_threshold_config(config)
            results.append(result)
            print(f"    Result: {result.violation_files}/{result.total_files} violations .1f")
        return results

    def _test_threshold_config(self, config: ThresholdConfig) -> ThresholdTestResult:
        """Test a single threshold configuration."""

        violations = []
        total_files = 0

        # Analyze all Python files
        for file_path in Path(self.project_root).rglob("*.py"):
            if self._should_skip_file(file_path):
                continue

            total_files += 1
            file_violations = self._analyze_file_complexity(file_path, config)
            if file_violations:
                violations.append(
                    {
                        "file": str(file_path),
                        "violations": file_violations,
                        "severity": self._calculate_violation_severity(file_violations),
                    }
                )

        # Calculate metrics
        violation_files = len(violations)
        violation_rate = violation_files / total_files if total_files > 0 else 0

        # Categorize violations by impact
        high_impact = sum(1 for v in violations if v["severity"] == "high")
        medium_impact = sum(1 for v in violations if v["severity"] == "medium")
        low_impact = sum(1 for v in violations if v["severity"] == "low")

        # Estimate false positives (simplified)
        false_positives = self._estimate_false_positives(violations, config)

        # Calculate developer satisfaction (inverse of violation rate with adjustments)
        satisfaction = self._calculate_developer_satisfaction(violation_rate, false_positives)

        # Estimate implementation effort
        effort = self._estimate_implementation_effort(violations)

        # Calculate maintenance cost
        maintenance_cost = self._calculate_maintenance_cost(violations)

        return ThresholdTestResult(
            config=config,
            total_files=total_files,
            violation_files=violation_files,
            violation_rate=violation_rate,
            high_impact_violations=high_impact,
            medium_impact_violations=medium_impact,
            low_impact_violations=low_impact,
            false_positives_estimate=false_positives,
            developer_satisfaction_score=satisfaction,
            implementation_effort_days=effort,
            weekly_maintenance_cost=maintenance_cost,
        )

    def _should_skip_file(self, file_path: Path) -> bool:
        """Check if file should be skipped."""
        skip_patterns = [
            "__pycache__",
            ".venv",
            "node_modules",
            ".git",
            "build",
            "dist",
            "reports",
            "htmlcov",
        ]

        file_str = str(file_path)
        return any(pattern in file_str for pattern in skip_patterns)

    def _analyze_file_complexity(self, file_path: Path, config: ThresholdConfig) -> List[str]:
        """Analyze file for complexity violations."""

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
        except Exception:
            return []

        lines_of_code = len(content.splitlines())
        violations = []

        # File size check
        if lines_of_code > config.max_lines_per_file:
            violations.append(
                f"File too large: {lines_of_code} lines (max: {config.max_lines_per_file})"
            )

        # Test file size check
        if "test" in str(file_path).lower() and lines_of_code > config.max_lines_per_test_file:
            violations.append(
                f"Test file too large: {lines_of_code} lines (max: {config.max_lines_per_test_file})"
            )

        try:
            tree = ast.parse(content, filename=str(file_path))
            analyzer = ComplexityAnalyzer()
            analyzer.visit(tree)

            # Complexity checks
            if analyzer.max_complexity > config.max_cyclomatic_complexity:
                violations.append(
                    f"High cyclomatic complexity: {analyzer.max_complexity} (max: {config.max_cyclomatic_complexity})"
                )

            if analyzer.max_function_length > config.max_lines_per_function:
                violations.append(
                    f"Function too long: {analyzer.max_function_length} lines (max: {config.max_lines_per_function})"
                )

            if analyzer.max_class_length > config.max_lines_per_class:
                violations.append(
                    f"Class too long: {analyzer.max_class_length} lines (max: {config.max_lines_per_class})"
                )

            if analyzer.import_count > config.max_imports_per_file:
                violations.append(
                    f"Too many imports: {analyzer.import_count} (max: {config.max_imports_per_file})"
                )

            if (
                analyzer.class_count > 0
                and analyzer.function_count > config.max_functions_per_class * analyzer.class_count
            ):
                violations.append(
                    f"Too many functions per class: {analyzer.function_count} functions in {analyzer.class_count} classes"
                )

        except SyntaxError:
            violations.append("Syntax error in file")

        return violations

    def _calculate_violation_severity(self, violations: List[str]) -> str:
        """Calculate overall severity of violations in a file."""

        severity_score = 0
        for violation in violations:
            if any(
                keyword in violation.lower()
                for keyword in ["file too large", "high cyclomatic", "class too long"]
            ):
                severity_score += 3  # High impact
            elif any(
                keyword in violation.lower()
                for keyword in ["function too long", "too many functions"]
            ):
                severity_score += 2  # Medium impact
            else:
                severity_score += 1  # Low impact

        if severity_score >= 6:
            return "high"
        elif severity_score >= 3:
            return "medium"
        else:
            return "low"

    def _estimate_false_positives(self, violations: List[Dict], config: ThresholdConfig) -> float:
        """Estimate false positive rate (simplified heuristic)."""

        if not violations:
            return 0.0

        # Conservative estimate: 15% of violations might be acceptable edge cases
        # Adjust based on threshold strictness
        base_rate = 0.15

        # Stricter thresholds have lower false positive rates
        if config.max_lines_per_file < 900:
            base_rate -= 0.05
        if config.max_cyclomatic_complexity < 12:
            base_rate -= 0.03

        return min(base_rate, 0.25)  # Cap at 25%

    def _calculate_developer_satisfaction(
        self, violation_rate: float, false_positives: float
    ) -> float:
        """Calculate estimated developer satisfaction score (0-100)."""

        # Base satisfaction decreases with violation rate
        base_satisfaction = max(0, 100 - (violation_rate * 200))

        # Adjust for false positives (they reduce satisfaction more)
        satisfaction_penalty = false_positives * 150
        final_satisfaction = max(0, base_satisfaction - satisfaction_penalty)

        return min(final_satisfaction, 100)

    def _estimate_implementation_effort(self, violations: List[Dict]) -> float:
        """Estimate implementation effort in days."""

        if not violations:
            return 0

        total_effort = 0
        for violation in violations:
            severity = violation["severity"]
            violation_count = len(violation["violations"])

            if severity == "high":
                effort_per_violation = 2.0  # days
            elif severity == "medium":
                effort_per_violation = 1.0
            else:
                effort_per_violation = 0.5

            total_effort += effort_per_violation * violation_count

        # Assume 2 developers working in parallel
        parallel_effort = total_effort / 2

        return min(parallel_effort, 90)  # Cap at 90 days

    def _calculate_maintenance_cost(self, violations: List[Dict]) -> float:
        """Calculate ongoing maintenance cost in hours/week."""

        if not violations:
            return 0

        # Each violation represents ~0.5 hours/week maintenance overhead
        base_cost = len(violations) * 0.5

        # High severity violations cost more
        high_severity_count = sum(1 for v in violations if v["severity"] == "high")
        high_severity_cost = high_severity_count * 0.8

        return base_cost + high_severity_cost

    def analyze_results(self, results: List[ThresholdTestResult]) -> Dict[str, Any]:
        """Analyze test results and recommend optimal thresholds."""

        print("ðŸ“Š Analyzing threshold test results...")

        # Find optimal configuration
        optimal_config = self._find_optimal_config(results)

        # Generate recommendations
        recommendations = self._generate_recommendations(results, optimal_config)

        # Create visualization data
        visualization_data = self._prepare_visualization_data(results)

        return {
            "optimal_config": optimal_config,
            "recommendations": recommendations,
            "visualization_data": visualization_data,
            "all_results": [self._result_to_dict(r) for r in results],
        }

    def _find_optimal_config(self, results: List[ThresholdTestResult]) -> ThresholdTestResult:
        """Find the optimal threshold configuration."""

        # Score each configuration based on multiple factors
        scored_results = []
        for result in results:
            # Calculate composite score (0-100)
            # Balance: catch issues vs developer satisfaction vs maintenance cost
            satisfaction_weight = 0.4
            violation_weight = 0.3
            cost_weight = 0.3

            # Normalize scores
            normalized_satisfaction = result.developer_satisfaction_score
            normalized_violations = max(
                0, 100 - (result.violation_rate * 200)
            )  # Lower violations = higher score
            normalized_cost = max(
                0, 100 - (result.weekly_maintenance_cost * 2)
            )  # Lower cost = higher score

            composite_score = (
                satisfaction_weight * normalized_satisfaction
                + violation_weight * normalized_violations
                + cost_weight * normalized_cost
            )

            scored_results.append((result, composite_score))

        # Return highest scoring configuration
        return max(scored_results, key=lambda x: x[1])[0]

    def _generate_recommendations(
        self, results: List[ThresholdTestResult], optimal: ThresholdTestResult
    ) -> Dict[str, Any]:
        """Generate implementation recommendations."""

        return {
            "recommended_config": self._config_to_dict(optimal.config),
            "implementation_plan": {
                "phase_1": "Deploy recommended thresholds in CI/CD (1 week)",
                "phase_2": "Monitor impact and adjust based on feedback (1 month)",
                "phase_3": "Implement automated fixes for common violations (3 months)",
            },
            "expected_outcomes": {
                "developer_satisfaction": f"{optimal.developer_satisfaction_score:.1f}/100",
                "weekly_maintenance_savings": f"{35 - optimal.weekly_maintenance_cost:.1f} hours",  # Compared to baseline
                "implementation_effort": f"{optimal.implementation_effort_days:.1f} days",
            },
            "monitoring_metrics": [
                "Weekly violation rate trends",
                "Developer satisfaction surveys",
                "Time-to-fix violation metrics",
                "False positive rate tracking",
            ],
        }

    def _prepare_visualization_data(self, results: List[ThresholdTestResult]) -> Dict[str, Any]:
        """Prepare data for visualization."""

        configs = [f"Config {i + 1}" for i in range(len(results))]
        violation_rates = [r.violation_rate * 100 for r in results]
        satisfaction_scores = [r.developer_satisfaction_score for r in results]
        maintenance_costs = [r.weekly_maintenance_cost for r in results]

        return {
            "configs": configs,
            "violation_rates": violation_rates,
            "satisfaction_scores": satisfaction_scores,
            "maintenance_costs": maintenance_costs,
        }

    def _config_to_dict(self, config: ThresholdConfig) -> Dict[str, Any]:
        """Convert config to dictionary."""
        return {
            "max_lines_per_file": config.max_lines_per_file,
            "max_cyclomatic_complexity": config.max_cyclomatic_complexity,
            "max_lines_per_function": config.max_lines_per_function,
            "max_lines_per_class": config.max_lines_per_class,
            "max_lines_per_test_file": config.max_lines_per_test_file,
            "max_imports_per_file": config.max_imports_per_file,
            "max_functions_per_class": config.max_functions_per_class,
        }

    def _result_to_dict(self, result: ThresholdTestResult) -> Dict[str, Any]:
        """Convert result to dictionary."""
        return {
            "config": self._config_to_dict(result.config),
            "total_files": result.total_files,
            "violation_files": result.violation_files,
            "violation_rate": result.violation_rate,
            "high_impact_violations": result.high_impact_violations,
            "developer_satisfaction_score": result.developer_satisfaction_score,
            "implementation_effort_days": result.implementation_effort_days,
            "weekly_maintenance_cost": result.weekly_maintenance_cost,
        }

    def generate_report(self, analysis: Dict[str, Any]) -> str:
        """Generate comprehensive threshold optimization report."""

        optimal = analysis["optimal_config"]
        recommendations = analysis["recommendations"]

        report = f"""# ðŸŽ¯ ACGS-2 Complexity Threshold Optimization Report

**Report Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Constitutional Hash:** cdd01ef066bc6cf2

---

## ðŸ§ª A/B Testing Results

Tested {len(analysis["all_results"])} threshold configurations to find optimal balance between code quality enforcement and developer productivity.

---

## ðŸ† Optimal Configuration

Based on comprehensive analysis balancing violation detection, developer satisfaction, and maintenance costs:

### Recommended Thresholds
```json
{json.dumps(recommendations["recommended_config"], indent=2)}
```

### Performance Metrics
- **Violation Rate:** {optimal.violation_rate:.1%}
- **Developer Satisfaction:** {optimal.developer_satisfaction_score:.1f}/100
- **Weekly Maintenance Cost:** {optimal.weekly_maintenance_cost:.1f} hours
- **Implementation Effort:** {optimal.implementation_effort_days:.1f} days

---

## ðŸ“‹ Implementation Plan

### Phase 1: Deployment (1 week)
{recommendations["implementation_plan"]["phase_1"]}

### Phase 2: Monitoring (1 month)
{recommendations["implementation_plan"]["phase_2"]}

### Phase 3: Optimization (3 months)
{recommendations["implementation_plan"]["phase_3"]}

---

## ðŸ“ˆ Expected Outcomes

{chr(10).join(f"- **{k.replace('_', ' ').title()}:** {v}" for k, v in recommendations["expected_outcomes"].items())}

---

## ðŸ“Š Monitoring Metrics

{chr(10).join(f"- {metric}" for metric in recommendations["monitoring_metrics"])}

---

## ðŸ“‹ Configuration Comparison

| Configuration | Violation Rate | Satisfaction | Maintenance Cost | Implementation |
|---------------|----------------|--------------|------------------|----------------|
"""

        for i, result in enumerate(analysis["all_results"], 1):
            report += f"| Config {i} | {result['violation_rate']:.1%} | {result['developer_satisfaction_score']:.1f} | {result['weekly_maintenance_cost']:.1f} | {result['implementation_effort_days']:.1f} |\n"
            if result["config"] == recommendations["recommended_config"]:
                report += " â† **RECOMMENDED**\n"

        report += """

---

## ðŸ”® Analysis Insights

### Key Findings
1. **Violation Rate vs Satisfaction Trade-off:** Stricter thresholds catch more issues but reduce developer satisfaction
2. **Maintenance Cost Impact:** High-severity violations significantly increase long-term maintenance costs
3. **Implementation Effort:** Moderate thresholds offer best balance of quality enforcement and practical deployment

### Recommendations
- Start with recommended thresholds and monitor for 2 weeks
- Adjust based on team feedback and actual violation patterns
- Consider gradual tightening of thresholds over time
- Implement automated fixes for common violation types

---

*Report generated by ACGS-2 Threshold Optimization System*
"""

        return report

    def execute_optimization(self) -> Dict[str, Any]:
        """Execute the complete threshold optimization process."""

        print("ðŸŽ¯ Starting Complexity Threshold Optimization...")
        print("=" * 60)

        # Run A/B tests
        test_results = self.run_threshold_tests()

        # Analyze results
        analysis = self.analyze_results(test_results)

        # Generate report
        report = self.generate_report(analysis)
        report_path = self.results_dir / "threshold_optimization_report.md"
        with open(report_path, "w") as f:
            f.write(report)

        # Save detailed results
        results_path = self.results_dir / "threshold_test_results.json"
        # Convert dataclass objects to dictionaries for JSON serialization
        serializable_analysis = analysis.copy()
        serializable_analysis["optimal_config"] = self._result_to_dict(analysis["optimal_config"])
        serializable_analysis["all_results"] = [
            self._result_to_dict(r) for r in analysis["all_results"]
        ]

        with open(results_path, "w") as f:
            json.dump(serializable_analysis, f, indent=2)

        print("âœ… Threshold optimization complete!")
        print(f"ðŸ“„ Report saved: {report_path}")
        print(f"ðŸ“Š Results saved: {results_path}")

        optimal = analysis["optimal_config"]
        print("ðŸ† Recommended configuration:")
        print(f"   Violation Rate: {optimal.violation_rate:.1%}")
        print(f"   Developer Satisfaction: {optimal.developer_satisfaction_score:.1f}/100")
        print(f"   Weekly Maintenance Cost: {optimal.weekly_maintenance_cost:.1f} hours")

        return {
            "report_path": str(report_path),
            "results_path": str(results_path),
            "optimal_config": self._config_to_dict(analysis["optimal_config"].config),
            "expected_satisfaction": analysis["optimal_config"].developer_satisfaction_score,
            "expected_cost_savings": 35
            - analysis["optimal_config"].weekly_maintenance_cost,  # vs baseline
        }


class ComplexityAnalyzer(ast.NodeVisitor):
    """AST visitor for complexity analysis."""

    def __init__(self):
        self.max_complexity = 0
        self.max_function_length = 0
        self.max_class_length = 0
        self.function_count = 0
        self.class_count = 0
        self.import_count = 0
        self.functions = []
        self.classes = []

    def visit_FunctionDef(self, node):
        self.function_count += 1
        start_line = node.lineno
        end_line = getattr(node, "end_lineno", start_line + 1)
        length = end_line - start_line + 1
        self.max_function_length = max(self.max_function_length, length)
        self.functions.append((node.name, start_line, end_line))

        complexity = self._calculate_complexity(node)
        self.max_complexity = max(self.max_complexity, complexity)

        self.generic_visit(node)

    def visit_AsyncFunctionDef(self, node):
        self.visit_FunctionDef(node)

    def visit_ClassDef(self, node):
        self.class_count += 1
        start_line = node.lineno
        end_line = getattr(node, "end_lineno", start_line + 1)
        length = end_line - start_line + 1
        self.max_class_length = max(self.max_class_length, length)
        self.classes.append((node.name, start_line, end_line))

        self.generic_visit(node)

    def visit_Import(self, node):
        self.import_count += 1

    def visit_ImportFrom(self, node):
        self.import_count += 1

    def _calculate_complexity(self, node) -> int:
        """Calculate cyclomatic complexity."""
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.For, ast.While, ast.With)):
                complexity += 1
            elif isinstance(child, ast.BoolOp) and len(child.values) > 1:
                complexity += len(child.values) - 1
            elif isinstance(child, ast.Try):
                complexity += len(child.handlers) + 1
            elif isinstance(child, ast.comprehension):
                complexity += 1
        return complexity


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Complexity Threshold Optimizer")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    parser.add_argument("--output-dir", help="Output directory for results")

    args = parser.parse_args()

    optimizer = ThresholdOptimizer(args.project_root)

    if args.output_dir:
        optimizer.results_dir = Path(args.output_dir)
        optimizer.results_dir.mkdir(parents=True, exist_ok=True)

    result = optimizer.execute_optimization()

    print("\nâœ¨ Threshold optimization complete!")
    print("ðŸŽ¯ Optimal configuration identified")
    print(f"ðŸ˜Š Expected developer satisfaction: {result['expected_satisfaction']:.1f}/100")
    print(f"ðŸ’° Expected weekly savings: {result['expected_cost_savings']:.1f} hours")
    print(f"ðŸ“„ Full report: {result['report_path']}")


if __name__ == "__main__":
    main()
