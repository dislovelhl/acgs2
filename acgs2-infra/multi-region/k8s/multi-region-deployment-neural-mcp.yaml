apiVersion: apps/v1
kind: Deployment
metadata:
  name: acgs2-neural-mcp
  namespace: default
  labels:
    app: acgs2-neural-mcp
    acgs2/component: neural
    acgs2/multi-region: enabled
    acgs2/service-type: mcp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: acgs2-neural-mcp
  template:
    metadata:
      labels:
        app: acgs2-neural-mcp
        acgs2/component: neural
        acgs2/multi-region: enabled
        acgs2/service-type: mcp
        # Region-specific labels for locality-aware routing
        topology.kubernetes.io/region: "{{ .Values.region }}"
        topology.istio.io/network: "network{{ .Values.networkId }}"
      annotations:
        # Enable Istio injection
        sidecar.istio.io/inject: "true"
        # Prometheus scraping
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
        # MCP-specific annotations
        acgs2/mcp-version: "1.0"
        acgs2/neural-model: "{{ .Values.neuralModel | default "default" }}"
    spec:
      # Node affinity for regional deployment
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - "{{ .Values.region }}"
              # Prefer nodes with GPU for neural processing
              - key: accelerator
                operator: In
                values:
                - nvidia-tesla-t4
                - nvidia-tesla-v100
                - nvidia-a100
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: acgs2-neural-mcp
              topologyKey: kubernetes.io/hostname

      # Topology spread constraints for high availability
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: acgs2-neural-mcp
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: acgs2-neural-mcp

      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

      # Service account for multi-region access
      serviceAccountName: neural-mcp-service-account

      containers:
      - name: neural-mcp
        image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command:
        - /usr/local/bin/neural-mcp
        - --config=/etc/neural-mcp/config.yaml
        - --port=3000

        # Environment variables
        env:
        - name: REGION
          value: "{{ .Values.region }}"
        - name: TENANT_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['acgs2/tenant-id']
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: neural-mcp-secrets
              key: redis-url
        - name: KAFKA_BOOTSTRAP_SERVERS
          valueFrom:
            configMapKeyRef:
              name: neural-mcp-config
              key: kafka-bootstrap
        - name: AGENT_BUS_URL
          value: "http://enhanced-agent-bus.default.svc.cluster.local:8000"
        - name: OPA_URL
          value: "http://opa.default.svc.cluster.local:8181"
        - name: AUDIT_SERVICE_URL
          value: "http://audit-service.default.svc.cluster.local:8080"
        - name: PROMETHEUS_METRICS_PORT
          value: "9090"
        - name: LOG_LEVEL
          value: "{{ .Values.logLevel | default "info" }}"
        # Neural-specific environment
        - name: NEURAL_MODEL_PATH
          value: "/models/{{ .Values.neuralModel | default "default" }}"
        - name: GPU_MEMORY_FRACTION
          value: "{{ .Values.gpuMemoryFraction | default "0.8" }}"
        - name: INFERENCE_BATCH_SIZE
          value: "{{ .Values.inferenceBatchSize | default "1" }}"
        # Multi-region specific environment
        - name: MULTI_REGION_ENABLED
          value: "true"
        - name: PRIMARY_REGION
          value: "{{ .Values.primaryRegion | default "us-east-1" }}"
        - name: CROSS_REGION_TIMEOUT
          value: "{{ .Values.crossRegionTimeout | default "30s" }}"
        - name: HEALTH_CHECK_INTERVAL
          value: "{{ .Values.healthCheckInterval | default "30s" }}"
        # Model serving environment
        - name: MODEL_CACHE_SIZE
          value: "{{ .Values.modelCacheSize | default "2GB" }}"
        - name: ENABLE_MODEL_WARMUP
          value: "{{ .Values.enableModelWarmup | default "true" }}"

        ports:
        - name: mcp
          containerPort: 3000
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP
        - name: health
          containerPort: 8080
          protocol: TCP

        # Resource limits (GPU-aware)
        resources:
          limits:
            cpu: "{{ .Values.resources.limits.cpu | default "2000m" }}"
            memory: "{{ .Values.resources.limits.memory | default "4Gi" }}"
            {{- if .Values.gpu.enabled }}
            nvidia.com/gpu: "{{ .Values.gpu.count | default 1 }}"
            {{- end }}
          requests:
            cpu: "{{ .Values.resources.requests.cpu | default "500m" }}"
            memory: "{{ .Values.resources.requests.memory | default "1Gi" }}"
            {{- if .Values.gpu.enabled }}
            nvidia.com/gpu: "{{ .Values.gpu.count | default 1 }}"
            {{- end }}

        # Health checks
        livenessProbe:
          httpGet:
            path: /health/live
            port: health
          initialDelaySeconds: 60  # Longer delay for model loading
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: health
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        # Volume mounts
        volumeMounts:
        - name: config-volume
          mountPath: /etc/neural-mcp
          readOnly: true
        - name: model-volume
          mountPath: /models
          readOnly: true
        - name: tmp-volume
          mountPath: /tmp
        - name: logs-volume
          mountPath: /var/log/neural-mcp
        {{- if .Values.gpu.enabled }}
        - name: gpu-devices
          mountPath: /dev/nvidia0
        {{- end }}

        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1000
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
          {{- if .Values.gpu.enabled }}
          privileged: true  # Required for GPU access
          {{- end }}

      # Init containers for model loading and dependencies
      initContainers:
      - name: model-preloader
        image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        command:
        - sh
        - -c
        - |
          # Download/preload neural models
          echo "Preloading neural models..."

          # Check model availability
          if [ -f "/models/{{ .Values.neuralModel | default "default" }}/config.json" ]; then
            echo "Model {{ .Values.neuralModel | default "default" }} is available"
          else
            echo "Model not found, downloading..."
            # Model download logic would go here
            echo "Model download not implemented in this template"
          fi

          echo "Model preload complete"

        volumeMounts:
        - name: model-volume
          mountPath: /models
          readOnly: true

        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 256Mi

      - name: wait-for-dependencies
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          # Wait for Redis
          until nc -z redis.{{ .Values.region }}.svc.cluster.local 6379; do
            echo "Waiting for Redis..."
            sleep 5
          done

          # Wait for Kafka
          until nc -z kafka.{{ .Values.region }}.svc.cluster.local 9092; do
            echo "Waiting for Kafka..."
            sleep 5
          done

          # Wait for Agent Bus
          until nc -z enhanced-agent-bus.default.svc.cluster.local 8000; do
            echo "Waiting for Agent Bus..."
            sleep 5
          done

          echo "All dependencies ready"

        resources:
          limits:
            cpu: 50m
            memory: 32Mi
          requests:
            cpu: 10m
            memory: 8Mi

      # Volumes
      volumes:
      - name: config-volume
        configMap:
          name: neural-mcp-config
      - name: model-volume
        persistentVolumeClaim:
          claimName: neural-models-pvc
      - name: tmp-volume
        emptyDir: {}
      - name: logs-volume
        emptyDir: {}
      {{- if .Values.gpu.enabled }}
      - name: gpu-devices
        hostPath:
          path: /dev/nvidia0
      {{- end }}

  # Deployment strategy
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

  # Pod disruption budget
  # Note: PDB is defined separately due to Helm limitations with nested resources
---
# Pod Disruption Budget for neural-mcp
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: neural-mcp-pdb
  namespace: default
  labels:
    app: acgs2-neural-mcp
    acgs2/component: neural
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: acgs2-neural-mcp
---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: neural-mcp-hpa
  namespace: default
  labels:
    app: acgs2-neural-mcp
    acgs2/component: neural
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: acgs2-neural-mcp
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: neural_inference_queue_length
      target:
        type: AverageValue
        averageValue: 10
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # Longer stabilization for neural models
      policies:
      - type: Percent
        value: 10
        periodSeconds: 120
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
---
# Persistent Volume Claim for neural models
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neural-models-pvc
  namespace: default
  labels:
    app: acgs2-neural-mcp
    acgs2/component: neural
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: "{{ .Values.storageClass | default "gp3" }}"
  resources:
    requests:
      storage: "{{ .Values.modelStorageSize | default "50Gi" }}"
---
# Network Policy for multi-region security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: neural-mcp-network-policy
  namespace: default
  labels:
    app: acgs2-neural-mcp
    acgs2/component: neural
spec:
  podSelector:
    matchLabels:
      app: acgs2-neural-mcp
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow traffic from Istio sidecar
  - from:
    - podSelector:
        matchLabels:
          security.istio.io/tlsMode: istio
    ports:
    - protocol: TCP
      port: 3000
  # Allow health checks from Kubernetes
  - from: []
    ports:
    - protocol: TCP
      port: 8080
  egress:
  # Allow DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # Allow traffic to Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  # Allow traffic to Kafka
  - to:
    - podSelector:
        matchLabels:
          app: kafka
    ports:
    - protocol: TCP
      port: 9092
  # Allow traffic to Agent Bus
  - to:
    - podSelector:
        matchLabels:
          app: enhanced-agent-bus
    ports:
    - protocol: TCP
      port: 8000
  # Allow traffic to OPA
  - to:
    - podSelector:
        matchLabels:
          app: opa
    ports:
    - protocol: TCP
      port: 8181
  # Allow traffic to audit service
  - to:
    - podSelector:
        matchLabels:
          app: audit-service
    ports:
    - protocol: TCP
      port: 8080
  # Allow model registry access (if external)
  {{- if .Values.externalModelRegistry }}
  - to: []
    ports:
    - protocol: HTTPS
      port: 443
  {{- end }}
