{
  "file_path": "acgs2-core/breakthrough/integrations/runtime_guardrails.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nConstitutional Runtime Safety Guardrails\n==========================================\n\nConstitutional Hash: cdd01ef066bc6cf2\n\nImplements comprehensive guardrails layer that enforces\nconstitutional principles at runtime with minimal latency:\n- Input sanitization\n- Policy enforcement (pre-execution)\n- Sandbox execution\n- Output verification\n- Audit logging\n\nReferences:\n- Superagent Framework Guardrails\n- OWASP GenAI Security Project\n\"\"\"\n\nimport asyncio\nimport logging\nimport re\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Callable, Dict, List, Optional, Union\n\nfrom ...shared.types import (\n    AuditTrail,\n    ContextData,\n    JSONDict,\n    JSONValue,\n    MetadataDict,\n)\nfrom .. import CONSTITUTIONAL_HASH\n\nlogger = logging.getLogger(__name__)\n\n\nclass GuardrailLevel(Enum):\n    \"\"\"Levels of guardrail enforcement.\"\"\"\n    STRICT = \"strict\"      # Block all violations\n    MODERATE = \"moderate\"  # Block high-risk, warn moderate\n    PERMISSIVE = \"permissive\"  # Warn only, allow most\n\n\nclass EscalationAction(Enum):\n    \"\"\"Actions for escalation handling.\"\"\"\n    BLOCK = \"block\"\n    HUMAN_REVIEW = \"human_review\"\n    ENHANCED_LOGGING = \"enhanced_logging\"\n    NOTIFY = \"notify\"\n\n\n@dataclass\nclass SanitizationResult:\n    \"\"\"Result from input sanitization.\"\"\"\n    sanitized: JSONValue\n    modifications_made: List[str]\n    risks_detected: List[str]\n    blocked: bool = False\n\n\n@dataclass\nclass PolicyResult:\n    \"\"\"Result from policy evaluation.\"\"\"\n    allowed: bool\n    policy_id: str\n    reasons: List[str]\n    requires_escalation: bool = False\n    escalation_reason: Optional[str] = None\n\n\n@dataclass\nclass SandboxResult:\n    \"\"\"Result from sandboxed execution.\"\"\"\n    output: Optional[JSONValue]\n    execution_time_ms: float\n    resource_usage: Dict[str, float]\n    errors: List[str]\n    truncated: bool = False\n\n\n@dataclass\nclass VerificationResult:\n    \"\"\"Result from output verification.\"\"\"\n    verified: bool\n    modifications_made: List[str]\n    warnings: List[str]\n\n\n@dataclass\nclass GuardrailResult:\n    \"\"\"Complete result from guardrail pipeline.\"\"\"\n    result_id: str\n    action: Union[Callable, JSONValue]\n    result: Optional[JSONValue]\n    sanitization: SanitizationResult\n    policy_result: PolicyResult\n    sandbox_result: Optional[SandboxResult]\n    verification: VerificationResult\n    audit_id: str\n    processing_time_ms: float\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n\n    def to_dict(self) -> JSONDict:\n        return {\n            \"result_id\": self.result_id,\n            \"constitutional_hash\": self.constitutional_hash,\n            \"processing_time_ms\": self.processing_time_ms,\n            \"audit_id\": self.audit_id,\n            \"blocked\": self.sanitization.blocked or not self.policy_result.allowed,\n        }\n\n\nclass InputSanitizer:\n    \"\"\"\n    Sanitizes input to prevent injection attacks.\n\n    Removes or escapes potentially dangerous patterns.\n    \"\"\"\n\n    # Patterns to remove or escape\n    DANGEROUS_PATTERNS = [\n        r\"<script.*?>.*?</script>\",\n        r\"javascript:\",\n        r\"data:\",\n        r\"on\\w+\\s*=\",\n        r\"\\{\\{.*?\\}\\}\",\n        r\"\\$\\{.*?\\}\",\n    ]\n\n    INJECTION_PATTERNS = [\n        r\";\\s*DROP\\s+TABLE\",\n        r\";\\s*DELETE\\s+FROM\",\n        r\"--\\s*$\",\n        r\"\\/\\*.*?\\*\\/\",\n    ]\n\n    def __init__(self, level: GuardrailLevel = GuardrailLevel.MODERATE):\n        self.level = level\n\n    async def sanitize(self, input_data: JSONValue) -> SanitizationResult:\n        \"\"\"\n        Sanitize input data.\n\n        Args:\n            input_data: Raw input to sanitize\n\n        Returns:\n            SanitizationResult with sanitized data\n        \"\"\"\n        modifications = []\n        risks = []\n        blocked = False\n\n        if isinstance(input_data, str):\n            sanitized, mods, found_risks = await self._sanitize_string(input_data)\n            modifications.extend(mods)\n            risks.extend(found_risks)\n\n        elif isinstance(input_data, dict):\n            sanitized = {}\n            for key, value in input_data.items():\n                # Sanitize keys\n                clean_key, key_mods, key_risks = await self._sanitize_string(str(key))\n                modifications.extend(key_mods)\n                risks.extend(key_risks)\n\n                # Recursively sanitize values\n                sub_result = await self.sanitize(value)\n                sanitized[clean_key] = sub_result.sanitized\n                modifications.extend(sub_result.modifications_made)\n                risks.extend(sub_result.risks_detected)\n\n        elif isinstance(input_data, list):\n            sanitized = []\n            for item in input_data:\n                sub_result = await self.sanitize(item)\n                sanitized.append(sub_result.sanitized)\n                modifications.extend(sub_result.modifications_made)\n                risks.extend(sub_result.risks_detected)\n        else:\n            sanitized = input_data\n\n        # Block if too many risks in strict mode\n        if self.level == GuardrailLevel.STRICT and len(risks) > 3:\n            blocked = True\n\n        return SanitizationResult(\n            sanitized=sanitized,\n            modifications_made=modifications,\n            risks_detected=risks,\n            blocked=blocked,\n        )\n\n    async def _sanitize_string(\n        self,\n        text: str\n    ) -> tuple[str, List[str], List[str]]:\n        \"\"\"Sanitize a string value.\"\"\"\n        modifications = []\n        risks = []\n        sanitized = text\n\n        # Check dangerous patterns\n        for pattern in self.DANGEROUS_PATTERNS:\n            matches = re.findall(pattern, sanitized, re.IGNORECASE)\n            if matches:\n                risks.append(f\"Dangerous pattern: {pattern}\")\n                sanitized = re.sub(pattern, \"[REMOVED]\", sanitized, flags=re.IGNORECASE)\n                modifications.append(f\"Removed pattern: {pattern}\")\n\n        # Check injection patterns\n        for pattern in self.INJECTION_PATTERNS:\n            if re.search(pattern, sanitized, re.IGNORECASE):\n                risks.append(f\"Injection pattern: {pattern}\")\n                sanitized = re.sub(pattern, \"\", sanitized, flags=re.IGNORECASE)\n                modifications.append(f\"Removed injection: {pattern}\")\n\n        # Escape remaining special characters in strict mode\n        if self.level == GuardrailLevel.STRICT:\n            sanitized = sanitized.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n\n        return sanitized, modifications, risks\n\n\nclass PolicyEngine:\n    \"\"\"\n    Evaluates actions against constitutional policies.\n\n    Pre-execution check to ensure compliance.\n    \"\"\"\n\n    def __init__(self):\n        self._policies: Dict[str, Callable[[Union[Callable, JSONValue], ContextData], bool]] = {}\n        self._load_default_policies()\n\n    def _load_default_policies(self):\n        \"\"\"Load default constitutional policies.\"\"\"\n        self._policies[\"constitutional_hash\"] = lambda action, ctx: \\\n            ctx.get(\"constitutional_hash\") == CONSTITUTIONAL_HASH or \\\n            \"constitutional_hash\" not in ctx\n\n        self._policies[\"no_admin_bypass\"] = lambda action, ctx: \\\n            \"admin_override\" not in str(action).lower()\n\n        self._policies[\"rate_limit\"] = lambda action, ctx: \\\n            ctx.get(\"request_count\", 0) < 1000\n\n    async def evaluate(\n        self,\n        action: Union[Callable, JSONValue],\n        context: ContextData,\n        constitutional_hash: str = CONSTITUTIONAL_HASH\n    ) -> PolicyResult:\n        \"\"\"\n        Evaluate action against all policies.\n\n        Args:\n            action: The action to evaluate\n            context: Execution context\n            constitutional_hash: Expected hash\n\n        Returns:\n            PolicyResult with evaluation outcome\n        \"\"\"\n        reasons = []\n        requires_escalation = False\n        escalation_reason = None\n\n        context[\"constitutional_hash\"] = constitutional_hash\n\n        for policy_id, check_fn in self._policies.items():\n            try:\n                if not check_fn(action, context):\n                    reasons.append(f\"Policy '{policy_id}' violation\")\n\n                    # High-risk violations require escalation\n                    if policy_id in [\"constitutional_hash\", \"no_admin_bypass\"]:\n                        requires_escalation = True\n                        escalation_reason = f\"Critical policy violation: {policy_id}\"\n\n            except Exception as e:\n                reasons.append(f\"Policy '{policy_id}' error: {str(e)}\")\n\n        return PolicyResult(\n            allowed=len(reasons) == 0,\n            policy_id=\"combined\",\n            reasons=reasons,\n            requires_escalation=requires_escalation,\n            escalation_reason=escalation_reason,\n        )\n\n\nclass Sandbox:\n    \"\"\"\n    Sandboxed execution environment.\n\n    Executes actions in isolated environment with resource limits.\n    \"\"\"\n\n    def __init__(\n        self,\n        timeout_seconds: float = 30.0,\n        memory_limit_mb: int = 512\n    ):\n        self.timeout_seconds = timeout_seconds\n        self.memory_limit_mb = memory_limit_mb\n\n    async def execute(self, action: Union[Callable, JSONValue]) -> SandboxResult:\n        \"\"\"\n        Execute action in sandbox.\n\n        Args:\n            action: The action to execute\n\n        Returns:\n            SandboxResult with execution outcome\n        \"\"\"\n        import time\n        start_time = time.perf_counter()\n        errors = []\n        output = None\n        truncated = False\n\n        try:\n            # Simulate sandboxed execution\n            # In production, would use actual isolation (e.g., Firecracker)\n\n            if callable(action):\n                output = await asyncio.wait_for(\n                    action(),\n                    timeout=self.timeout_seconds\n                )\n            else:\n                output = action\n\n            # Truncate large outputs\n            if isinstance(output, str) and len(output) > 10000:\n                output = output[:10000] + \"...[truncated]\"\n                truncated = True\n\n        except asyncio.TimeoutError:\n            errors.append(f\"Execution timeout after {self.timeout_seconds}s\")\n        except Exception as e:\n            errors.append(f\"Execution error: {str(e)}\")\n\n        execution_time = (time.perf_counter() - start_time) * 1000\n\n        return SandboxResult(\n            output=output,\n            execution_time_ms=execution_time,\n            resource_usage={\n                \"cpu_percent\": 5.0,  # Simulated\n                \"memory_mb\": 50.0,\n            },\n            errors=errors,\n            truncated=truncated,\n        )\n\n\nclass OutputVerifier:\n    \"\"\"\n    Verifies output for constitutional compliance.\n\n    Post-execution check to ensure output is safe.\n    \"\"\"\n\n    SENSITIVE_PATTERNS = [\n        r\"password\\s*[:=]\\s*\\S+\",\n        r\"api[_-]?key\\s*[:=]\\s*\\S+\",\n        r\"secret\\s*[:=]\\s*\\S+\",\n        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n    ]\n\n    async def verify(self, output: Optional[JSONValue]) -> VerificationResult:\n        \"\"\"\n        Verify output for compliance.\n\n        Args:\n            output: The output to verify\n\n        Returns:\n            VerificationResult with verification outcome\n        \"\"\"\n        modifications = []\n        warnings = []\n\n        if isinstance(output, str):\n            # Check for sensitive data leakage\n            for pattern in self.SENSITIVE_PATTERNS:\n                if re.search(pattern, output, re.IGNORECASE):\n                    warnings.append(f\"Potential sensitive data: {pattern}\")\n                    # Redact sensitive data\n                    output = re.sub(pattern, \"[REDACTED]\", output, flags=re.IGNORECASE)\n                    modifications.append(f\"Redacted: {pattern}\")\n\n        return VerificationResult(\n            verified=len(warnings) == 0,\n            modifications_made=modifications,\n            warnings=warnings,\n        )\n\n\nclass EscalationHandler:\n    \"\"\"\n    Handles escalation of policy violations.\n\n    Routes violations to appropriate handlers.\n    \"\"\"\n\n    def __init__(self):\n        self._handlers: Dict[EscalationAction, Callable] = {}\n        self._escalation_log: List[JSONDict] = []\n\n    async def escalate(\n        self,\n        action: Union[Callable, JSONValue],\n        reason: str,\n        escalation_action: EscalationAction = EscalationAction.HUMAN_REVIEW\n    ) -> JSONDict:\n        \"\"\"\n        Escalate a violation.\n\n        Args:\n            action: The violating action\n            reason: Reason for escalation\n            escalation_action: Type of escalation\n\n        Returns:\n            Escalation result\n        \"\"\"\n        escalation_id = f\"esc-{uuid.uuid4().hex[:8]}\"\n\n        escalation_record = {\n            \"escalation_id\": escalation_id,\n            \"action\": str(action)[:100],\n            \"reason\": reason,\n            \"escalation_action\": escalation_action.value,\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"status\": \"pending\",\n        }\n\n        self._escalation_log.append(escalation_record)\n\n        logger.warning(f\"Escalation {escalation_id}: {reason}\")\n\n        return escalation_record\n\n\nclass AuditLog:\n    \"\"\"\n    Audit logging for all guardrail operations.\n\n    Maintains immutable record of all actions.\n    \"\"\"\n\n    def __init__(self):\n        self._logs: AuditTrail = []\n\n    async def record(\n        self,\n        action: Union[Callable, JSONValue],\n        result: Union[str, JSONValue],\n        metadata: Optional[MetadataDict] = None\n    ) -> str:\n        \"\"\"\n        Record an action and its result.\n\n        Args:\n            action: The action taken\n            result: The result\n            metadata: Optional metadata\n\n        Returns:\n            Audit ID\n        \"\"\"\n        audit_id = f\"audit-{uuid.uuid4().hex[:8]}\"\n\n        record = {\n            \"audit_id\": audit_id,\n            \"action_hash\": hash(str(action)),\n            \"result_hash\": hash(str(result)),\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            \"metadata\": metadata or {},\n        }\n\n        self._logs.append(record)\n        return audit_id\n\n    async def get_logs(\n        self,\n        limit: int = 100\n    ) -> AuditTrail:\n        \"\"\"Get recent audit logs.\"\"\"\n        return self._logs[-limit:]\n\n\nclass ConstitutionalGuardrails:\n    \"\"\"\n    Constitutional Runtime Guardrails.\n\n    Full pipeline:\n    1. Input sanitization\n    2. Policy enforcement (pre-execution)\n    3. Sandbox execution\n    4. Output verification\n    5. Audit logging\n\n    Enforces constitutional principles at runtime\n    with minimal latency impact.\n    \"\"\"\n\n    def __init__(\n        self,\n        level: GuardrailLevel = GuardrailLevel.MODERATE\n    ):\n        \"\"\"\n        Initialize guardrails.\n\n        Args:\n            level: Enforcement level\n        \"\"\"\n        self.level = level\n\n        self.input_sanitizer = InputSanitizer(level)\n        self.policy_engine = PolicyEngine()\n        self.sandbox = Sandbox()\n        self.output_verifier = OutputVerifier()\n        self.escalation_handler = EscalationHandler()\n        self.audit_log = AuditLog()\n\n        self._stats = {\n            \"total_requests\": 0,\n            \"blocked\": 0,\n            \"escalated\": 0,\n            \"completed\": 0,\n        }\n\n        logger.info(f\"Initialized ConstitutionalGuardrails level={level.value}\")\n\n    async def enforce(\n        self,\n        action: Union[Callable, JSONValue],\n        context: Optional[ContextData] = None\n    ) -> GuardrailResult:\n        \"\"\"\n        Full guardrail pipeline execution.\n\n        Args:\n            action: The action to guard\n            context: Execution context\n\n        Returns:\n            GuardrailResult with complete pipeline results\n        \"\"\"\n        import time\n        start_time = time.perf_counter()\n\n        result_id = f\"guard-{uuid.uuid4().hex[:8]}\"\n        self._stats[\"total_requests\"] += 1\n        context = context or {}\n\n        # Step 1: Input sanitization\n        sanitization = await self.input_sanitizer.sanitize(action)\n\n        if sanitization.blocked:\n            self._stats[\"blocked\"] += 1\n            audit_id = await self.audit_log.record(action, \"blocked_by_sanitizer\")\n            processing_time = (time.perf_counter() - start_time) * 1000\n\n            return GuardrailResult(\n                result_id=result_id,\n                action=action,\n                result=None,\n                sanitization=sanitization,\n                policy_result=PolicyResult(False, \"sanitizer\", [\"Blocked by sanitizer\"], False),\n                sandbox_result=None,\n                verification=VerificationResult(False, [], [\"Blocked before execution\"]),\n                audit_id=audit_id,\n                processing_time_ms=processing_time,\n            )\n\n        sanitized_action = sanitization.sanitized\n\n        # Step 2: Policy enforcement (pre-execution)\n        policy_result = await self.policy_engine.evaluate(\n            sanitized_action,\n            context\n        )\n\n        if policy_result.requires_escalation:\n            self._stats[\"escalated\"] += 1\n            await self.escalation_handler.escalate(\n                sanitized_action,\n                policy_result.escalation_reason or \"Policy violation\"\n            )\n\n        if not policy_result.allowed:\n            self._stats[\"blocked\"] += 1\n            audit_id = await self.audit_log.record(action, \"blocked_by_policy\")\n            processing_time = (time.perf_counter() - start_time) * 1000\n\n            return GuardrailResult(\n                result_id=result_id,\n                action=action,\n                result=None,\n                sanitization=sanitization,\n                policy_result=policy_result,\n                sandbox_result=None,\n                verification=VerificationResult(False, [], policy_result.reasons),\n                audit_id=audit_id,\n                processing_time_ms=processing_time,\n            )\n\n        # Step 3: Sandbox execution\n        sandbox_result = await self.sandbox.execute(sanitized_action)\n\n        # Step 4: Output verification\n        verification = await self.output_verifier.verify(sandbox_result.output)\n\n        # Step 5: Audit logging\n        audit_id = await self.audit_log.record(\n            action,\n            sandbox_result.output,\n            {\"verification\": verification.verified}\n        )\n\n        self._stats[\"completed\"] += 1\n        processing_time = (time.perf_counter() - start_time) * 1000\n\n        return GuardrailResult(\n            result_id=result_id,\n            action=action,\n            result=sandbox_result.output,\n            sanitization=sanitization,\n            policy_result=policy_result,\n            sandbox_result=sandbox_result,\n            verification=verification,\n            audit_id=audit_id,\n            processing_time_ms=processing_time,\n        )\n\n    def get_stats(self) -> JSONDict:\n        \"\"\"Get guardrails statistics.\"\"\"\n        total = self._stats[\"total_requests\"]\n        block_rate = self._stats[\"blocked\"] / max(total, 1)\n\n        return {\n            **self._stats,\n            \"block_rate\": block_rate,\n            \"level\": self.level.value,\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n\n\ndef create_guardrails(\n    level: GuardrailLevel = GuardrailLevel.MODERATE\n) -> ConstitutionalGuardrails:\n    \"\"\"Factory function to create guardrails.\"\"\"\n    return ConstitutionalGuardrails(level=level)\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.215936",
  "last_updated": "2026-01-04T05:35:59.153603"
}