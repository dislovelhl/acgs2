{
  "file_path": "acgs2-core/enhanced_agent_bus/ai_assistant/nlu.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 AI Assistant - Natural Language Understanding\nConstitutional Hash: cdd01ef066bc6cf2\n\nAdvanced NLU with intent classification, entity extraction,\nand sentiment analysis. Integrates with constitutional governance.\n\"\"\"\n\nimport logging\nimport re\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\n# Import centralized constitutional hash with fallback\ntry:\n    from shared.constants import CONSTITUTIONAL_HASH\nexcept ImportError:\n    CONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\n\nclass Sentiment(Enum):\n    \"\"\"Sentiment categories.\"\"\"\n\n    VERY_NEGATIVE = -2\n    NEGATIVE = -1\n    NEUTRAL = 0\n    POSITIVE = 1\n    VERY_POSITIVE = 2\n\n\n@dataclass\nclass Intent:\n    \"\"\"Represents a detected intent.\"\"\"\n\n    name: str\n    confidence: float\n    parameters: Dict[str, Any] = field(default_factory=dict)\n    is_primary: bool = True\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"name\": self.name,\n            \"confidence\": self.confidence,\n            \"parameters\": self.parameters,\n            \"is_primary\": self.is_primary,\n        }\n\n\n@dataclass\nclass Entity:\n    \"\"\"Represents an extracted entity.\"\"\"\n\n    text: str\n    type: str\n    value: Any\n    start: int\n    end: int\n    confidence: float = 1.0\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"text\": self.text,\n            \"type\": self.type,\n            \"value\": self.value,\n            \"start\": self.start,\n            \"end\": self.end,\n            \"confidence\": self.confidence,\n            \"metadata\": self.metadata,\n        }\n\n\n@dataclass\nclass NLUResult:\n    \"\"\"Complete NLU processing result.\"\"\"\n\n    original_text: str = \"\"\n    processed_text: str = \"\"\n    primary_intent: Optional[Intent] = None\n    secondary_intents: List[Intent] = field(default_factory=list)\n    entities: Union[List[Entity], Dict[str, Any]] = field(default_factory=dict)\n    sentiment: Sentiment = Sentiment.NEUTRAL\n    sentiment_score: float = 0.5\n    language: str = \"en\"\n    confidence: float = 0.0\n    requires_clarification: bool = False\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n    processing_time_ms: float = 0.0\n    # Convenience parameter for simpler initialization\n    intents: Optional[List[Dict[str, Any]]] = field(default=None, repr=False)\n\n    def __post_init__(self):\n        \"\"\"Process intents convenience parameter if provided.\"\"\"\n        if self.intents:\n            # Convert dict-based intents to Intent objects\n            intent_objects = []\n            for intent_data in self.intents:\n                if isinstance(intent_data, dict):\n                    intent = Intent(\n                        name=intent_data.get(\"intent\", intent_data.get(\"name\", \"unknown\")),\n                        confidence=intent_data.get(\"confidence\", 0.0),\n                    )\n                    intent_objects.append(intent)\n                elif isinstance(intent_data, Intent):\n                    intent_objects.append(intent_data)\n\n            if intent_objects:\n                self.primary_intent = intent_objects[0]\n                self.secondary_intents = intent_objects[1:] if len(intent_objects) > 1 else []\n\n            # Clear the convenience parameter\n            self.intents = None\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"original_text\": self.original_text,\n            \"processed_text\": self.processed_text,\n            \"primary_intent\": self.primary_intent.to_dict() if self.primary_intent else None,\n            \"secondary_intents\": [i.to_dict() for i in self.secondary_intents],\n            \"entities\": (\n                self.entities\n                if isinstance(self.entities, dict)\n                else [e.to_dict() for e in self.entities]\n            ),\n            \"sentiment\": (\n                self.sentiment.name if hasattr(self.sentiment, \"name\") else str(self.sentiment)\n            ),\n            \"sentiment_score\": self.sentiment_score,\n            \"language\": self.language,\n            \"confidence\": self.confidence,\n            \"requires_clarification\": self.requires_clarification,\n            \"constitutional_hash\": self.constitutional_hash,\n            \"processing_time_ms\": self.processing_time_ms,\n        }\n\n\nclass IntentClassifier(ABC):\n    \"\"\"Abstract base class for intent classification.\"\"\"\n\n    @abstractmethod\n    async def classify(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> List[Intent]:\n        \"\"\"Classify text into intents.\"\"\"\n        pass\n\n\nclass EntityExtractor(ABC):\n    \"\"\"Abstract base class for entity extraction.\"\"\"\n\n    @abstractmethod\n    async def extract(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> List[Entity]:\n        \"\"\"Extract entities from text.\"\"\"\n        pass\n\n\nclass RuleBasedIntentClassifier(IntentClassifier):\n    \"\"\"\n    Rule-based intent classifier using pattern matching.\n\n    Good for well-defined intents with clear patterns.\n    Can be extended with ML-based classification.\n    \"\"\"\n\n    def __init__(self, intent_patterns: Optional[Dict[str, List[str]]] = None):\n        self.intent_patterns = intent_patterns or self._default_patterns()\n        self._compiled_patterns = self._compile_patterns()\n\n    def _default_patterns(self) -> Dict[str, List[str]]:\n        \"\"\"Default intent patterns.\"\"\"\n        return {\n            \"greeting\": [\n                r\"\\b(hi|hello|hey|greetings|good\\s*(morning|afternoon|evening))\\b\",\n                r\"^(hi|hello|hey)$\",\n            ],\n            \"farewell\": [\n                r\"\\b(bye|goodbye|see\\s*you|take\\s*care|farewell)\\b\",\n            ],\n            \"help\": [\n                r\"\\b(help|assist|support|stuck|confused)\\b\",\n                r\"(can|could)\\s+you\\s+help\",\n                r\"i\\s+need\\s+help\",\n            ],\n            \"question\": [\n                r\"^(what|who|where|when|why|how|which|can|could|would|is|are|do|does)\\b\",\n                r\"\\?$\",\n            ],\n            \"confirmation\": [\n                r\"^(yes|yeah|yep|sure|ok|okay|correct|right|exactly|confirm)$\",\n                r\"\\b(that's\\s*right|that's\\s*correct)\\b\",\n            ],\n            \"denial\": [\n                r\"^(no|nope|nah|wrong|incorrect|cancel)$\",\n                r\"\\b(that's\\s*wrong|not\\s*correct)\\b\",\n            ],\n            \"order_status\": [\n                r\"\\b(order|delivery|shipment)\\s*(status|update|tracking)\\b\",\n                r\"where\\s*is\\s*my\\s*(order|package|delivery)\",\n                r\"track\\s*(my\\s*)?(order|package)\",\n            ],\n            \"complaint\": [\n                r\"\\b(problem|issue|broken|not\\s*working|disappointed|frustrated|angry)\\b\",\n                r\"this\\s+is\\s+(terrible|awful|unacceptable)\",\n            ],\n            \"request_info\": [\n                r\"(tell|inform|let)\\s+me\\s+about\",\n                r\"i\\s+want\\s+to\\s+know\",\n                r\"(can|could)\\s+you\\s+(tell|explain)\",\n            ],\n            \"feedback\": [\n                r\"\\b(feedback|suggestion|recommend|improve)\\b\",\n                r\"i\\s+(think|suggest|recommend)\",\n            ],\n        }\n\n    def _compile_patterns(self) -> Dict[str, List[re.Pattern]]:\n        \"\"\"Compile regex patterns for efficiency.\"\"\"\n        compiled = {}\n        for intent, patterns in self.intent_patterns.items():\n            compiled[intent] = [re.compile(pattern, re.IGNORECASE) for pattern in patterns]\n        return compiled\n\n    async def classify(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> List[Intent]:\n        \"\"\"Classify text into intents using pattern matching.\"\"\"\n        intents = []\n        text_lower = text.lower().strip()\n\n        for intent_name, patterns in self._compiled_patterns.items():\n            for pattern in patterns:\n                match = pattern.search(text_lower)\n                if match:\n                    # Calculate confidence based on match coverage\n                    match_length = match.end() - match.start()\n                    confidence = min(0.95, 0.5 + (match_length / len(text_lower)) * 0.5)\n\n                    intents.append(\n                        Intent(\n                            name=intent_name,\n                            confidence=confidence,\n                            parameters={\n                                \"matched_pattern\": pattern.pattern,\n                                \"match_text\": match.group(),\n                            },\n                        )\n                    )\n                    break  # One match per intent is enough\n\n        # Sort by confidence\n        intents.sort(key=lambda x: x.confidence, reverse=True)\n\n        # Mark primary intent\n        if intents:\n            intents[0].is_primary = True\n            for intent in intents[1:]:\n                intent.is_primary = False\n\n        # Add fallback if no intents found\n        if not intents:\n            intents.append(\n                Intent(\n                    name=\"unknown\",\n                    confidence=0.5,\n                    parameters={\"requires_clarification\": True},\n                )\n            )\n\n        return intents\n\n\nclass PatternEntityExtractor(EntityExtractor):\n    \"\"\"\n    Pattern-based entity extraction.\n\n    Extracts common entity types using regex patterns.\n    Can be extended with NER models.\n    \"\"\"\n\n    def __init__(self, custom_patterns: Optional[Dict[str, str]] = None):\n        self.patterns = self._default_patterns()\n        if custom_patterns:\n            self.patterns.update(custom_patterns)\n        self._compiled_patterns = self._compile_patterns()\n\n    def _default_patterns(self) -> Dict[str, str]:\n        \"\"\"Default entity patterns.\"\"\"\n        return {\n            \"email\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n            \"phone\": r\"\\b(\\+?1?[-.\\s]?)?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b\",\n            \"order_id\": r\"\\b[A-Z]{2,3}[-]?[0-9]{5,10}\\b\",\n            \"date\": r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b\",\n            \"time\": r\"\\b([01]?[0-9]|2[0-3]):[0-5][0-9](\\s*(am|pm|AM|PM))?\\b\",\n            \"money\": r\"\\$[0-9]+(?:\\.[0-9]{2})?\",\n            \"number\": r\"\\b\\d+(?:\\.\\d+)?\\b\",\n            \"url\": r\"https?://[^\\s<>\\\"{}|\\\\^`\\[\\]]+\",\n            \"product_code\": r\"\\b[A-Z]{2,4}-[0-9]{3,6}\\b\",\n        }\n\n    def _compile_patterns(self) -> Dict[str, re.Pattern]:\n        \"\"\"Compile patterns for efficiency.\"\"\"\n        return {\n            entity_type: re.compile(pattern, re.IGNORECASE)\n            for entity_type, pattern in self.patterns.items()\n        }\n\n    async def extract(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> List[Entity]:\n        \"\"\"Extract entities from text.\"\"\"\n        entities = []\n\n        for entity_type, pattern in self._compiled_patterns.items():\n            for match in pattern.finditer(text):\n                entity = Entity(\n                    text=match.group(),\n                    type=entity_type,\n                    value=self._normalize_value(entity_type, match.group()),\n                    start=match.start(),\n                    end=match.end(),\n                    confidence=0.9,\n                )\n                entities.append(entity)\n\n        # Sort by position\n        entities.sort(key=lambda x: x.start)\n        return entities\n\n    def _normalize_value(self, entity_type: str, raw_value: str) -> Any:\n        \"\"\"Normalize entity value based on type.\"\"\"\n        normalizers = {\n            \"phone\": lambda x: re.sub(r\"[^0-9+]\", \"\", x),\n            \"email\": lambda x: x.lower().strip(),\n            \"number\": lambda x: float(x) if \".\" in x else int(x),\n            \"money\": lambda x: float(x.replace(\"$\", \"\").replace(\",\", \"\")),\n        }\n\n        normalizer = normalizers.get(entity_type, lambda x: x)\n        try:\n            return normalizer(raw_value)\n        except (ValueError, AttributeError):\n            return raw_value\n\n\nclass SentimentAnalyzer(ABC):\n    \"\"\"Abstract base class for sentiment analysis.\"\"\"\n\n    @abstractmethod\n    async def analyze(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> str:\n        \"\"\"\n        Analyze sentiment of text.\n\n        Args:\n            text: Text to analyze\n            context: Optional context for analysis\n\n        Returns:\n            Sentiment string: \"positive\", \"negative\", or \"neutral\"\n        \"\"\"\n        pass\n\n\nclass BasicSentimentAnalyzer(SentimentAnalyzer):\n    \"\"\"\n    Simple sentiment analyzer using keyword matching.\n\n    Can be replaced with ML-based sentiment analysis.\n    \"\"\"\n\n    def __init__(self):\n        self.positive_words = {\n            \"great\",\n            \"good\",\n            \"excellent\",\n            \"amazing\",\n            \"wonderful\",\n            \"fantastic\",\n            \"love\",\n            \"like\",\n            \"best\",\n            \"happy\",\n            \"pleased\",\n            \"satisfied\",\n            \"thank\",\n            \"thanks\",\n            \"helpful\",\n            \"perfect\",\n            \"awesome\",\n            \"brilliant\",\n            \"superb\",\n        }\n        self.negative_words = {\n            \"bad\",\n            \"terrible\",\n            \"awful\",\n            \"horrible\",\n            \"worst\",\n            \"hate\",\n            \"angry\",\n            \"frustrated\",\n            \"disappointed\",\n            \"upset\",\n            \"annoyed\",\n            \"poor\",\n            \"problem\",\n            \"issue\",\n            \"broken\",\n            \"wrong\",\n            \"fail\",\n            \"failed\",\n            \"error\",\n            \"bug\",\n        }\n        self.intensifiers = {\"very\", \"really\", \"extremely\", \"absolutely\", \"totally\"}\n        self.negators = {\"not\", \"no\", \"never\", \"don't\", \"doesn't\", \"didn't\", \"won't\"}\n\n    async def analyze(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> str:\n        \"\"\"Analyze sentiment of text and return sentiment string.\"\"\"\n        sentiment, score = self._analyze_internal(text)\n\n        # Map enum to simple string\n        if sentiment in (Sentiment.POSITIVE, Sentiment.VERY_POSITIVE):\n            return \"positive\"\n        elif sentiment in (Sentiment.NEGATIVE, Sentiment.VERY_NEGATIVE):\n            return \"negative\"\n        else:\n            return \"neutral\"\n\n    def _analyze_internal(self, text: str) -> Tuple[Sentiment, float]:\n        \"\"\"Internal analysis returning enum and score.\"\"\"\n        words = text.lower().split()\n        score = 0.0\n        word_count = len(words)\n\n        if word_count == 0:\n            return Sentiment.NEUTRAL, 0.0\n\n        negation = False\n        intensity = 1.0\n\n        for _i, word in enumerate(words):\n            # Check for negation\n            if word in self.negators:\n                negation = True\n                continue\n\n            # Check for intensifier\n            if word in self.intensifiers:\n                intensity = 1.5\n                continue\n\n            # Score positive/negative words\n            if word in self.positive_words:\n                word_score = 1.0 * intensity\n                score += -word_score if negation else word_score\n            elif word in self.negative_words:\n                word_score = -1.0 * intensity\n                score += -word_score if negation else word_score\n\n            # Reset modifiers after using\n            negation = False\n            intensity = 1.0\n\n        # Normalize score\n        normalized_score = score / max(word_count, 1)\n        normalized_score = max(-1.0, min(1.0, normalized_score))\n\n        # Map to sentiment category\n        if normalized_score >= 0.5:\n            sentiment = Sentiment.VERY_POSITIVE\n        elif normalized_score >= 0.2:\n            sentiment = Sentiment.POSITIVE\n        elif normalized_score <= -0.5:\n            sentiment = Sentiment.VERY_NEGATIVE\n        elif normalized_score <= -0.2:\n            sentiment = Sentiment.NEGATIVE\n        else:\n            sentiment = Sentiment.NEUTRAL\n\n        return sentiment, normalized_score\n\n\nclass NLUEngine:\n    \"\"\"\n    Complete NLU processing engine.\n\n    Combines intent classification, entity extraction, and sentiment analysis\n    with constitutional governance integration.\n    \"\"\"\n\n    def __init__(\n        self,\n        intent_classifier: Optional[IntentClassifier] = None,\n        entity_extractor: Optional[EntityExtractor] = None,\n        sentiment_analyzer: Optional[SentimentAnalyzer] = None,\n        constitutional_hash: str = CONSTITUTIONAL_HASH,\n        confidence_threshold: float = 0.65,\n    ):\n        self.intent_classifier = intent_classifier or RuleBasedIntentClassifier()\n        self.entity_extractor = entity_extractor or PatternEntityExtractor()\n        self.sentiment_analyzer = sentiment_analyzer or BasicSentimentAnalyzer()\n        self.constitutional_hash = constitutional_hash\n        self.confidence_threshold = confidence_threshold\n\n    async def process(\n        self,\n        text: str,\n        context: Optional[Dict[str, Any]] = None,\n    ) -> NLUResult:\n        \"\"\"\n        Process text through full NLU pipeline.\n\n        Args:\n            text: Input text to process\n            context: Optional conversation context\n\n        Returns:\n            NLUResult with intents, entities, sentiment\n        \"\"\"\n        import time\n\n        start_time = time.perf_counter()\n\n        # Preprocess text\n        processed_text = self._preprocess(text)\n\n        # Classify intent\n        intents = await self.intent_classifier.classify(processed_text, context)\n        primary_intent = intents[0] if intents else None\n        secondary_intents = intents[1:] if len(intents) > 1 else []\n\n        # Extract entities\n        entities = await self.entity_extractor.extract(processed_text, context)\n\n        # Analyze sentiment\n        sentiment_str = await self.sentiment_analyzer.analyze(processed_text, context)\n        # Map string back to enum for NLUResult\n        sentiment_map = {\n            \"positive\": Sentiment.POSITIVE,\n            \"negative\": Sentiment.NEGATIVE,\n            \"neutral\": Sentiment.NEUTRAL,\n        }\n        sentiment = sentiment_map.get(sentiment_str, Sentiment.NEUTRAL)\n        sentiment_score = (\n            0.5 if sentiment_str == \"positive\" else (-0.5 if sentiment_str == \"negative\" else 0.0)\n        )\n\n        # Detect language (simplified - just English detection)\n        language = self._detect_language(text)\n\n        # Calculate overall confidence\n        confidence = self._calculate_confidence(primary_intent, entities)\n\n        # Determine if clarification is needed\n        requires_clarification = self._needs_clarification(\n            primary_intent, secondary_intents, confidence\n        )\n\n        processing_time = (time.perf_counter() - start_time) * 1000\n\n        return NLUResult(\n            original_text=text,\n            processed_text=processed_text,\n            primary_intent=primary_intent,\n            secondary_intents=secondary_intents,\n            entities=entities,\n            sentiment=sentiment,\n            sentiment_score=sentiment_score,\n            language=language,\n            confidence=confidence,\n            requires_clarification=requires_clarification,\n            constitutional_hash=self.constitutional_hash,\n            processing_time_ms=processing_time,\n        )\n\n    def _preprocess(self, text: str) -> str:\n        \"\"\"Preprocess text for NLU.\"\"\"\n        # Remove extra whitespace\n        text = \" \".join(text.split())\n\n        # Basic normalization\n        text = text.strip()\n\n        return text\n\n    def _detect_language(self, text: str) -> str:\n        \"\"\"Simple language detection.\"\"\"\n        # This is a placeholder - would use a proper language detection library\n        return \"en\"\n\n    def _calculate_confidence(\n        self,\n        primary_intent: Optional[Intent],\n        entities: List[Entity],\n    ) -> float:\n        \"\"\"Calculate overall NLU confidence.\"\"\"\n        if not primary_intent:\n            return 0.0\n\n        intent_confidence = primary_intent.confidence\n\n        # Boost confidence if entities were found\n        if entities:\n            entity_boost = min(0.1, len(entities) * 0.02)\n            intent_confidence = min(1.0, intent_confidence + entity_boost)\n\n        return intent_confidence\n\n    def _needs_clarification(\n        self,\n        primary_intent: Optional[Intent],\n        secondary_intents: List[Intent],\n        confidence: float,\n    ) -> bool:\n        \"\"\"Determine if clarification is needed.\"\"\"\n        # Unknown intent\n        if not primary_intent or primary_intent.name == \"unknown\":\n            return True\n\n        # Low confidence\n        if confidence < self.confidence_threshold:\n            return True\n\n        # Competing intents with similar confidence\n        if secondary_intents:\n            for secondary in secondary_intents:\n                if primary_intent.confidence - secondary.confidence < 0.15:\n                    return True\n\n        return False\n\n    def add_intent_pattern(self, intent_name: str, patterns: List[str]) -> None:\n        \"\"\"Add or update intent patterns.\"\"\"\n        if isinstance(self.intent_classifier, RuleBasedIntentClassifier):\n            self.intent_classifier.intent_patterns[intent_name] = patterns\n            self.intent_classifier._compiled_patterns = self.intent_classifier._compile_patterns()\n\n    def add_entity_pattern(self, entity_type: str, pattern: str) -> None:\n        \"\"\"Add or update entity pattern.\"\"\"\n        if isinstance(self.entity_extractor, PatternEntityExtractor):\n            self.entity_extractor.patterns[entity_type] = pattern\n            self.entity_extractor._compiled_patterns = self.entity_extractor._compile_patterns()\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.341762",
  "last_updated": "2026-01-04T05:35:59.184568"
}