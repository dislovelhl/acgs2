{
  "file_path": ".agent/workflows/base/workflow.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Base Workflow\nConstitutional Hash: cdd01ef066bc6cf2\n\nAbstract base class for all workflow implementations.\nProvides common infrastructure for constitutional validation,\nstep execution, compensation handling, and audit recording.\n\"\"\"\n\nimport asyncio\nimport logging\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\n\nfrom .activities import BaseActivities, get_default_activities\nfrom .context import WorkflowContext\nfrom .result import WorkflowResult, WorkflowStatus\nfrom .step import StepCompensation, WorkflowStep\n\ntry:\n    from shared.constants import CONSTITUTIONAL_HASH\nexcept ImportError:\n    CONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\n# Metrics integration\ntry:\n    from shared import metrics\n\n    HAS_METRICS = True\nexcept ImportError:\n    HAS_METRICS = False\n\ntry:\n    from enhanced_agent_bus.exceptions import ConstitutionalHashMismatchError\nexcept ImportError:\n\n    class ConstitutionalHashMismatchError(Exception):\n        def __init__(self, expected_hash: str, actual_hash: str, context: Optional[str] = None):\n            self.expected_hash = expected_hash\n            self.actual_hash = actual_hash\n            self.context = context\n            super().__init__(\n                f\"Constitutional hash mismatch: expected {expected_hash}, got {actual_hash}\"\n            )\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseWorkflow(ABC):\n    \"\"\"\n    Abstract base class for all ACGS-2 workflows.\n\n    Provides common infrastructure:\n    - Constitutional hash validation at boundaries\n    - Step execution with retries and timeouts\n    - Compensation handling (LIFO order)\n    - Audit trail recording\n    - Metrics and tracing support\n\n    Subclasses must implement:\n    - execute(input: Dict) -> WorkflowResult\n\n    Example:\n        class MyWorkflow(BaseWorkflow):\n            async def execute(self, input: Dict) -> WorkflowResult:\n                # Validate constitutional hash (always first)\n                await self.validate_constitutional_hash()\n\n                # Execute steps\n                result1 = await self.run_step(self.step1, input)\n                result2 = await self.run_step(self.step2, {\"prev\": result1})\n\n                # Record audit (always last)\n                return await self.complete(result2)\n    \"\"\"\n\n    def __init__(\n        self,\n        workflow_id: Optional[str] = None,\n        workflow_name: str = \"base\",\n        activities: Optional[BaseActivities] = None,\n        constitutional_hash: str = CONSTITUTIONAL_HASH,\n        timeout_seconds: int = 300,\n        fail_closed: bool = True,\n    ):\n        \"\"\"\n        Initialize workflow.\n\n        Args:\n            workflow_id: Unique workflow instance ID (generated if not provided)\n            activities: Activity implementations for external operations\n            constitutional_hash: Expected constitutional hash\n            timeout_seconds: Maximum workflow execution time\n            fail_closed: If True, reject on validation/policy failures\n        \"\"\"\n        self.workflow_id = workflow_id or str(uuid.uuid4())\n        self.workflow_name = workflow_name\n        self.activities = activities or get_default_activities()\n        self.constitutional_hash = constitutional_hash\n        self.timeout_seconds = timeout_seconds\n        self.fail_closed = fail_closed\n\n        # Workflow state\n        self._status = WorkflowStatus.PENDING\n        self._context: Optional[WorkflowContext] = None\n        self._steps: List[WorkflowStep] = []\n        self._compensations: List[StepCompensation] = []\n        self._completed_steps: List[str] = []\n        self._failed_steps: List[str] = []\n        self._errors: List[str] = []\n        self._start_time: Optional[datetime] = None\n        self._output: Optional[Any] = None\n\n    @property\n    def status(self) -> WorkflowStatus:\n        \"\"\"Get current workflow status.\"\"\"\n        return self._status\n\n    @property\n    def context(self) -> Optional[WorkflowContext]:\n        \"\"\"Get workflow context.\"\"\"\n        return self._context\n\n    @abstractmethod\n    async def execute(self, input: Dict[str, Any]) -> WorkflowResult:\n        \"\"\"\n        Execute the workflow.\n\n        This is the main entry point for workflow execution.\n        Subclasses must implement this method.\n\n        Args:\n            input: Workflow input data\n\n        Returns:\n            WorkflowResult with execution outcome\n        \"\"\"\n        pass\n\n    async def run(self, input: Dict[str, Any]) -> WorkflowResult:\n        \"\"\"\n        Run the workflow with error handling and timeout.\n\n        This is the public interface for workflow execution.\n        Wraps execute() with timeout and error handling.\n\n        Args:\n            input: Workflow input data\n\n        Returns:\n            WorkflowResult with execution outcome\n        \"\"\"\n        self._start_time = datetime.now(timezone.utc)\n        self._status = WorkflowStatus.EXECUTING\n\n        # Initialize context\n        self._context = WorkflowContext(\n            workflow_id=self.workflow_id,\n            constitutional_hash=self.constitutional_hash,\n            metadata={\"input\": input},\n        )\n\n        try:\n            # Execute with timeout\n            result = await asyncio.wait_for(self.execute(input), timeout=self.timeout_seconds)\n            return result\n\n        except asyncio.TimeoutError:\n            self._status = WorkflowStatus.TIMED_OUT\n            self._errors.append(f\"Workflow timed out after {self.timeout_seconds}s\")\n\n            # Run compensations on timeout\n            await self._run_compensations()\n\n            return WorkflowResult.timeout(\n                workflow_id=self.workflow_id,\n                execution_time_ms=self._get_elapsed_time_ms(),\n                steps_completed=self._completed_steps,\n            )\n\n        except Exception as e:\n            self._status = WorkflowStatus.FAILED\n            self._errors.append(str(e))\n            logger.exception(f\"Workflow {self.workflow_id} failed: {e}\")\n\n            # Run compensations on failure\n            await self._run_compensations()\n\n            result = WorkflowResult.failure(\n                workflow_id=self.workflow_id,\n                errors=self._errors,\n                execution_time_ms=self._get_elapsed_time_ms(),\n                steps_completed=self._completed_steps,\n                steps_failed=self._failed_steps,\n                compensations_executed=[c.name for c in self._compensations if c],\n            )\n            return result\n        finally:\n            # Emit workflow metrics\n            if HAS_METRICS:\n                try:\n                    duration_s = self._get_elapsed_time_ms() / 1000.0\n                    metrics.WORKFLOW_EXECUTION_DURATION.labels(\n                        workflow_name=self.workflow_name, status=self._status.value\n                    ).observe(duration_s)\n\n                    metrics.WORKFLOW_EXECUTIONS_TOTAL.labels(\n                        workflow_name=self.workflow_name, status=self._status.value\n                    ).inc()\n                except Exception as me:\n                    logger.debug(f\"Failed to emit workflow metrics: {me}\")\n\n    async def validate_constitutional_hash(self, provided_hash: Optional[str] = None) -> bool:\n        \"\"\"\n        Validate constitutional hash.\n\n        This should be called at the beginning of every workflow.\n        Raises ConstitutionalHashMismatchError if validation fails\n        and fail_closed is True.\n\n        Args:\n            provided_hash: Hash to validate (uses workflow hash if not provided)\n\n        Returns:\n            True if validation passes\n\n        Raises:\n            ConstitutionalHashMismatchError: If validation fails and fail_closed\n        \"\"\"\n        hash_to_check = provided_hash or self.constitutional_hash\n\n        result = await self.activities.validate_constitutional_hash(\n            workflow_id=self.workflow_id,\n            provided_hash=hash_to_check,\n            expected_hash=CONSTITUTIONAL_HASH,\n        )\n\n        if not result[\"is_valid\"]:\n            error_msg = \"; \".join(result[\"errors\"])\n            self._errors.append(error_msg)\n\n            if self.fail_closed:\n                raise ConstitutionalHashMismatchError(\n                    expected_hash=CONSTITUTIONAL_HASH,\n                    actual_hash=hash_to_check,\n                )\n\n            logger.warning(\n                f\"Workflow {self.workflow_id}: Constitutional validation failed \"\n                f\"but fail_closed=False, continuing\"\n            )\n            return False\n\n        logger.info(f\"Workflow {self.workflow_id}: Constitutional validation passed\")\n        return True\n\n    async def run_step(\n        self,\n        step: WorkflowStep,\n        input: Dict[str, Any],\n    ) -> Any:\n        \"\"\"\n        Execute a single workflow step.\n\n        Handles:\n        - Constitutional validation (if required by step)\n        - Compensation registration (BEFORE execution)\n        - Retry logic with exponential backoff\n        - Timeout handling\n        - Result storage in context\n\n        Args:\n            step: WorkflowStep to execute\n            input: Step input data\n\n        Returns:\n            Step result\n\n        Raises:\n            Exception: If step fails after all retries\n        \"\"\"\n        # Constitutional check if required\n        if step.requires_constitutional_check:\n            await self.validate_constitutional_hash()\n\n        # CRITICAL: Register compensation BEFORE executing\n        if step.compensation:\n            self._compensations.append(step.compensation)\n\n        # Mark step as executing\n        step.mark_executing()\n\n        # Retry loop\n        last_error: Optional[Exception] = None\n\n        while step.can_retry():\n            try:\n                # Build step input\n                step_input = {\n                    \"workflow_id\": self.workflow_id,\n                    \"step_name\": step.name,\n                    \"attempt\": step.attempt_count,\n                    \"input\": input,\n                    \"context\": self._context.step_results if self._context else {},\n                    \"constitutional_hash\": self.constitutional_hash,\n                }\n\n                # Execute with timeout\n                result = await asyncio.wait_for(\n                    step.execute(step_input), timeout=step.timeout_seconds\n                )\n\n                # Success\n                step.mark_completed(result)\n                self._completed_steps.append(step.name)\n\n                if self._context:\n                    self._context.set_step_result(step.name, result)\n\n                logger.info(\n                    f\"Workflow {self.workflow_id}: Step '{step.name}' completed \"\n                    f\"(attempt {step.attempt_count}, {step.execution_time_ms:.2f}ms)\"\n                )\n\n                # Emit step metrics\n                if HAS_METRICS:\n                    try:\n                        metrics.WORKFLOW_STEP_DURATION.labels(\n                            workflow_name=self.workflow_name, step_name=step.name, status=\"success\"\n                        ).observe(step.execution_time_ms / 1000.0)\n                    except Exception as me:\n                        logger.debug(f\"Failed to emit step metrics: {me}\")\n\n                return result\n\n            except asyncio.TimeoutError:\n                last_error = TimeoutError(\n                    f\"Step '{step.name}' timed out after {step.timeout_seconds}s\"\n                )\n                step.mark_executing()  # Allow retry\n                logger.warning(\n                    f\"Workflow {self.workflow_id}: Step '{step.name}' timed out \"\n                    f\"(attempt {step.attempt_count})\"\n                )\n\n            except Exception as e:\n                last_error = e\n                step.mark_executing()  # Allow retry\n                logger.warning(\n                    f\"Workflow {self.workflow_id}: Step '{step.name}' failed \"\n                    f\"(attempt {step.attempt_count}): {e}\"\n                )\n\n            # Wait before retry\n            if step.can_retry():\n                if HAS_METRICS:\n                    try:\n                        metrics.WORKFLOW_STEP_RETRIES_TOTAL.labels(\n                            workflow_name=self.workflow_name, step_name=step.name\n                        ).inc()\n                    except Exception as me:\n                        logger.debug(f\"Failed to emit retry metrics: {me}\")\n                await asyncio.sleep(step.retry_delay_seconds)\n\n        # All retries exhausted\n        step.mark_failed(str(last_error))\n        self._failed_steps.append(step.name)\n        self._errors.append(f\"Step '{step.name}' failed: {last_error}\")\n\n        if not step.is_optional:\n            raise last_error or Exception(f\"Step '{step.name}' failed\")\n\n        logger.warning(\n            f\"Workflow {self.workflow_id}: Optional step '{step.name}' failed, continuing\"\n        )\n\n        # Emit step metrics for failure\n        if HAS_METRICS:\n            try:\n                metrics.WORKFLOW_STEP_DURATION.labels(\n                    workflow_name=self.workflow_name, step_name=step.name, status=\"failed\"\n                ).observe(step.execution_time_ms / 1000.0)\n            except Exception as me:\n                logger.debug(f\"Failed to emit step metrics: {me}\")\n\n        return None\n\n    def register_compensation(self, compensation: StepCompensation) -> None:\n        \"\"\"\n        Register a compensation action.\n\n        MUST be called BEFORE the operation that needs compensation.\n        Compensations are executed in LIFO order on failure.\n\n        Args:\n            compensation: Compensation to register\n        \"\"\"\n        self._compensations.append(compensation)\n\n    async def _run_compensations(self) -> List[str]:\n        \"\"\"\n        Execute compensations in reverse order (LIFO).\n\n        Returns:\n            List of successfully executed compensation names\n        \"\"\"\n        if not self._compensations:\n            return []\n\n        self._status = WorkflowStatus.COMPENSATING\n        executed = []\n        failed = []\n\n        logger.info(\n            f\"Workflow {self.workflow_id}: Running {len(self._compensations)} compensations\"\n        )\n\n        # Reverse order - LIFO\n        for compensation in reversed(self._compensations):\n            try:\n                compensation_input = {\n                    \"workflow_id\": self.workflow_id,\n                    \"compensation_name\": compensation.name,\n                    \"context\": self._context.step_results if self._context else {},\n                    \"idempotency_key\": compensation.idempotency_key\n                    or f\"{self.workflow_id}:{compensation.name}\",\n                }\n\n                success = False\n                for attempt in range(compensation.max_retries):\n                    try:\n                        result = await compensation.execute(compensation_input)\n                        if result:\n                            success = True\n                            break\n                    except Exception as e:\n                        logger.warning(\n                            f\"Workflow {self.workflow_id}: Compensation '{compensation.name}' \"\n                            f\"failed (attempt {attempt + 1}): {e}\"\n                        )\n                        await asyncio.sleep(compensation.retry_delay_seconds)\n\n                if success:\n                    executed.append(compensation.name)\n                    logger.info(\n                        f\"Workflow {self.workflow_id}: Compensation '{compensation.name}' completed\"\n                    )\n                else:\n                    failed.append(compensation.name)\n                    self._errors.append(f\"Compensation '{compensation.name}' failed\")\n\n            except Exception as e:\n                failed.append(compensation.name)\n                self._errors.append(f\"Compensation '{compensation.name}' error: {e}\")\n                logger.error(\n                    f\"Workflow {self.workflow_id}: Compensation '{compensation.name}' error: {e}\"\n                )\n\n        # Update status\n        if failed:\n            self._status = WorkflowStatus.PARTIALLY_COMPENSATED\n        else:\n            self._status = WorkflowStatus.COMPENSATED\n\n        return executed\n\n    async def complete(self, output: Any, record_audit: bool = True) -> WorkflowResult:\n        \"\"\"\n        Complete the workflow successfully.\n\n        Args:\n            output: Workflow output data\n            record_audit: Whether to record to audit trail\n\n        Returns:\n            WorkflowResult with success status\n        \"\"\"\n        self._status = WorkflowStatus.COMPLETED\n        self._output = output\n\n        audit_hash = None\n        if record_audit:\n            try:\n                audit_hash = await self.activities.record_audit(\n                    workflow_id=self.workflow_id,\n                    event_type=\"workflow_completed\",\n                    event_data={\n                        \"output\": output,\n                        \"steps_completed\": self._completed_steps,\n                        \"execution_time_ms\": self._get_elapsed_time_ms(),\n                    },\n                )\n            except Exception as e:\n                logger.warning(f\"Audit recording failed: {e}\")\n\n        return WorkflowResult.success(\n            workflow_id=self.workflow_id,\n            output=output,\n            execution_time_ms=self._get_elapsed_time_ms(),\n            steps_completed=self._completed_steps,\n            audit_hash=audit_hash,\n        )\n\n    def _get_elapsed_time_ms(self) -> float:\n        \"\"\"Get elapsed time since workflow start in milliseconds.\"\"\"\n        if self._start_time is None:\n            return 0.0\n        elapsed = datetime.now(timezone.utc) - self._start_time\n        return elapsed.total_seconds() * 1000\n\n\n__all__ = [\n    \"BaseWorkflow\",\n    \"WorkflowStatus\",\n]\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.127335",
  "last_updated": "2026-01-04T05:35:59.047390"
}