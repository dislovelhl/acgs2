{
  "file_path": "./examples/04-end-to-end-governance-workflow/src/audit_logger.py",
  "main_branch_history": [],
  "task_views": {
    "061-create-end-to-end-governance-workflow-examples-wit": {
      "task_id": "061-create-end-to-end-governance-workflow-examples-wit",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "#!/usr/bin/env python3\n\"\"\"\nAudit Logger Module for ACGS-2 Governance Workflow\n\nProvides structured audit logging to PostgreSQL with immutable, append-only\nstorage and comprehensive query capabilities for compliance and analysis.\n\nUsage:\n    from src.audit_logger import AuditLogger, AuditEntry, DatabaseConfig\n\n    config = DatabaseConfig(\n        host=\"localhost\",\n        port=5432,\n        database=\"governance_audit\",\n        user=\"postgres\",\n        password=\"your_password\"\n    )\n    logger = AuditLogger(config)\n\n    # Log a decision\n    entry = AuditEntry(\n        timestamp=datetime.now(timezone.utc),\n        action_type=\"read_data\",\n        requester_id=\"agent-001\",\n        resource=\"customer_data\",\n        decision=\"allow\",\n        risk_score=0.25\n    )\n    audit_id = logger.log_decision(entry)\n\n    # Query recent decisions\n    recent = logger.query_recent(limit=10)\n\nConstitutional Hash: cdd01ef066bc6cf2\n\"\"\"\n\nimport json\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import UTC, datetime\nfrom uuid import UUID, uuid4\n\nimport psycopg2\nfrom psycopg2 import extras\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\n# Custom exceptions for audit logger errors\nclass AuditLoggerError(Exception):\n    \"\"\"Base exception for audit logger errors\"\"\"\n    pass\n\n\nclass AuditDatabaseError(AuditLoggerError):\n    \"\"\"Raised when database operations fail\"\"\"\n    pass\n\n\nclass AuditConnectionError(AuditLoggerError):\n    \"\"\"Raised when database connection fails\"\"\"\n    pass\n\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Configuration for PostgreSQL database connection\"\"\"\n\n    host: str = \"localhost\"\n    port: int = 5432\n    database: str = \"governance_audit\"\n    user: str = \"postgres\"\n    password: str = \"\"\n    min_connections: int = 1\n    max_connections: int = 10\n\n\n@dataclass\nclass AuditEntry:\n    \"\"\"\n    Structured audit log entry for governance decisions.\n\n    This dataclass represents a single governance decision to be logged.\n    All fields are designed to match the audit_logs database schema.\n    \"\"\"\n\n    # Required fields\n    timestamp: datetime\n    action_type: str\n    requester_id: str\n    resource: str\n    decision: str  # \"allow\" or \"deny\"\n\n    # Optional fields with defaults\n    audit_id: UUID | None = None\n    environment: str | None = None\n    requester_type: str | None = None\n    resource_type: str | None = None\n    risk_score: float | None = None\n    risk_category: str | None = None\n    constitutional_valid: bool | None = None\n    constitutional_violations: list | None = None\n    hitl_required: bool = False\n    hitl_decision: dict | None = None\n    denial_reasons: list | None = None\n    compliance_tags: list | None = None\n    retention_days: int | None = None\n    log_level: str | None = None\n    metadata: dict | None = None\n\n    def __post_init__(self):\n        \"\"\"Validate and normalize the audit entry\"\"\"\n        # Generate UUID if not provided\n        if self.audit_id is None:\n            self.audit_id = uuid4()\n\n        # Ensure timestamp has timezone\n        if self.timestamp.tzinfo is None:\n            self.timestamp = self.timestamp.replace(tzinfo=UTC)\n\n        # Validate decision value\n        if self.decision not in (\"allow\", \"deny\"):\n            raise ValueError(f\"Invalid decision: {self.decision}. Must be 'allow' or 'deny'\")\n\n        # Validate risk score if provided\n        if self.risk_score is not None and not 0.0 <= self.risk_score <= 1.0:\n            raise ValueError(f\"Invalid risk_score: {self.risk_score}. Must be between 0.0 and 1.0\")\n\n\nclass AuditLogger:\n    \"\"\"\n    PostgreSQL-backed audit logger with query capabilities.\n\n    This class provides immutable, append-only audit logging with comprehensive\n    query methods for compliance reporting and analysis. Uses connection pooling\n    for efficient database access.\n\n    Attributes:\n        config: DatabaseConfig instance\n        pool: psycopg2 connection pool\n    \"\"\"\n\n    def __init__(self, config: DatabaseConfig):\n        \"\"\"\n        Initialize audit logger with database configuration.\n\n        Args:\n            config: DatabaseConfig instance with connection settings\n\n        Raises:\n            AuditConnectionError: If database connection fails\n        \"\"\"\n        self.config = config\n        self.pool = None\n\n        try:\n            # Create connection pool\n            self.pool = psycopg2.pool.SimpleConnectionPool(\n                config.min_connections,\n                config.max_connections,\n                host=config.host,\n                port=config.port,\n                database=config.database,\n                user=config.user,\n                password=config.password,\n                options=\"-c statement_timeout=30000\"  # 30 second timeout\n            )\n            logger.info(\n                f\"Audit logger initialized with connection pool \"\n                f\"(min={config.min_connections}, max={config.max_connections})\"\n            )\n\n            # Verify connection\n            if not self.health_check():\n                raise AuditConnectionError(\"Database health check failed\")\n\n        except psycopg2.Error as e:\n            logger.error(f\"Failed to initialize audit logger: {e}\")\n            raise AuditConnectionError(\n                f\"Cannot connect to database at {config.host}:{config.port}. \"\n                f\"Error: {e}\"\n            ) from e\n\n    def health_check(self) -> bool:\n        \"\"\"\n        Check database connectivity and health.\n\n        Returns:\n            True if database is healthy and accessible, False otherwise\n        \"\"\"\n        try:\n            conn = self.pool.getconn()\n            try:\n                with conn.cursor() as cur:\n                    cur.execute(\"SELECT 1\")\n                    result = cur.fetchone()\n                    is_healthy = result == (1,)\n\n                    if is_healthy:\n                        logger.debug(\"Database health check passed\")\n                    else:\n                        logger.warning(\"Database health check returned unexpected result\")\n\n                    return is_healthy\n            finally:\n                self.pool.putconn(conn)\n\n        except psycopg2.Error as e:\n            logger.error(f\"Database health check failed: {e}\")\n            return False\n\n    def log_decision(self, entry: AuditEntry) -> UUID:\n        \"\"\"\n        Log a governance decision to the audit trail (append-only).\n\n        This method inserts a new audit entry into the database. The entry\n        is immutable once written and cannot be modified or deleted.\n\n        Args:\n            entry: AuditEntry instance containing the decision details\n\n        Returns:\n            UUID of the logged audit entry\n\n        Raises:\n            AuditDatabaseError: If logging fails\n        \"\"\"\n        conn = self.pool.getconn()\n        try:\n            with conn.cursor() as cur:\n                # Prepare the insert statement\n                insert_sql = \"\"\"\n                INSERT INTO audit_logs (\n                    audit_id, timestamp, action_type, environment,\n                    requester_id, requester_type, resource, resource_type,\n                    decision, risk_score, risk_category,\n                    constitutional_valid, constitutional_violations,\n                    hitl_required, hitl_decision, denial_reasons,\n                    compliance_tags, retention_days, log_level, metadata\n                ) VALUES (\n                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n                )\n                RETURNING audit_id;\n                \"\"\"\n\n                # Convert lists and dicts to JSON\n                constitutional_violations_json = (\n                    json.dumps(entry.constitutional_violations)\n                    if entry.constitutional_violations else None\n                )\n                hitl_decision_json = (\n                    json.dumps(entry.hitl_decision)\n                    if entry.hitl_decision else None\n                )\n                denial_reasons_json = (\n                    json.dumps(entry.denial_reasons)\n                    if entry.denial_reasons else None\n                )\n                compliance_tags_json = (\n                    json.dumps(entry.compliance_tags)\n                    if entry.compliance_tags else None\n                )\n                metadata_json = (\n                    json.dumps(entry.metadata)\n                    if entry.metadata else None\n                )\n\n                # Execute insert\n                cur.execute(insert_sql, (\n                    entry.audit_id,\n                    entry.timestamp,\n                    entry.action_type,\n                    entry.environment,\n                    entry.requester_id,\n                    entry.requester_type,\n                    entry.resource,\n                    entry.resource_type,\n                    entry.decision,\n                    entry.risk_score,\n                    entry.risk_category,\n                    entry.constitutional_valid,\n                    constitutional_violations_json,\n                    entry.hitl_required,\n                    hitl_decision_json,\n                    denial_reasons_json,\n                    compliance_tags_json,\n                    entry.retention_days,\n                    entry.log_level,\n                    metadata_json\n                ))\n\n                # Get the returned audit_id\n                result = cur.fetchone()\n                audit_id = result[0]\n\n                # Commit the transaction\n                conn.commit()\n\n                logger.info(\n                    f\"Logged audit entry {audit_id}: \"\n                    f\"{entry.action_type} on {entry.resource} -> {entry.decision}\"\n                )\n\n                return audit_id\n\n        except psycopg2.Error as e:\n            conn.rollback()\n            logger.error(f\"Failed to log audit entry: {e}\")\n            raise AuditDatabaseError(f\"Failed to log decision: {e}\") from e\n        finally:\n            self.pool.putconn(conn)\n\n    def query_by_id(self, audit_id: UUID) -> AuditEntry | None:\n        \"\"\"\n        Retrieve audit entry by UUID.\n\n        Args:\n            audit_id: UUID of the audit entry to retrieve\n\n        Returns:\n            AuditEntry if found, None otherwise\n        \"\"\"\n        conn = self.pool.getconn()\n        try:\n            with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:\n                cur.execute(\n                    \"SELECT * FROM audit_logs WHERE audit_id = %s\",\n                    (audit_id,)\n                )\n                row = cur.fetchone()\n\n                if row:\n                    return self._row_to_entry(dict(row))\n                return None\n\n        except psycopg2.Error as e:\n            logger.error(f\"Failed to query audit entry by ID: {e}\")\n            raise AuditDatabaseError(f\"Query failed: {e}\") from e\n        finally:\n            self.pool.putconn(conn)\n\n    def query_by_requester(\n        self,\n        requester_id: str,\n        limit: int = 100\n    ) -> list[AuditEntry]:\n        \"\"\"\n        Retrieve audit entries for a specific requester.\n\n        Args:\n            requester_id: ID of the requester to search for\n            limit: Maximum number of entries to return (default: 100)\n\n        Returns:\n            List of AuditEntry instances, ordered by timestamp descending\n        \"\"\"\n        conn = self.pool.getconn()\n        try:\n            with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:\n                cur.execute(\n                    \"\"\"\n                    SELECT * FROM audit_logs\n                    WHERE requester_id = %s\n                    ORDER BY timestamp DESC\n                    LIMIT %s\n                    \"\"\",\n                    (requester_id, limit)\n                )\n                rows = cur.fetchall()\n\n                return [self._row_to_entry(dict(row)) for row in rows]\n\n        except psycopg2.Error as e:\n            logger.error(f\"Failed to query audit entries by requester: {e}\")\n            raise AuditDatabaseError(f\"Query failed: {e}\") from e\n        finally:\n            self.pool.putconn(conn)\n\n    def query_by_decision(\n        self,\n        decision: str,\n        start_time: datetime | None = None,\n        end_time: datetime | None = None,\n        limit: int = 100\n    ) -> list[AuditEntry]:\n        \"\"\"\n        Retrieve audit entries by decision type.\n\n        Args:\n            decision: Decision type to filter (\"allow\" or \"deny\")\n            start_time: Optional start timestamp for filtering\n            end_time: Optional end timestamp for filtering\n            limit: Maximum number of entries to return (default: 100)\n\n        Returns:\n            List of AuditEntry instances, ordered by timestamp descending\n        \"\"\"\n        conn = self.pool.getconn()\n        try:\n            with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:\n                # Build query dynamically based on time filters\n                query = \"SELECT * FROM audit_logs WHERE decision = %s\"\n                params = [decision]\n\n                if start_time:\n                    query += \" AND timestamp >= %s\"\n                    params.append(start_time)\n\n                if end_time:\n                    query += \" AND timestamp <= %s\"\n                    params.append(end_time)\n\n                query += \" ORDER BY timestamp DESC LIMIT %s\"\n                params.append(limit)\n\n                cur.execute(query, params)\n                rows = cur.fetchall()\n\n                return [self._row_to_entry(dict(row)) for row in rows]\n\n        except psycopg2.Error as e:\n            logger.error(f\"Failed to query audit entries by decision: {e}\")\n            raise AuditDatabaseError(f\"Query failed: {e}\") from e\n        finally:\n            self.pool.putconn(conn)\n\n    def query_recent(self, limit: int = 50) -> list[AuditEntry]:\n        \"\"\"\n        Retrieve the most recent audit entries.\n\n        Args:\n            limit: Maximum number of entries to return (default: 50)\n\n        Returns:\n            List of AuditEntry instances, ordered by timestamp descending\n        \"\"\"\n        conn = self.pool.getconn()\n        try:\n            with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:\n                cur.execute(\n                    \"\"\"\n                    SELECT * FROM audit_logs\n                    ORDER BY timestamp DESC\n                    LIMIT %s\n                    \"\"\",\n                    (limit,)\n                )\n                rows = cur.fetchall()\n\n                return [self._row_to_entry(dict(row)) for row in rows]\n\n        except psycopg2.Error as e:\n            logger.error(f\"Failed to query recent audit entries: {e}\")\n            raise AuditDatabaseError(f\"Query failed: {e}\") from e\n        finally:\n            self.pool.putconn(conn)\n\n    def get_statistics(\n        self,\n        start_time: datetime | None = None,\n        end_time: datetime | None = None\n    ) -> dict:\n        \"\"\"\n        Get audit statistics for compliance reporting and analysis.\n\n        Args:\n            start_time: Optional start timestamp for filtering\n            end_time: Optional end timestamp for filtering\n\n        Returns:\n            Dictionary containing statistics:\n            - total_decisions: Total number of decisions\n            - allow_count: Number of allowed decisions\n            - deny_count: Number of denied decisions\n            - allow_rate: Percentage of allowed decisions\n            - avg_risk_score: Average risk score\n            - hitl_count: Number of decisions requiring HITL\n            - hitl_rate: Percentage requiring HITL\n            - top_requesters: List of top requesters\n            - top_actions: List of most common action types\n        \"\"\"\n        conn = self.pool.getconn()\n        try:\n            with conn.cursor(cursor_factory=extras.RealDictCursor) as cur:\n                # Build time filter\n                time_filter = \"\"\n                params = []\n\n                if start_time:\n                    time_filter += \" WHERE timestamp >= %s\"\n                    params.append(start_time)\n\n                    if end_time:\n                        time_filter += \" AND timestamp <= %s\"\n                        params.append(end_time)\n                elif end_time:\n                    time_filter += \" WHERE timestamp <= %s\"\n                    params.append(end_time)\n\n                # Get overall statistics\n                cur.execute(\n                    f\"\"\"\n                    SELECT\n                        COUNT(*) as total_decisions,\n                        SUM(CASE WHEN decision = 'allow' THEN 1 ELSE 0 END) as allow_count,\n                        SUM(CASE WHEN decision = 'deny' THEN 1 ELSE 0 END) as deny_count,\n                        AVG(risk_score) as avg_risk_score,\n                        SUM(CASE WHEN hitl_required THEN 1 ELSE 0 END) as hitl_count\n                    FROM audit_logs\n                    {time_filter}\n                    \"\"\",\n                    params\n                )\n                stats = dict(cur.fetchone())\n\n                # Calculate rates\n                total = stats['total_decisions'] or 0\n                stats['allow_rate'] = (stats['allow_count'] / total * 100) if total > 0 else 0\n                stats['deny_rate'] = (stats['deny_count'] / total * 100) if total > 0 else 0\n                stats['hitl_rate'] = (stats['hitl_count'] / total * 100) if total > 0 else 0\n\n                # Get top requesters\n                cur.execute(\n                    f\"\"\"\n                    SELECT requester_id, COUNT(*) as count\n                    FROM audit_logs\n                    {time_filter}\n                    GROUP BY requester_id\n                    ORDER BY count DESC\n                    LIMIT 10\n                    \"\"\",\n                    params\n                )\n                stats['top_requesters'] = [dict(row) for row in cur.fetchall()]\n\n                # Get top action types\n                cur.execute(\n                    f\"\"\"\n                    SELECT action_type, COUNT(*) as count\n                    FROM audit_logs\n                    {time_filter}\n                    GROUP BY action_type\n                    ORDER BY count DESC\n                    LIMIT 10\n                    \"\"\",\n                    params\n                )\n                stats['top_actions'] = [dict(row) for row in cur.fetchall()]\n\n                return stats\n\n        except psycopg2.Error as e:\n            logger.error(f\"Failed to get audit statistics: {e}\")\n            raise AuditDatabaseError(f\"Statistics query failed: {e}\") from e\n        finally:\n            self.pool.putconn(conn)\n\n    def _row_to_entry(self, row: dict) -> AuditEntry:\n        \"\"\"\n        Convert a database row to an AuditEntry instance.\n\n        Args:\n            row: Dictionary representing a database row\n\n        Returns:\n            AuditEntry instance\n        \"\"\"\n        # Parse JSON fields\n        if isinstance(row.get('constitutional_violations'), str):\n            row['constitutional_violations'] = json.loads(row['constitutional_violations'])\n        if isinstance(row.get('hitl_decision'), str):\n            row['hitl_decision'] = json.loads(row['hitl_decision'])\n        if isinstance(row.get('denial_reasons'), str):\n            row['denial_reasons'] = json.loads(row['denial_reasons'])\n        if isinstance(row.get('compliance_tags'), str):\n            row['compliance_tags'] = json.loads(row['compliance_tags'])\n        if isinstance(row.get('metadata'), str):\n            row['metadata'] = json.loads(row['metadata'])\n\n        # Remove database-specific fields\n        row.pop('created_at', None)\n        row.pop('updated_at', None)\n\n        return AuditEntry(**row)\n\n    def close(self) -> None:\n        \"\"\"\n        Close all database connections and release resources.\n\n        Call this when you're done using the audit logger to ensure\n        proper cleanup of the connection pool.\n        \"\"\"\n        if self.pool:\n            self.pool.closeall()\n            logger.info(\"Audit logger connection pool closed\")\n\n    def __enter__(self) -> \"AuditLogger\":\n        \"\"\"Support for context manager (with statement)\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Cleanup when exiting context manager\"\"\"\n        self.close()\n",
        "timestamp": "2026-01-04T05:35:49.744738"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "061-create-end-to-end-governance-workflow-examples-wit",
        "description": "Create comprehensive examples demonstrating complete governance workflows from policy creation through enforcement to audit logging, targeting Enterprise DevOps/MLOps engineers deploying AI systems. The examples will show: defining constitutional policies, enforcing them on agent actions, handling HITL approvals, and auditing decisions.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:07.555741",
  "last_updated": "2026-01-04T05:35:49.807268"
}