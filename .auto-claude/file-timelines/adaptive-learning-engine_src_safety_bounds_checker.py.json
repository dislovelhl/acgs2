{
  "file_path": "adaptive-learning-engine/src/safety/bounds_checker.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nAdaptive Learning Engine - Safety Bounds Checker\nConstitutional Hash: cdd01ef066bc6cf2\n\nSafety bounds checking to prevent model degradation through\naccuracy threshold validation and circuit breaker patterns.\n\nCIRCUIT BREAKER PATTERN FOR ML SAFETY\n======================================\n\nThis module implements a circuit breaker pattern adapted from reliability engineering\nfor machine learning safety. Circuit breakers prevent cascading failures by detecting\nproblematic conditions and temporarily halting operations before they cause wider damage.\n\nTraditional Circuit Breaker (Electrical/Software):\n- Monitors for failures (errors, timeouts, resource exhaustion)\n- Opens circuit after threshold to prevent cascading failures\n- Allows system recovery before resuming operations\n- Three states: CLOSED (normal), OPEN (failing), HALF-OPEN (testing recovery)\n\nML Safety Circuit Breaker (This Implementation):\n- Monitors model accuracy and degradation\n- Pauses learning after consecutive accuracy failures\n- Prevents bad model updates from reaching production\n- Four states: OK, WARNING, CRITICAL, PAUSED (see SafetyStatus enum)\n\nWhy Circuit Breakers Are Critical for ML Safety:\n1. **Prevents Cascading Degradation**: A bad model update could corrupt future training,\n   leading to progressive degradation. Circuit breaker halts learning before this cascade.\n\n2. **Fail-Safe for Governance**: In access control/governance, a degraded model could\n   grant improper permissions or deny legitimate access. The circuit breaker ensures\n   the model maintains minimum quality standards.\n\n3. **Observable Failure States**: Instead of silent degradation, the circuit breaker\n   provides clear state transitions (OK \u2192 WARNING \u2192 CRITICAL \u2192 PAUSED) that can be\n   monitored and alerted on.\n\n4. **Human-in-the-Loop**: By pausing on critical failures, the circuit breaker creates\n   an opportunity for manual review before resuming operations. This is essential for\n   safety-critical applications where automated recovery is risky.\n\nFAILURE MODES AND DETECTION\n============================\n\nThis checker detects multiple failure modes to provide defense-in-depth:\n\n1. **FAILED_ACCURACY**: Model accuracy below absolute threshold\n   - Indicates overall model performance is unacceptable\n   - Threshold: accuracy_threshold (default 0.85 = 85%)\n   - Example: Model drops to 80% accuracy after bad training batch\n\n2. **FAILED_DEGRADATION**: Significant accuracy drop from previous check\n   - Detects sudden performance regression even if still above threshold\n   - Threshold: degradation_threshold (default 0.05 = 5% drop)\n   - Example: Model goes from 90% \u2192 84% accuracy (6% drop)\n\n3. **FAILED_DRIFT**: Model failing due to distribution drift (reserved for integration)\n   - Reserved for future integration with DriftDetector\n   - Would indicate data distribution has shifted beyond model's capability\n\n4. **SKIPPED_COLD_START**: Safety check bypassed during model initialization\n   - Model has insufficient samples for meaningful accuracy estimation\n   - Threshold: min_samples_for_check (default 100 samples)\n\n5. **SKIPPED_INSUFFICIENT_DATA**: Safety check bypassed due to lack of validation data\n   - No validation data provided and model has no internal accuracy metric\n\nSTATE MACHINE AND TRANSITIONS\n==============================\n\nThe circuit breaker implements a state machine that escalates based on consecutive failures:\n\nState: OK (CLOSED CIRCUIT)\n- All safety checks passing\n- Learning proceeds normally\n- Consecutive failures: 0\n\n    \u2193 (First accuracy failure)\n\nState: WARNING (DEGRADED)\n- Some safety checks failing but below consecutive limit\n- Learning continues but system is on alert\n- Consecutive failures: 1 to (limit - 1)\n- Example: 1/3 or 2/3 failures\n\n    \u2193 (Consecutive failures reach limit)\n\nState: CRITICAL \u2192 PAUSED (OPEN CIRCUIT)\n- Too many consecutive failures detected\n- Learning automatically paused to prevent further degradation\n- Consecutive failures: \u2265 limit (default 3)\n- Requires manual intervention (force_resume) to restart\n\n    \u2193 (Manual intervention + passing check)\n\nState: OK (CIRCUIT RESET)\n- Manual reset or passing check after pause\n- Consecutive failures counter reset to 0\n- Learning resumes\n\nWhy Consecutive Failures Matter:\n- Single failure could be transient (bad batch, outlier data)\n- Consecutive failures indicate systematic problem\n- Default limit of 3 balances noise tolerance vs. safety\n- Similar to TCP retransmit (3 strikes before circuit opens)\n\nGOVERNANCE-SPECIFIC CONSIDERATIONS\n===================================\n\nFor access control and governance applications:\n\n1. **Conservative Thresholds**: Default 85% accuracy may seem low for some ML tasks,\n   but governance systems often deal with:\n   - Imbalanced data (rare access patterns)\n   - High cost of false positives (denied legitimate access)\n   - High cost of false negatives (granted improper access)\n\n2. **Safety-Critical Operation**: Unlike recommendation systems where wrong predictions\n   are annoying, governance errors can:\n   - Violate compliance regulations (GDPR, SOC2, etc.)\n   - Expose sensitive data to unauthorized users\n   - Block critical business operations\n\n3. **Audit Trail**: Circuit breaker state transitions create observable events for\n   compliance auditing and incident investigation.\n\nReferences:\n- Michael Nygard, \"Release It! Design and Deploy Production-Ready Software\" (2007)\n  Chapter on Circuit Breaker pattern for fault tolerance\n- Martin Fowler, \"CircuitBreaker\" (2014)\n  https://martinfowler.com/bliki/CircuitBreaker.html\n- Netflix Hystrix (implementation reference)\n  https://github.com/Netflix/Hystrix/wiki/How-it-Works#CircuitBreaker\n- Sculley et al., \"Hidden Technical Debt in Machine Learning Systems\" (2015)\n  NIPS paper on ML system failure modes and safety\n\nUsage Example with Circuit Breaker Pattern:\n    # Initialize with circuit breaker thresholds\n    checker = SafetyBoundsChecker(\n        accuracy_threshold=0.85,           # Minimum acceptable accuracy\n        consecutive_failures_limit=3,      # Open circuit after 3 failures\n        enable_auto_pause=True,            # Auto-pause on critical failure\n    )\n\n    # Register callbacks for circuit breaker events\n    checker.register_alert_callback(lambda alert:\n        notify_ops(f\"Circuit breaker alert: {alert.severity}\"))\n    checker.register_pause_callback(lambda:\n        notify_ops(\"CRITICAL: Learning circuit OPENED (paused)\"))\n    checker.register_resume_callback(lambda:\n        notify_ops(\"Learning circuit CLOSED (resumed)\"))\n\n    # Check model before each update (circuit breaker check)\n    result = checker.check_model(new_model)\n\n    # State transitions based on check result:\n    # OK \u2192 WARNING (first failure)\n    # WARNING \u2192 WARNING (second failure)\n    # WARNING \u2192 CRITICAL/PAUSED (third failure - circuit opens)\n\n    if result.safety_status == SafetyStatus.PAUSED:\n        # Circuit is open - learning halted\n        logger.critical(\"Circuit breaker opened - manual intervention required\")\n        # Investigate root cause, fix data/model issues\n        # Manually reset: checker.force_resume()\n    elif result.safety_status == SafetyStatus.WARNING:\n        # Circuit degraded but still closed\n        logger.warning(f\"Circuit breaker warning: {result.consecutive_failures}/3 failures\")\n    elif result.passed:\n        # Circuit closed and healthy\n        manager.swap_model(new_model)\n\"\"\"\n\nimport logging\nimport threading\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nlogger = logging.getLogger(__name__)\n\n\nclass SafetyStatus(Enum):\n    \"\"\"Current safety status of the learning system.\n\n    These states represent the circuit breaker state machine:\n\n    OK (CLOSED CIRCUIT):\n        - All safety checks are passing\n        - Model accuracy is above threshold\n        - No recent degradation detected\n        - Consecutive failures: 0\n        - Learning: ENABLED\n        - Analogy: Electrical circuit is closed, current flows normally\n\n    WARNING (DEGRADED CIRCUIT):\n        - Some safety checks are failing\n        - Below consecutive failure limit\n        - System is on alert but still operational\n        - Consecutive failures: 1 to (limit - 1)\n        - Learning: ENABLED but monitored\n        - Analogy: Circuit is experiencing intermittent issues but not yet tripped\n        - Example: Model failed 1 or 2 checks but not yet at limit of 3\n\n    CRITICAL (CIRCUIT TRIPPING):\n        - Consecutive failure limit reached\n        - Circuit breaker is opening to prevent cascading failures\n        - Transitional state before PAUSED\n        - Consecutive failures: \u2265 limit\n        - Learning: BEING DISABLED\n        - Analogy: Circuit breaker is actively tripping\n\n    PAUSED (OPEN CIRCUIT):\n        - Learning has been halted by circuit breaker\n        - Too many consecutive failures detected\n        - Requires manual intervention to resume\n        - Consecutive failures: \u2265 limit\n        - Learning: DISABLED\n        - Analogy: Circuit breaker is open, no current flows\n        - Recovery: Requires force_resume() or manual reset\n\n    State Transition Flow:\n        OK \u2192 WARNING \u2192 CRITICAL \u2192 PAUSED \u2192 (manual reset) \u2192 OK\n\n    The circuit breaker prevents cascading ML failures by halting learning\n    when model quality degrades below acceptable levels. This is critical for\n    governance systems where bad model updates could grant improper permissions\n    or deny legitimate access.\n    \"\"\"\n\n    OK = \"ok\"  # All checks passing (CLOSED CIRCUIT)\n    WARNING = \"warning\"  # Below threshold but still learning (DEGRADED CIRCUIT)\n    PAUSED = \"paused\"  # Learning paused due to consecutive failures (OPEN CIRCUIT)\n    CRITICAL = \"critical\"  # Circuit breaker tripping, requires manual intervention\n\n\nclass CheckResult(Enum):\n    \"\"\"Result of a single safety check.\n\n    These represent different failure modes the circuit breaker monitors:\n\n    PASSED:\n        - All safety checks passed\n        - Model accuracy \u2265 threshold\n        - No significant degradation detected\n        - Resets consecutive failure counter to 0\n        - Circuit breaker closes (or stays closed)\n\n    FAILED_ACCURACY:\n        - Model accuracy below absolute threshold\n        - Check: current_accuracy < accuracy_threshold\n        - Default threshold: 0.85 (85% accuracy)\n        - Indicates overall model performance is unacceptable\n        - Example: Model accuracy drops to 80% after bad training batch\n        - Increments consecutive failure counter\n        - May trigger circuit breaker if consecutive failures \u2265 limit\n\n    FAILED_DEGRADATION:\n        - Significant accuracy drop from previous check\n        - Check: (previous_accuracy - current_accuracy) > degradation_threshold\n        - Default threshold: 0.05 (5% drop)\n        - Detects sudden performance regression even if above absolute threshold\n        - Example: Model goes from 90% \u2192 84% accuracy (6% drop)\n        - Prevents gradual model rot from going unnoticed\n        - Increments consecutive failure counter\n        - More sensitive than FAILED_ACCURACY for detecting regressions\n\n    FAILED_DRIFT:\n        - Model failing due to distribution drift (reserved for future integration)\n        - Would integrate with DriftDetector to detect when data distribution\n          has shifted beyond the model's capability to adapt\n        - Example: Governance policies fundamentally changed, requiring retraining\n        - Currently not implemented but reserved for future use\n\n    SKIPPED_COLD_START:\n        - Safety check bypassed during model initialization\n        - Model has insufficient samples for meaningful accuracy estimation\n        - Check: sample_count < min_samples_for_check (default 100)\n        - Accuracy metrics are unreliable with few samples (high variance)\n        - Does NOT count as pass or fail (neutral result)\n        - Does NOT reset consecutive failure counter\n        - Example: Model trained on only 50 samples cannot provide reliable accuracy\n\n    SKIPPED_INSUFFICIENT_DATA:\n        - Safety check bypassed due to lack of validation data\n        - No validation_data provided and model has no internal accuracy metric\n        - Does NOT count as pass or fail (neutral result)\n        - Does NOT reset consecutive failure counter\n        - Indicates configuration issue that should be addressed\n\n    Failure Mode Detection Strategy (Defense-in-Depth):\n\n    The checker uses multiple failure modes to provide robust safety:\n    1. FAILED_ACCURACY catches absolute performance issues\n    2. FAILED_DEGRADATION catches relative regressions\n    3. Both contribute to consecutive failure counter\n    4. Circuit breaker trips on ANY consecutive failures (regardless of type)\n\n    This multi-layered approach ensures the circuit breaker catches both:\n    - Sudden crashes in model quality (FAILED_DEGRADATION)\n    - Gradual decay below acceptable levels (FAILED_ACCURACY)\n    \"\"\"\n\n    PASSED = \"passed\"\n    FAILED_ACCURACY = \"failed_accuracy\"\n    FAILED_DEGRADATION = \"failed_degradation\"\n    FAILED_DRIFT = \"failed_drift\"\n    SKIPPED_COLD_START = \"skipped_cold_start\"\n    SKIPPED_INSUFFICIENT_DATA = \"skipped_insufficient_data\"\n\n\n@dataclass\nclass SafetyCheckResult:\n    \"\"\"Result of a safety bounds check.\"\"\"\n\n    passed: bool\n    result: CheckResult\n    current_accuracy: float\n    threshold: float\n    message: str\n    consecutive_failures: int\n    safety_status: SafetyStatus\n    timestamp: float = field(default_factory=time.time)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"passed\": self.passed,\n            \"result\": self.result.value,\n            \"current_accuracy\": self.current_accuracy,\n            \"threshold\": self.threshold,\n            \"message\": self.message,\n            \"consecutive_failures\": self.consecutive_failures,\n            \"safety_status\": self.safety_status.value,\n            \"timestamp\": self.timestamp,\n            \"metadata\": self.metadata,\n        }\n\n\n@dataclass\nclass SafetyAlert:\n    \"\"\"Alert generated when safety bounds are violated.\"\"\"\n\n    severity: str  # \"warning\", \"critical\"\n    message: str\n    consecutive_failures: int\n    current_accuracy: float\n    threshold: float\n    action_taken: str  # \"none\", \"paused_learning\", \"alert_sent\"\n    timestamp: float = field(default_factory=time.time)\n    context: Dict[str, Any] = field(default_factory=dict)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"severity\": self.severity,\n            \"message\": self.message,\n            \"consecutive_failures\": self.consecutive_failures,\n            \"current_accuracy\": self.current_accuracy,\n            \"threshold\": self.threshold,\n            \"action_taken\": self.action_taken,\n            \"timestamp\": self.timestamp,\n            \"context\": self.context,\n        }\n\n\n@dataclass\nclass SafetyMetrics:\n    \"\"\"Metrics for safety bounds checking.\"\"\"\n\n    total_checks: int\n    passed_checks: int\n    failed_checks: int\n    consecutive_failures: int\n    max_consecutive_failures: int\n    times_paused: int\n    times_resumed: int\n    current_status: SafetyStatus\n    last_check_time: Optional[float]\n    last_failure_time: Optional[float]\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"total_checks\": self.total_checks,\n            \"passed_checks\": self.passed_checks,\n            \"failed_checks\": self.failed_checks,\n            \"consecutive_failures\": self.consecutive_failures,\n            \"max_consecutive_failures\": self.max_consecutive_failures,\n            \"times_paused\": self.times_paused,\n            \"times_resumed\": self.times_resumed,\n            \"current_status\": self.current_status.value,\n            \"last_check_time\": self.last_check_time,\n            \"last_failure_time\": self.last_failure_time,\n        }\n\n\nclass SafetyBoundsChecker:\n    \"\"\"Safety bounds checker to prevent model degradation.\n\n    Implements circuit breaker pattern for online learning:\n    - Checks model accuracy against configurable threshold\n    - Tracks consecutive failures\n    - Pauses learning after too many consecutive failures\n    - Provides alert callbacks for monitoring integration\n\n    The safety bounds prevent model degradation by:\n    1. Rejecting updates when accuracy drops below threshold\n    2. Pausing learning when consecutive failures exceed limit\n    3. Alerting operators for manual intervention\n\n    Example usage:\n        # Initialize checker with thresholds\n        checker = SafetyBoundsChecker(\n            accuracy_threshold=0.85,\n            consecutive_failures_limit=3,\n        )\n\n        # Check model before update\n        result = checker.check_model(model)\n        if result.passed:\n            # Proceed with model update\n            model_manager.swap_model(new_model)\n        else:\n            # Handle rejection\n            logger.warning(f\"Safety check failed: {result.message}\")\n\n        # Register alert callback\n        checker.register_alert_callback(lambda alert: notify_ops(alert))\n\n    Integration with ModelManager:\n        async def safe_model_update(\n            manager: ModelManager,\n            checker: SafetyBoundsChecker,\n            new_model: OnlineLearner,\n        ) -> SwapResult:\n            result = checker.check_model(new_model)\n            if not result.passed:\n                if result.safety_status == SafetyStatus.PAUSED:\n                    manager.pause_learning()\n                return SwapResult(\n                    status=SwapStatus.REJECTED_SAFETY,\n                    message=result.message,\n                )\n            return await manager.swap_model(new_model)\n    \"\"\"\n\n    def __init__(\n        self,\n        accuracy_threshold: float = 0.85,\n        consecutive_failures_limit: int = 3,\n        min_samples_for_check: int = 100,\n        enable_auto_pause: bool = True,\n        degradation_threshold: float = 0.05,\n    ) -> None:\n        \"\"\"Initialize the safety bounds checker.\n\n        Args:\n            accuracy_threshold: Minimum accuracy required (0.0-1.0).\n            consecutive_failures_limit: Number of consecutive failures before pause.\n            min_samples_for_check: Minimum samples before safety check is active.\n            enable_auto_pause: If True, automatically pause learning on limit breach.\n            degradation_threshold: Maximum allowed accuracy drop per update.\n        \"\"\"\n        # Validate thresholds\n        if not 0.0 <= accuracy_threshold <= 1.0:\n            raise ValueError(\n                f\"accuracy_threshold must be between 0 and 1, got {accuracy_threshold}\"\n            )\n        if consecutive_failures_limit < 1:\n            raise ValueError(\n                f\"consecutive_failures_limit must be >= 1, got {consecutive_failures_limit}\"\n            )\n        if min_samples_for_check < 0:\n            raise ValueError(f\"min_samples_for_check must be >= 0, got {min_samples_for_check}\")\n        if not 0.0 <= degradation_threshold <= 1.0:\n            raise ValueError(\n                f\"degradation_threshold must be between 0 and 1, got {degradation_threshold}\"\n            )\n\n        self.accuracy_threshold = accuracy_threshold\n        self.consecutive_failures_limit = consecutive_failures_limit\n        self.min_samples_for_check = min_samples_for_check\n        self.enable_auto_pause = enable_auto_pause\n        self.degradation_threshold = degradation_threshold\n\n        # Thread safety\n        self._lock = threading.RLock()\n\n        # State tracking\n        self._consecutive_failures = 0\n        self._status = SafetyStatus.OK\n        self._last_accuracy: Optional[float] = None\n\n        # Metrics\n        self._total_checks = 0\n        self._passed_checks = 0\n        self._failed_checks = 0\n        self._max_consecutive_failures = 0\n        self._times_paused = 0\n        self._times_resumed = 0\n        self._last_check_time: Optional[float] = None\n        self._last_failure_time: Optional[float] = None\n\n        # Alert history\n        self._alert_history: List[SafetyAlert] = []\n        self._max_alert_history = 100\n\n        # Callbacks\n        self._alert_callbacks: List[Callable[[SafetyAlert], None]] = []\n        self._pause_callbacks: List[Callable[[], None]] = []\n        self._resume_callbacks: List[Callable[[], None]] = []\n\n        logger.info(\n            \"SafetyBoundsChecker initialized\",\n            extra={\n                \"accuracy_threshold\": accuracy_threshold,\n                \"consecutive_failures_limit\": consecutive_failures_limit,\n                \"min_samples_for_check\": min_samples_for_check,\n                \"enable_auto_pause\": enable_auto_pause,\n            },\n        )\n\n    def check_model(\n        self,\n        model: Any,\n        validation_data: Optional[List[Tuple[Dict[str, Any], int]]] = None,\n    ) -> SafetyCheckResult:\n        \"\"\"Check if a model passes safety bounds.\n\n        Validates model accuracy against threshold and checks for degradation.\n        If validation_data is provided, uses it to calculate accuracy.\n        Otherwise, uses the model's internal accuracy metric.\n\n        This is the main entry point for circuit breaker safety checks. It implements\n        defense-in-depth by checking multiple failure modes:\n        1. Cold start bypass (insufficient data for meaningful check)\n        2. Absolute accuracy threshold (FAILED_ACCURACY)\n        3. Relative degradation threshold (FAILED_DEGRADATION)\n\n        Args:\n            model: The OnlineLearner model to check.\n            validation_data: Optional list of (features, label) tuples for validation.\n\n        Returns:\n            SafetyCheckResult with pass/fail status and details.\n        \"\"\"\n        with self._lock:\n            self._total_checks += 1\n            self._last_check_time = time.time()\n\n            # FAILURE MODE: SKIPPED_COLD_START (Cold Start Safety Bypass)\n            # ============================================================\n            #\n            # During model initialization (cold start), the model has insufficient training\n            # samples for reliable accuracy estimation. Safety checks are bypassed during\n            # this phase to prevent false alarms from statistical noise and premature\n            # circuit breaker activation.\n            #\n            # WHY SKIP SAFETY CHECKS DURING COLD START?\n            # ==========================================\n            #\n            # 1. STATISTICAL UNRELIABILITY:\n            #    With few samples (n < 100), accuracy estimates have high variance and\n            #    are not statistically significant. Random fluctuations dominate signal.\n            #\n            #    Standard Error (SE) of Accuracy Estimate:\n            #      SE = sqrt(p * (1-p) / n)\n            #\n            #    Where:\n            #      - p = true accuracy (unknown, assume ~0.85 for governance models)\n            #      - n = sample count\n            #      - SE = standard error of the accuracy estimate\n            #\n            #    Cold Start Examples (n < 100):\n            #      - n=10:  SE = sqrt(0.85*0.15/10)  \u2248 0.113 (\u00b111.3% margin of error!)\n            #      - n=25:  SE = sqrt(0.85*0.15/25)  \u2248 0.071 (\u00b17.1% margin of error)\n            #      - n=50:  SE = sqrt(0.85*0.15/50)  \u2248 0.050 (\u00b15.0% margin of error)\n            #\n            #    With n=10, measured 75% accuracy could represent true accuracy anywhere\n            #    from 64% to 86% (75% \u00b1 11.3% at 95% CI). This massive uncertainty makes\n            #    safety checks meaningless - failures would be mostly random noise.\n            #\n            # 2. RISK OF FALSE CIRCUIT BREAKER TRIPS:\n            #    If safety checks run during cold start, high variance could trigger\n            #    consecutive failures purely from statistical noise, causing the circuit\n            #    breaker to open (PAUSED state) before the model has a chance to stabilize.\n            #\n            #    Scenario Without Cold Start Skip:\n            #      Check 1 (n=20): Measured 78% accuracy (SE\u00b19%) \u2192 FAILED_ACCURACY\n            #      Check 2 (n=40): Measured 81% accuracy (SE\u00b16%) \u2192 FAILED_ACCURACY\n            #      Check 3 (n=60): Measured 83% accuracy (SE\u00b15%) \u2192 FAILED_ACCURACY\n            #      Result: Circuit breaker OPENS (3 consecutive failures)\n            #      Reality: Model was actually improving (78%\u219281%\u219283%) but high variance\n            #               caused all checks to fail threshold (85%). Model never got\n            #               chance to reach statistical stability.\n            #\n            #    With Cold Start Skip:\n            #      Check 1 (n=20): SKIPPED_COLD_START (neutral, doesn't count as failure)\n            #      Check 2 (n=40): SKIPPED_COLD_START (neutral, doesn't count as failure)\n            #      Check 3 (n=60): SKIPPED_COLD_START (neutral, doesn't check yet)\n            #      Check 4 (n=120): Measured 87% accuracy \u2192 PASSED (circuit stays closed)\n            #      Result: Model reaches statistical stability before safety validation begins\n            #\n            # 3. COORDINATION WITH MODEL WARMING PHASE:\n            #    OnlineLearner transitions through states: COLD_START \u2192 WARMING \u2192 ACTIVE\n            #    The WARMING state persists until min_training_samples=1000 is reached,\n            #    during which:\n            #      - StandardScaler statistics are stabilizing (mean/variance estimates)\n            #      - Model weights are converging through gradient descent\n            #      - Predictions are unreliable and should not be used for production\n            #\n            #    SafetyBoundsChecker's min_samples_for_check=100 is deliberately set\n            #    LOWER than OnlineLearner's min_training_samples=1000 to provide early\n            #    safety validation during the WARMING phase:\n            #\n            #    Sample Count Timeline:\n            #      n=0-99:    COLD_START state, safety checks SKIPPED (too unstable)\n            #      n=100-999: WARMING state, safety checks ACTIVE (early validation)\n            #      n=1000+:   ACTIVE state, safety checks ACTIVE (production validation)\n            #\n            #    This staged approach allows safety checks to begin validating model\n            #    quality during the warming phase (100-999 samples) while still\n            #    preventing premature failures during extreme cold start (0-99 samples).\n            #\n            # WHY MIN_SAMPLES_FOR_CHECK = 100 SPECIFICALLY?\n            # ==============================================\n            #\n            # The threshold of 100 samples balances statistical significance with\n            # responsive safety validation. It's chosen based on several factors:\n            #\n            # 1. STATISTICAL SIGNIFICANCE (Primary Justification):\n            #\n            #    Standard Error at n=100:\n            #      SE = sqrt(0.85 * 0.15 / 100) = sqrt(0.1275 / 100) = sqrt(0.001275) \u2248 0.036\n            #\n            #    This means at 100 samples:\n            #      - Standard error: \u00b13.6%\n            #      - 95% Confidence Interval: \u00b11.96 * 0.036 \u2248 \u00b17% (about \u00b10.07)\n            #      - Measured 85% accuracy likely represents true accuracy in [78%, 92%] range\n            #\n            #    Comparison to Accuracy Threshold (85%):\n            #      - Threshold: 85% minimum required accuracy\n            #      - SE at n=100: \u00b13.6%\n            #      - If measured accuracy is exactly 85%, true accuracy is likely 81-89%\n            #      - This provides reasonable confidence that measured accuracy reflects reality\n            #\n            #    With n=100, the standard error (3.6%) is small enough that:\n            #      - True failures (model actually below 85%) will be detected reliably\n            #      - False alarms from noise are reduced (but circuit breaker's consecutive\n            #        failure requirement provides additional filtering)\n            #\n            # 2. CENTRAL LIMIT THEOREM CONVERGENCE:\n            #\n            #    The Central Limit Theorem (CLT) states that sample means approach a\n            #    normal distribution as sample size increases. For binary classification\n            #    accuracy (Bernoulli trials):\n            #      - n=30: CLT starts to apply (rough approximation)\n            #      - n=50: Better convergence to normal distribution\n            #      - n=100: Strong convergence, reliable confidence intervals\n            #\n            #    At n=100, we can reliably use normal distribution approximations for\n            #    confidence intervals, making statistical tests (like comparing accuracy\n            #    to threshold) mathematically sound.\n            #\n            # 3. TRADE-OFF: EARLY WARNING VS. FALSE ALARMS\n            #\n            #    Lower Threshold (e.g., n=50):\n            #      Advantages:\n            #        - Earlier safety validation (detects problems sooner)\n            #        - Shorter cold start period (safety checks begin at 50 samples)\n            #      Disadvantages:\n            #        - SE \u2248 5% (higher variance, more noise)\n            #        - More false alarms during model initialization\n            #        - Circuit breaker may trip on statistical noise (despite consecutive\n            #          failure filtering, 3 consecutive noisy checks could still fail)\n            #      Use case: High-risk governance where immediate validation is critical\n            #\n            #    Higher Threshold (e.g., n=200):\n            #      Advantages:\n            #        - SE \u2248 2.5% (lower variance, more reliable estimates)\n            #        - Fewer false alarms (very stable accuracy measurement)\n            #        - Strong statistical confidence in threshold violations\n            #      Disadvantages:\n            #        - Delayed safety validation (checks don't start until 200 samples)\n            #        - Model could degrade during samples 100-199 without detection\n            #        - Longer exposure window if model is fundamentally broken\n            #      Use case: Low-risk systems where stability > responsiveness\n            #\n            #    Optimal n=100:\n            #      - Balances responsiveness (checks start at 100 samples) with reliability\n            #        (SE \u2248 3.6% is acceptable for safety decisions with circuit breaker)\n            #      - SE of 3.6% means ~16% false alarm rate per check, but consecutive\n            #        failure requirement (3 strikes) reduces cumulative false alarm rate\n            #        to 0.16^3 \u2248 0.4% (highly unlikely to trip circuit breaker on noise)\n            #      - Detects real issues within 100-300 samples (1-3 checks if consecutive)\n            #        while maintaining statistical rigor\n            #\n            # 4. ALIGNMENT WITH DOMAIN STANDARDS:\n            #\n            #    Machine Learning Evaluation Best Practices:\n            #      - Scikit-learn cross-validation typically uses k=5 or k=10 folds\n            #      - With 1000-sample dataset, each fold has 100-200 test samples\n            #      - n=100 aligns with minimum fold size for reliable evaluation\n            #\n            #    Statistical Testing Standards:\n            #      - Psychology/social sciences: n=30 minimum (CLT approximation)\n            #      - Medical trials: n=100+ for Phase II studies (preliminary efficacy)\n            #      - A/B testing: n=100+ per variant for basic significance tests\n            #      - n=100 is widely recognized as \"sufficient for initial analysis\"\n            #\n            #    Governance-Specific Considerations:\n            #      - With 10% minority class (rare access patterns), n=100 provides\n            #        ~10 samples from minority class (borderline sufficient)\n            #      - Below n=100, minority class may have <5 samples (unreliable metrics)\n            #      - At n=100, both classes have enough samples for basic validation\n            #\n            # 5. COORDINATION WITH ONLINELEARNER STATE TRANSITIONS:\n            #\n            #    OnlineLearner State Machine:\n            #      - COLD_START:  0-999 samples (model not production-ready)\n            #      - WARMING:     100-999 samples (model stabilizing but not ready)\n            #      - ACTIVE:      1000+ samples (model production-ready)\n            #\n            #    SafetyBoundsChecker Integration:\n            #      - n=0-99:   SKIPPED_COLD_START (no safety checks, model too unstable)\n            #      - n=100-999: WARMING safety validation (early checks during stabilization)\n            #      - n=1000+:  ACTIVE safety validation (production checks)\n            #\n            #    Why min_samples_for_check (100) < min_training_samples (1000)?\n            #    ---------------------------------------------------------------\n            #\n            #    This intentional gap (100 vs 1000) serves two critical purposes:\n            #\n            #    a) EARLY ANOMALY DETECTION DURING WARMING:\n            #       If the model is fundamentally broken (bad hyperparameters, corrupted\n            #       data pipeline, severe distribution mismatch), we want to detect this\n            #       during the WARMING phase (100-999 samples) rather than waiting until\n            #       ACTIVE state (1000+ samples).\n            #\n            #       Example: Data Pipeline Corruption\n            #         - Model trained with features not being normalized correctly\n            #         - By n=100, accuracy might be obviously bad (e.g., 60%)\n            #         - Safety check at n=100 detects: FAILED_ACCURACY (60% < 85%)\n            #         - Circuit breaker WARNING issued (1/3 failures)\n            #         - Operators alerted early to investigate data pipeline\n            #         - Without early checks, model would continue to n=1000 with\n            #           bad data, wasting 900 more samples and operator time\n            #\n            #    b) PROGRESSIVE VALIDATION PHILOSOPHY:\n            #       Safety validation doesn't need the same confidence level as\n            #       production deployment. The circuit breaker provides defense-in-depth:\n            #         - Single failure at n=100: WARNING (could be noise, keep watching)\n            #         - Two failures at n=100, n=200: WARNING (pattern emerging)\n            #         - Three failures at n=100, n=200, n=300: PAUSED (systematic issue)\n            #\n            #       Even with higher variance at n=100 (SE\u22483.6%), the consecutive failure\n            #       requirement (3 strikes) filters out transient noise. Real systematic\n            #       issues will fail consistently across multiple checks.\n            #\n            #    c) RISK MITIGATION DURING WARMING:\n            #       The WARMING phase (100-999 samples) is higher risk than ACTIVE:\n            #         - StandardScaler statistics still converging (unstable normalization)\n            #         - Model weights still adjusting (gradient descent not converged)\n            #         - Predictions unreliable (not used for production yet)\n            #\n            #       Starting safety checks at n=100 provides oversight during this risky\n            #       phase, even though the model isn't production-ready. If safety checks\n            #       fail during WARMING, it's a signal to investigate before reaching ACTIVE.\n            #\n            #    Example Integration Flow:\n            #      n=50:   OnlineLearner: COLD_START, SafetyBoundsChecker: SKIPPED\n            #              \u2192 Model too unstable for any validation\n            #\n            #      n=150:  OnlineLearner: WARMING, SafetyBoundsChecker: ACTIVE\n            #              \u2192 Safety check runs: 87% accuracy \u2192 PASSED\n            #              \u2192 Model is warming but showing good signs (above 85% threshold)\n            #              \u2192 Continue warming phase with safety oversight\n            #\n            #      n=250:  OnlineLearner: WARMING, SafetyBoundsChecker: ACTIVE\n            #              \u2192 Safety check runs: 82% accuracy \u2192 FAILED_ACCURACY\n            #              \u2192 Circuit breaker: WARNING (1/3 failures)\n            #              \u2192 Alert: \"Model accuracy dropped during warming - investigate\"\n            #              \u2192 Operator checks data pipeline, finds normalization bug\n            #\n            #      n=1000: OnlineLearner: ACTIVE, SafetyBoundsChecker: ACTIVE\n            #              \u2192 Model now production-ready (if safety checks passed)\n            #              \u2192 Both systems agree: model is stable and safe\n            #\n            # 6. WHAT HAPPENS DURING SKIP (Neutral Result):\n            #\n            #    When sample_count < min_samples_for_check (n < 100):\n            #      - Return: SKIPPED_COLD_START result\n            #      - passed=True: Allows model swap to proceed (not blocking)\n            #      - Does NOT increment consecutive_failures counter\n            #      - Does NOT reset consecutive_failures counter\n            #      - Circuit breaker state UNCHANGED (stays in current state)\n            #      - No alerts generated (neither warning nor critical)\n            #\n            #    This neutral behavior is critical because:\n            #      - Cold start is expected and normal (not a failure)\n            #      - Shouldn't penalize model for being new (not fair to increment failures)\n            #      - Shouldn't reward model for being untested (not safe to reset failures)\n            #      - Circuit breaker should remain in previous state until real validation\n            #\n            #    Neutral Result Philosophy:\n            #      - PASSED (passed=False, failures reset): Model validated as safe\n            #      - FAILED (passed=False, failures increment): Model validated as unsafe\n            #      - SKIPPED (passed=True, failures unchanged): Model not yet validated\n            #\n            #    The passed=True allows ModelManager to swap the model during cold start\n            #    (not blocking deployment), but the neutral circuit breaker state ensures\n            #    previous safety context is preserved (don't erase history of failures\n            #    just because a new untested model arrived).\n            #\n            # 7. PRODUCTION IMPACT OF THRESHOLD CHOICE:\n            #\n            #    In a typical governance deployment with 100 requests/hour:\n            #      - Time to 100 samples: ~1 hour (assuming 100% feedback rate)\n            #      - Cold start duration: 1 hour of SKIPPED safety checks\n            #      - WARMING phase: 1-10 hours (100-1000 samples)\n            #\n            #    With min_samples_for_check=50 (too low):\n            #      - Cold start duration: 30 minutes\n            #      - Advantage: 30 minutes faster initial validation\n            #      - Disadvantage: Higher false alarm rate during 30-60 min window\n            #                     (SE\u22485% means ~20% false alarm rate per check)\n            #      - Risk: Could trigger circuit breaker on noise, requiring manual force_resume\n            #\n            #    With min_samples_for_check=200 (too high):\n            #      - Cold start duration: 2 hours\n            #      - Advantage: Very low false alarm rate (SE\u22482.5%)\n            #      - Disadvantage: Model could degrade for 1 extra hour without detection\n            #                     (samples 100-199 have no safety oversight)\n            #      - Risk: Broken model has more time to impact production before detection\n            #\n            #    Optimal min_samples_for_check=100:\n            #      - Cold start duration: 1 hour (reasonable for new model initialization)\n            #      - SE\u22483.6% provides acceptable confidence (not perfect, but good enough\n            #        with consecutive failure filtering from circuit breaker)\n            #      - Balances early detection (starts at 1 hour) with reliability\n            #        (false alarms filtered by 3-strike rule)\n            #\n            # SUMMARY: Cold Start Skip Logic\n            # ===============================\n            #\n            # During cold start (n < 100 samples), safety checks are bypassed because:\n            #   1. Statistical unreliability: SE\u224811% at n=10, too much noise\n            #   2. Risk of false circuit breaker trips: Noise could trigger 3 consecutive failures\n            #   3. Coordination with WARMING phase: Checks begin during stabilization (100-999)\n            #\n            # The threshold min_samples_for_check=100 is optimal because:\n            #   1. Statistical significance: SE\u22483.6%, acceptable for safety decisions\n            #   2. CLT convergence: n=100 provides reliable normal approximation\n            #   3. Early warning: Checks start during WARMING phase (before production)\n            #   4. Domain standards: Aligns with ML evaluation best practices (k-fold CV)\n            #   5. Coordination: Provides safety oversight during model stabilization\n            #\n            # During skip (n < 100):\n            #   - Returns SKIPPED_COLD_START (neutral result)\n            #   - passed=True (allows model swap, doesn't block deployment)\n            #   - Circuit breaker state UNCHANGED (preserves safety context)\n            #   - No alerts generated (cold start is expected, not a failure)\n            #\n            # This approach prevents false alarms during model initialization while\n            # enabling early detection of systematic issues during the warming phase.\n            #\n            # Get sample count to determine if model has enough data for safety check\n            try:\n                sample_count = model.get_sample_count()\n            except (AttributeError, TypeError):\n                # Model doesn't implement get_sample_count() - assume 0 samples (cold start)\n                sample_count = 0\n\n            # COLD START SKIP: Bypass safety checks if insufficient samples for reliable validation\n            if sample_count < self.min_samples_for_check:\n                # Return neutral result (not pass/fail, just skipped)\n                # Does NOT affect consecutive failure counter\n                return SafetyCheckResult(\n                    passed=True,  # Allow model swap during cold start\n                    result=CheckResult.SKIPPED_COLD_START,\n                    current_accuracy=0.0,\n                    threshold=self.accuracy_threshold,\n                    message=f\"Safety check skipped: only {sample_count} samples (need {self.min_samples_for_check})\",\n                    consecutive_failures=self._consecutive_failures,\n                    safety_status=self._status,\n                    metadata={\"sample_count\": sample_count},\n                )\n\n            # ACCURACY CALCULATION\n            #\n            # Use validation data if provided (more reliable, held-out test set)\n            # Otherwise fall back to model's internal accuracy (progressive validation)\n            if validation_data is not None and len(validation_data) > 0:\n                current_accuracy = self._calculate_validation_accuracy(model, validation_data)\n            else:\n                try:\n                    current_accuracy = model.get_accuracy()\n                except (AttributeError, TypeError):\n                    current_accuracy = 0.0\n\n            # FAILURE MODE 1: FAILED_ACCURACY (Absolute Threshold Violation)\n            #\n            # Check if accuracy is below absolute threshold\n            # This catches overall model performance degradation\n            #\n            # ACCURACY THRESHOLD SELECTION: Why accuracy_threshold=0.85 for governance?\n            # =========================================================================\n            #\n            # The 0.85 threshold represents a carefully chosen balance between safety,\n            # adaptability, and operational feasibility for governance systems.\n            #\n            # 1. GOVERNANCE-SPECIFIC FALSE POSITIVE/NEGATIVE TRADE-OFFS:\n            #    In access control and governance systems:\n            #    - False Positive (FP): Granting access that should be denied\n            #      * CRITICAL SEVERITY: Security breach, compliance violation\n            #      * Exposes sensitive data to unauthorized users\n            #      * Legal/financial consequences (GDPR fines, audit failures)\n            #    - False Negative (FN): Denying access that should be granted\n            #      * MODERATE SEVERITY: User inconvenience, productivity impact\n            #      * Can be mitigated with human review/override\n            #      * Reversible with minimal long-term damage\n            #    Traditional ML often optimizes for balanced FP/FN, but governance\n            #    systems prioritize minimizing FP even at cost of higher FN.\n            #\n            # 2. WHY 85% SPECIFICALLY?\n            #    - Maximum 15% combined error rate (FP + FN)\n            #    - Typical governance model tuning:\n            #      * Optimize for high precision (minimize FP): ~5-8% FP rate\n            #      * Accept lower recall (higher FN): ~7-10% FN rate\n            #      * Overall accuracy: 85-90% range\n            #    - Lower threshold (e.g., 80%):\n            #      * 20% error rate unacceptable for compliance\n            #      * Too many false positives (data breaches)\n            #      * Undermines trust in automated governance\n            #    - Higher threshold (e.g., 90%):\n            #      * Too strict for concept drift in governance domain\n            #      * Would frequently trigger circuit breaker (excessive pausing)\n            #      * Reduces adaptability to evolving policies/roles\n            #      * Governance data often imbalanced (rare access patterns)\n            #\n            # 3. ABSOLUTE THRESHOLD IS CRITICAL FOR SAFETY:\n            #    Unlike relative checks (degradation_threshold), absolute threshold\n            #    provides a hard safety floor that model MUST maintain.\n            #    - Prevents \"boiling frog\" gradual degradation\n            #    - Example: Model improving from 70% \u2192 75% \u2192 80%\n            #      * Positive trend (improving), but 80% < 85% (REJECTED)\n            #      * Relative improvement doesn't matter if baseline is unsafe\n            #      * Must reach minimum safety level before production use\n            #    - Ensures model quality never drops below acceptable minimum\n            #    - No exceptions for \"improving but still below threshold\" scenarios\n            #\n            # 4. DEFENSE-IN-DEPTH WITH DEGRADATION CHECK:\n            #    Absolute threshold (0.85) + Relative degradation check (0.05) provide\n            #    comprehensive protection:\n            #    - Scenario 1: Model at 90% drops to 84%\n            #      * Fails absolute threshold (84% < 85%): FAILED_ACCURACY\n            #      * Also fails degradation check (6% > 5%): FAILED_DEGRADATION\n            #      * Both checks catch this dangerous regression\n            #    - Scenario 2: Model at 92% drops to 87%\n            #      * Passes absolute threshold (87% > 85%)\n            #      * Fails degradation check (5% \u2265 5%): FAILED_DEGRADATION\n            #      * Relative check catches regression even when above threshold\n            #    - Scenario 3: Model at 80% improves to 82%\n            #      * Fails absolute threshold (82% < 85%): FAILED_ACCURACY\n            #      * Relative check N/A (improving, not degrading)\n            #      * Absolute check prevents low-quality model deployment\n            #\n            # 5. STATISTICAL INTERPRETATION:\n            #    With min_samples_for_check=100:\n            #    - Standard Error (SE) \u2248 sqrt(p(1-p)/n) = sqrt(0.85*0.15/100) \u2248 0.036\n            #    - 95% Confidence Interval: 0.85 \u00b1 1.96*0.036 \u2248 [0.78, 0.92]\n            #    - Measured 85% accuracy likely represents true 78-92% range\n            #    - Provides buffer above critical safety level (~80%)\n            #    - Reduces false alarms from statistical noise\n            #\n            # This absolute threshold acts as the primary safety net. Combined with\n            # consecutive failure tracking (circuit breaker), it prevents deployment\n            # of unsafe models while tolerating transient noise.\n            if current_accuracy < self.accuracy_threshold:\n                # CIRCUIT BREAKER: Increment failure counter\n                # May transition OK \u2192 WARNING or WARNING \u2192 CRITICAL/PAUSED\n                return self._handle_failure(\n                    CheckResult.FAILED_ACCURACY,\n                    current_accuracy,\n                    f\"Accuracy {current_accuracy:.3f} below threshold {self.accuracy_threshold:.3f}\",\n                )\n\n            # FAILURE MODE 2: FAILED_DEGRADATION (Relative Regression Detection)\n            #\n            # DEGRADATION DETECTION ALGORITHM: Detecting Sudden Performance Regression\n            # =========================================================================\n            #\n            # This check detects significant accuracy drops from the previous safety check,\n            # catching sudden performance regressions that might be missed by absolute\n            # thresholds alone. It provides defense-in-depth by monitoring model stability\n            # over time, not just absolute performance.\n            #\n            # ALGORITHM: Relative Accuracy Comparison\n            # ----------------------------------------\n            #\n            # 1. Compare current accuracy to previous check's accuracy (if available)\n            #    - Only triggers if we have history (_last_accuracy is not None)\n            #    - First check after initialization doesn't have previous baseline\n            #\n            # 2. Calculate accuracy drop: accuracy_drop = previous_accuracy - current_accuracy\n            #    - Direction matters: positive drop means degradation (accuracy decreased)\n            #    - Negative drop means improvement (accuracy increased) - not a failure\n            #\n            #    Mathematical Calculation:\n            #      accuracy_drop = self._last_accuracy - current_accuracy\n            #\n            #    Examples:\n            #      - Previous: 0.90, Current: 0.84 \u2192 drop = 0.90 - 0.84 = 0.06 (6% degradation)\n            #      - Previous: 0.85, Current: 0.88 \u2192 drop = 0.85 - 0.88 = -0.03 (3% improvement)\n            #      - Previous: 0.92, Current: 0.92 \u2192 drop = 0.92 - 0.92 = 0.00 (stable)\n            #\n            # 3. Compare drop to degradation_threshold (default 0.05 = 5%)\n            #    - If accuracy_drop > degradation_threshold: FAILED_DEGRADATION\n            #    - If accuracy_drop \u2264 degradation_threshold: Continue to PASSED\n            #\n            # WHY DEGRADATION_THRESHOLD = 0.05 (5% Drop)?\n            # ---------------------------------------------\n            #\n            # The 5% threshold is carefully chosen to balance sensitivity to real issues\n            # vs. robustness to statistical noise in governance applications.\n            #\n            # 1. STATISTICAL SIGNIFICANCE:\n            #    With min_samples_for_check=100 and typical accuracy ~0.85:\n            #    - Standard Error (SE) \u2248 sqrt(0.85 * 0.15 / 100) \u2248 0.036 (3.6%)\n            #    - 95% Confidence Interval: \u00b1 1.96 * 0.036 \u2248 \u00b1 0.07 (7%)\n            #    - Observed 5% drop could be noise OR real degradation\n            #    - But 5% threshold is ~1.4 standard errors (about 84th percentile)\n            #    - This means ~16% chance of false alarm from noise alone\n            #    - Consecutive failure requirement (3 strikes) reduces noise:\n            #      * Probability of 3 consecutive false alarms: 0.16^3 \u2248 0.4% (very unlikely)\n            #      * Circuit breaker pattern filters transient noise while catching real issues\n            #\n            # 2. SUDDEN VS. GRADUAL DEGRADATION:\n            #    The degradation check serves a different purpose than absolute threshold:\n            #\n            #    Absolute Threshold (FAILED_ACCURACY):\n            #      - Catches: Gradual decay, models that never reach production quality\n            #      - Example: Model slowly degrades 88% \u2192 86% \u2192 84% \u2192 82%\n            #      - Each step is small (2% drop < 5% threshold), but crosses 85% floor\n            #      - FAILED_ACCURACY catches when it hits 84% < 85%\n            #\n            #    Relative Threshold (FAILED_DEGRADATION):\n            #      - Catches: Sudden crashes, acute model failures\n            #      - Example: Model suddenly crashes 92% \u2192 86%\n            #      - 6% drop > 5% threshold (FAILED_DEGRADATION)\n            #      - Still above 85% floor (passes absolute threshold)\n            #      - Relative check catches this acute regression\n            #\n            #    Defense-in-Depth: Both checks together provide comprehensive safety:\n            #      - Scenario A: 92% \u2192 86% (6% drop, above floor)\n            #        * FAILED_DEGRADATION catches acute crash\n            #        * Passes absolute threshold (86% > 85%)\n            #      - Scenario B: 86% \u2192 84% (2% drop, below floor)\n            #        * Passes degradation threshold (2% < 5%)\n            #        * FAILED_ACCURACY catches floor violation (84% < 85%)\n            #      - Scenario C: 90% \u2192 83% (7% drop, below floor)\n            #        * Both checks fail (defense-in-depth confirmation)\n            #\n            # 3. WHAT CAUSES SUDDEN DEGRADATION?\n            #    A 5%+ drop typically indicates systematic issues requiring investigation:\n            #\n            #    a) Bad Training Batch (Data Quality):\n            #       - Corrupted features (missing values, encoding errors)\n            #       - Label noise (incorrect ground truth in feedback)\n            #       - Outlier batch (adversarial samples, edge cases)\n            #       Example: Governance model trained on batch with flipped labels\n            #                (grant/deny swapped) \u2192 learns incorrect policy\n            #\n            #    b) Distribution Shift (Concept Drift):\n            #       - Sudden change in data distribution (policy update, org restructure)\n            #       - Covariate shift (feature distributions change but label mapping stays same)\n            #       - Prior probability shift (class balance changes dramatically)\n            #       Example: Company acquires new division with different access patterns\n            #\n            #    c) Model Corruption (Numerical Instability):\n            #       - Gradient explosion (learning_rate too high, unstable SGD)\n            #       - Weight overflow (numerical precision issues)\n            #       - Catastrophic forgetting (new data overwrites previous knowledge)\n            #       Example: Learning rate spike causes weights to diverge\n            #\n            #    d) Infrastructure Issues:\n            #       - Feature extraction pipeline broken (wrong features fed to model)\n            #       - Preprocessing bug (scaling, normalization errors)\n            #       - Model deserialization error (loaded wrong model version)\n            #       Example: StandardScaler reset during deployment, features no longer normalized\n            #\n            # 4. WHY NOT LOWER (e.g., 3%) OR HIGHER (e.g., 10%) THRESHOLD?\n            #\n            #    Lower Threshold (3%):\n            #      - Too sensitive to statistical noise (SE \u2248 3.6%)\n            #      - Would trigger false alarms on normal variance\n            #      - Excessive alerts lead to \"alert fatigue\" (operators ignore warnings)\n            #      - Circuit breaker would open too frequently (reduced adaptability)\n            #      - Better handled by consecutive failure filtering (not threshold tuning)\n            #\n            #    Higher Threshold (10%):\n            #      - Too slow to detect acute issues (92% \u2192 82% is catastrophic)\n            #      - By the time 10% drop detected, model may be unsafe (82% < 85%)\n            #      - Absolute threshold would catch it anyway (redundant)\n            #      - Misses moderate regressions (88% \u2192 81% is 7% drop, below 10%)\n            #      - Defeats purpose of early warning system\n            #\n            #    Optimal 5% Threshold:\n            #      - Statistically significant (~1.4 SE) but not excessive false alarms\n            #      - Catches acute issues before absolute threshold violation\n            #      - Combined with consecutive failures (3 strikes) filters noise\n            #      - Provides early warning for intervention before critical failure\n            #      - Balances sensitivity (catch real issues) vs. specificity (avoid false alarms)\n            #\n            # 5. GOVERNANCE-SPECIFIC CONSIDERATIONS:\n            #\n            #    Why degradation detection is critical for access control/governance:\n            #\n            #    a) Cascading Impact:\n            #       - Degraded model grants improper permissions\n            #       - Bad permissions propagate to downstream systems\n            #       - Audit logs filled with incorrect decisions\n            #       - Compliance violations accumulate over time\n            #       - Early detection (5% drop) prevents cascade\n            #\n            #    b) Feedback Loop Risk:\n            #       - Model learns from its own predictions (online learning)\n            #       - Degraded model makes bad predictions\n            #       - Bad predictions become training labels (feedback loop)\n            #       - Further degrades model (positive feedback, exponential decay)\n            #       - Circuit breaker halts loop before irreversible corruption\n            #\n            #    c) Auditability Requirements:\n            #       - Compliance requires explaining why decisions changed\n            #       - Sudden 5%+ drop is auditable event (clear threshold crossed)\n            #       - Gradual 1-2% drifts harder to detect and explain\n            #       - Degradation detection provides audit trail for investigations\n            #\n            #    d) User Trust:\n            #       - Users notice when access patterns change suddenly\n            #       - 5%+ regression means ~1 in 20 users sees different behavior\n            #       - Early detection maintains consistent user experience\n            #       - Prevents \"why did this suddenly get denied?\" support tickets\n            #\n            # 6. DIRECTION OF CALCULATION (Why previous - current, not current - previous):\n            #\n            #    Mathematical Calculation:\n            #      accuracy_drop = self._last_accuracy - current_accuracy\n            #\n            #    Why This Direction?\n            #      - Positive value = degradation (accuracy decreased, bad)\n            #      - Negative value = improvement (accuracy increased, good)\n            #      - Natural interpretation: \"drop\" means going down\n            #\n            #    Example:\n            #      Previous: 0.90, Current: 0.84\n            #      accuracy_drop = 0.90 - 0.84 = +0.06 (6% drop, positive means degradation)\n            #\n            #    Alternative (Wrong Direction):\n            #      accuracy_drop_wrong = current_accuracy - self._last_accuracy\n            #      = 0.84 - 0.90 = -0.06 (negative value, confusing)\n            #      Would need: if accuracy_drop_wrong < -self.degradation_threshold (awkward)\n            #\n            #    Our Direction Benefits:\n            #      - Intuitive: positive drop = degradation (matches English semantics)\n            #      - Simple comparison: accuracy_drop > threshold (no negation needed)\n            #      - Consistent with \"accuracy drop\" terminology throughout codebase\n            #      - Improvement (negative drop) naturally ignored (< 0, fails > threshold check)\n            #\n            # 7. INTEGRATION WITH CIRCUIT BREAKER:\n            #\n            #    When degradation detected:\n            #      - Increments consecutive_failures counter (same as FAILED_ACCURACY)\n            #      - Both failure modes contribute equally to circuit breaker\n            #      - After 3 consecutive degradation failures: circuit opens (PAUSED)\n            #      - Prevents continued learning from degraded model\n            #      - Requires investigation and manual intervention (force_resume)\n            #\n            #    State Transitions on Degradation Failure:\n            #      - 1st degradation: OK \u2192 WARNING (1/3 failures, alert sent)\n            #      - 2nd degradation: WARNING \u2192 WARNING (2/3 failures, concern growing)\n            #      - 3rd degradation: WARNING \u2192 CRITICAL \u2192 PAUSED (circuit opens)\n            #      - Manual fix + passing check: PAUSED \u2192 OK (circuit closes)\n            #\n            # Example Scenarios with Concrete Numbers:\n            # -----------------------------------------\n            #\n            # Scenario 1: Acute Regression (Caught by Degradation)\n            #   Check 1: 92% accuracy (baseline established)\n            #   Check 2: 86% accuracy\n            #     \u2192 drop = 92% - 86% = 6% > 5% threshold\n            #     \u2192 FAILED_DEGRADATION (1/3 failures, WARNING)\n            #     \u2192 Still above 85% absolute threshold (passes FAILED_ACCURACY check)\n            #     \u2192 Degradation check provides early warning\n            #\n            # Scenario 2: Gradual Decay (Caught by Absolute Threshold)\n            #   Check 1: 88% accuracy (baseline)\n            #   Check 2: 86% accuracy (drop = 2% < 5%, passes degradation)\n            #   Check 3: 84% accuracy (drop = 2% < 5%, passes degradation)\n            #     \u2192 But 84% < 85% absolute threshold\n            #     \u2192 FAILED_ACCURACY catches gradual decay\n            #     \u2192 Degradation check alone would miss this pattern\n            #\n            # Scenario 3: Catastrophic Failure (Both Checks Fail)\n            #   Check 1: 90% accuracy (baseline)\n            #   Check 2: 82% accuracy\n            #     \u2192 drop = 90% - 82% = 8% > 5% threshold (FAILED_DEGRADATION)\n            #     \u2192 82% < 85% absolute threshold (FAILED_ACCURACY)\n            #     \u2192 Both checks fail (defense-in-depth confirmation)\n            #     \u2192 High confidence this is real issue, not noise\n            #\n            # Scenario 4: Improvement (No Failure)\n            #   Check 1: 85% accuracy (baseline)\n            #   Check 2: 88% accuracy\n            #     \u2192 drop = 85% - 88% = -3% < 5% threshold (improvement, ignored)\n            #     \u2192 88% > 85% absolute threshold (passes)\n            #     \u2192 PASSED, consecutive_failures reset to 0\n            #     \u2192 Circuit closes if was previously open\n            #\n            # This degradation detection algorithm provides critical safety by catching\n            # sudden model failures that absolute thresholds might miss, enabling early\n            # intervention before cascading failures occur in production governance systems.\n            if self._last_accuracy is not None:\n                # Calculate accuracy drop from previous check\n                # Direction: previous - current makes positive = degradation (intuitive)\n                accuracy_drop = self._last_accuracy - current_accuracy\n\n                # Check if drop exceeds 5% threshold (statistically significant regression)\n                if accuracy_drop > self.degradation_threshold:\n                    # CIRCUIT BREAKER: Increment failure counter\n                    # Provides metadata about previous accuracy and drop magnitude\n                    return self._handle_failure(\n                        CheckResult.FAILED_DEGRADATION,\n                        current_accuracy,\n                        f\"Accuracy dropped by {accuracy_drop:.3f} (threshold: {self.degradation_threshold:.3f})\",\n                        metadata={\n                            \"previous_accuracy\": self._last_accuracy,\n                            \"accuracy_drop\": accuracy_drop,\n                        },\n                    )\n\n            # PASSED: All safety checks passed\n            #\n            # Model accuracy is above threshold AND no significant degradation\n            # CIRCUIT BREAKER: Reset failure counter to 0\n            # May transition WARNING \u2192 OK or PAUSED \u2192 OK (circuit closes)\n            return self._handle_success(current_accuracy)\n\n    def check_accuracy(self, accuracy: float) -> SafetyCheckResult:\n        \"\"\"Check if an accuracy value passes safety bounds.\n\n        Simplified check when you have the accuracy value directly.\n\n        Args:\n            accuracy: Accuracy value to check (0.0-1.0).\n\n        Returns:\n            SafetyCheckResult with pass/fail status and details.\n        \"\"\"\n        with self._lock:\n            self._total_checks += 1\n            self._last_check_time = time.time()\n\n            # ABSOLUTE ACCURACY THRESHOLD CHECK\n            #\n            # Why accuracy_threshold=0.85 for governance?\n            # ============================================\n            #\n            # Governance systems make high-stakes decisions about data access, permissions,\n            # and policy enforcement. The 0.85 threshold is chosen based on several factors:\n            #\n            # 1. SAFETY-CRITICAL CONTEXT:\n            #    - False Negatives (FN): Model predicts \"deny\" but should \"grant\"\n            #      * Blocks legitimate user access to critical systems\n            #      * Disrupts business operations and productivity\n            #      * Lower severity than false positives in most cases\n            #    - False Positives (FP): Model predicts \"grant\" but should \"deny\"\n            #      * Exposes sensitive data to unauthorized users\n            #      * Violates compliance regulations (GDPR, HIPAA, SOC2)\n            #      * Security breach with legal/financial consequences\n            #      * Higher severity - unacceptable in governance context\n            #\n            # 2. ACCURACY THRESHOLD SELECTION RATIONALE:\n            #    - With 85% accuracy threshold:\n            #      * Maximum 15% error rate (FP + FN combined)\n            #      * Assuming balanced precision/recall: ~7.5% FP, ~7.5% FN\n            #      * In practice: Can tune model to minimize FP at cost of higher FN\n            #    - Why not higher (e.g., 90%)?\n            #      * Governance data is often imbalanced (rare access patterns)\n            #      * Concept drift is common (policies/roles change frequently)\n            #      * Setting threshold too high causes excessive false alarms\n            #      * Would frequently pause learning, reducing adaptability\n            #    - Why not lower (e.g., 80%)?\n            #      * 20% error rate too high for compliance/security requirements\n            #      * Unacceptable false positive rate (potential data breaches)\n            #      * Undermines trust in ML-based governance system\n            #\n            # 3. ABSOLUTE VS. RELATIVE THRESHOLDS:\n            #    - Absolute threshold (0.85) provides hard safety floor\n            #    - Model MUST maintain minimum quality regardless of trend\n            #    - Prevents gradual degradation (\"boiling frog\" problem)\n            #    - Example: Model improving from 75% \u2192 80% is STILL rejected\n            #      * Even though accuracy is increasing (positive trend)\n            #      * 80% < 85% absolute threshold (unsafe for production)\n            #      * Relative improvement doesn't matter if baseline is too low\n            #    - Contrast with degradation_threshold (relative check):\n            #      * Catches regression: 90% \u2192 85% is 5% drop (triggers FAILED_DEGRADATION)\n            #      * But 85% still passes absolute threshold (borderline safe)\n            #      * Both checks provide defense-in-depth\n            #\n            # 4. DOMAIN-SPECIFIC GOVERNANCE CONSIDERATIONS:\n            #    - Access control decisions are binary (grant/deny) and irreversible\n            #    - Audit logs require high accuracy for compliance investigations\n            #    - Model errors can cascade (wrong permissions lead to more wrong decisions)\n            #    - Human review is expensive - can't manually verify all predictions\n            #    - Must maintain user trust while adapting to policy changes\n            #\n            # 5. STATISTICAL INTERPRETATION:\n            #    - With n=100 samples: SE \u2248 sqrt(0.85 * 0.15 / 100) \u2248 0.036 (3.6%)\n            #    - 95% CI: [81.4%, 88.6%] - reasonable confidence bounds\n            #    - Measured accuracy of 0.85 likely represents true range 82-88%\n            #    - Provides buffer above critical safety level (~80%)\n            #\n            # This absolute threshold acts as a safety net - the circuit breaker will\n            # pause learning before accuracy drops to dangerous levels. Combined with\n            # degradation_threshold (relative check), it provides comprehensive safety.\n            if accuracy < self.accuracy_threshold:\n                return self._handle_failure(\n                    CheckResult.FAILED_ACCURACY,\n                    accuracy,\n                    f\"Accuracy {accuracy:.3f} below threshold {self.accuracy_threshold:.3f}\",\n                )\n\n            # DEGRADATION CHECK (Relative Regression Detection)\n            #\n            # Check for significant accuracy drop from previous check.\n            # See check_model() DEGRADATION DETECTION ALGORITHM section for full explanation.\n            #\n            # Quick Summary:\n            # - Detects sudden performance regression (acute failures)\n            # - Complements absolute threshold (catches gradual decay)\n            # - accuracy_drop = previous - current (positive = degradation)\n            # - degradation_threshold = 0.05 (5% drop triggers failure)\n            # - Balances sensitivity to real issues vs. statistical noise\n            # - Integrates with circuit breaker (contributes to consecutive failures)\n            if self._last_accuracy is not None:\n                # Calculate accuracy drop from previous check\n                # Direction: previous - current makes positive = degradation (intuitive)\n                accuracy_drop = self._last_accuracy - accuracy\n\n                # Check if drop exceeds 5% threshold (statistically significant regression)\n                if accuracy_drop > self.degradation_threshold:\n                    # CIRCUIT BREAKER: Increment failure counter\n                    # Provides metadata about previous accuracy and drop magnitude\n                    return self._handle_failure(\n                        CheckResult.FAILED_DEGRADATION,\n                        accuracy,\n                        f\"Accuracy dropped by {accuracy_drop:.3f} (threshold: {self.degradation_threshold:.3f})\",\n                        metadata={\n                            \"previous_accuracy\": self._last_accuracy,\n                            \"accuracy_drop\": accuracy_drop,\n                        },\n                    )\n\n            # Passed\n            return self._handle_success(accuracy)\n\n    def _handle_success(self, accuracy: float) -> SafetyCheckResult:\n        \"\"\"Handle a successful safety check.\n\n        CIRCUIT BREAKER CLOSES/RESETS\n        ==============================\n\n        This method handles the circuit breaker closing/resetting when a safety check passes.\n        It represents the recovery path from failure states (WARNING, CRITICAL, PAUSED) back\n        to normal operation (OK).\n\n        Circuit Breaker Reset Behavior:\n        1. Reset consecutive failure counter to 0\n        2. Update last_accuracy for degradation tracking\n        3. If circuit was PAUSED, automatically resume learning (circuit closes)\n        4. Transition to OK state (normal operation)\n\n        Why Automatic Resume is Safe:\n        - Circuit only closes if a safety check PASSES (evidence of recovery)\n        - Model accuracy is above threshold (quality verified)\n        - No significant degradation detected (stability verified)\n        - Unlike traditional circuit breakers (timeout-based), ML circuit breakers\n          require proof of recovery before closing\n\n        State Transitions on Success:\n        - OK \u2192 OK: Circuit already closed, counter stays at 0\n        - WARNING \u2192 OK: Circuit closes, consecutive failures reset\n        - PAUSED \u2192 OK: Circuit closes, learning resumes (calls _resume_learning)\n\n        Args:\n            accuracy: The accuracy value that passed.\n\n        Returns:\n            SafetyCheckResult indicating success.\n        \"\"\"\n        # Track success metrics\n        self._passed_checks += 1\n\n        # CIRCUIT BREAKER RESET: Reset consecutive failure counter\n        # This is the key to circuit breaker recovery - a single success\n        # resets the failure count, requiring failures to be truly consecutive\n        # to trigger circuit open\n        self._consecutive_failures = 0\n\n        # Update accuracy for future degradation checks\n        self._last_accuracy = accuracy\n\n        # CIRCUIT BREAKER CLOSES: Resume from paused state if applicable\n        # If circuit was OPEN (PAUSED), this successful check provides evidence\n        # that the underlying issue has been resolved. Automatically close the\n        # circuit and resume normal learning operations.\n        if self._status == SafetyStatus.PAUSED:\n            self._resume_learning()  # Triggers resume callbacks, logs recovery\n\n        # Transition to OK state (circuit fully closed and healthy)\n        self._status = SafetyStatus.OK\n\n        return SafetyCheckResult(\n            passed=True,\n            result=CheckResult.PASSED,\n            current_accuracy=accuracy,\n            threshold=self.accuracy_threshold,\n            message=\"Safety check passed\",\n            consecutive_failures=0,  # Reset to 0 on success\n            safety_status=self._status,\n        )\n\n    def _handle_failure(\n        self,\n        result: CheckResult,\n        accuracy: float,\n        message: str,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> SafetyCheckResult:\n        \"\"\"Handle a failed safety check.\n\n        Increments failure counter and potentially pauses learning.\n\n        This method implements the core circuit breaker escalation logic:\n        1. Track consecutive failures (counter increments)\n        2. Escalate from OK \u2192 WARNING \u2192 CRITICAL \u2192 PAUSED based on count\n        3. Generate alerts at appropriate severity levels\n        4. Trigger auto-pause when threshold is reached (circuit opens)\n\n        Args:\n            result: The type of failure.\n            accuracy: The accuracy value that failed.\n            message: Human-readable failure message.\n            metadata: Additional context for the failure.\n\n        Returns:\n            SafetyCheckResult indicating failure.\n        \"\"\"\n        # CIRCUIT BREAKER: Track failure metrics\n        # These metrics are critical for circuit breaker state decisions\n        self._failed_checks += 1  # Total failures (all-time)\n        self._consecutive_failures += 1  # Consecutive failures (resets on success)\n        self._max_consecutive_failures = max(\n            self._max_consecutive_failures, self._consecutive_failures\n        )\n        self._last_failure_time = time.time()\n\n        # CIRCUIT BREAKER STATE TRANSITION LOGIC\n        #\n        # The circuit breaker uses consecutive failures to determine when to open:\n        # - Transient failures (single occurrence) don't trigger circuit open\n        # - Persistent failures (consecutive occurrences) indicate systematic issue\n        #\n        # State Transitions:\n        # OK (0 failures) \u2192 WARNING (1+ failures) \u2192 CRITICAL/PAUSED (3+ failures)\n        #\n        # WHY CONSECUTIVE_FAILURES_LIMIT = 3?\n        # This threshold balances two competing concerns:\n        # (1) CONFIRMATION: Need multiple failures to distinguish systematic issues from noise\n        # (2) SPEED: Must detect and halt degradation before significant production impact\n        #\n        # CONFIRMATION WITHOUT FALSE ALARMS:\n        # ------------------------------------\n        # Single failures can occur due to transient issues:\n        # - 1 failure: Could be statistical noise, outlier batch, temporary data spike\n        #   * With accuracy SE \u2248 3.6%, ~16% chance of false alarm from noise alone\n        #   * Too sensitive - would cause unnecessary circuit breaker trips\n        #   * Action: Issue WARNING alert but continue learning\n        #\n        # - 2 failures: Concerning pattern, but could still be coincidental\n        #   * Probability of 2 consecutive false alarms: 0.16 \u00d7 0.16 = 2.6%\n        #   * Still plausible as random fluctuation in noisy governance data\n        #   * Action: Escalate WARNING severity but continue learning\n        #\n        # - 3 failures: Extremely unlikely to be coincidence (statistical confirmation)\n        #   * Probability of 3 consecutive false alarms: 0.16^3 \u2248 0.4% (highly improbable)\n        #   * Strong evidence of systematic issue requiring intervention\n        #   * Action: CRITICAL alert + PAUSED (circuit breaker opens)\n        #\n        # DETECTION WITHOUT EXCESSIVE DELAYS:\n        # ------------------------------------\n        # Why not 4 or 5 failures? Timing matters for preventing production impact:\n        #\n        # Typical Governance Model Update Frequency:\n        # - High-traffic systems: Safety checks every 5-15 minutes (frequent model updates)\n        # - Medium-traffic systems: Safety checks every 30-60 minutes (moderate updates)\n        # - Low-traffic systems: Safety checks every 2-4 hours (infrequent updates)\n        #\n        # Time to Circuit Breaker Open with Different Thresholds:\n        #\n        # High-Traffic System (10-minute check intervals):\n        # - 3 failures: 30 minutes to detection (ACCEPTABLE)\n        #   * Fail at T=0, T=10, T=20 \u2192 Paused at T=20\n        #   * Production running degraded model for only 20 minutes\n        # - 5 failures: 50 minutes to detection (TOO SLOW)\n        #   * Fail at T=0, T=10, T=20, T=30, T=40 \u2192 Paused at T=40\n        #   * Production running degraded model for 40+ minutes (UNACCEPTABLE)\n        #\n        # Medium-Traffic System (30-minute check intervals):\n        # - 3 failures: 90 minutes to detection (ACCEPTABLE for moderate traffic)\n        #   * Fail at T=0, T=30, T=60 \u2192 Paused at T=60\n        #   * Balances false alarm prevention with reasonable detection time\n        # - 5 failures: 150 minutes to detection (2.5 hours - TOO SLOW)\n        #   * Would allow degraded model to impact production for hours\n        #\n        # Low-Traffic System (2-hour check intervals):\n        # - 3 failures: 6 hours to detection (ACCEPTABLE for low-traffic)\n        #   * Fail at T=0, T=2h, T=4h \u2192 Paused at T=4h\n        #   * Lower traffic means lower impact exposure\n        # - 5 failures: 10 hours to detection (TOO SLOW even for low traffic)\n        #\n        # PRODUCTION IMPACT ANALYSIS:\n        # In a high-traffic governance system processing 100 requests/minute:\n        # - 3-failure threshold (30 min): ~3,000 requests with degraded model\n        # - 5-failure threshold (50 min): ~5,000 requests with degraded model\n        #   * 2,000 additional requests at risk (67% more exposure)\n        #\n        # With 85% accuracy threshold and 80% degraded model accuracy:\n        # - Expected additional errors: 2,000 \u00d7 (0.85 - 0.80) = 100 more errors\n        #   * 100 more false grants (security breaches) or false denials (user impact)\n        #   * In governance, even small numbers matter (compliance violations, audit failures)\n        #\n        # WHY NOT LOWER (e.g., 2 failures)?\n        # - Too sensitive to noise (2.6% false alarm rate)\n        # - Would cause frequent unnecessary pauses\n        # - Reduces system adaptability (excessive circuit breaker trips)\n        # - Alert fatigue for operators (ignored warnings become dangerous)\n        #\n        # SIMILAR INDUSTRY STANDARDS:\n        # The 3-failure threshold aligns with established reliability engineering patterns:\n        # - TCP retransmit threshold (3 duplicate ACKs trigger fast retransmit)\n        # - Traditional circuit breakers (often use 3-5 failure threshold)\n        # - Statistical significance (3 standard deviations for 99.7% confidence)\n        # - AWS health checks (default 3 consecutive failures before instance marked unhealthy)\n        # - Kubernetes liveness probes (default failureThreshold=3)\n        #\n        # TUNING GUIDANCE:\n        # The consecutive_failures_limit can be adjusted based on your deployment:\n        #\n        # Use 2 failures if:\n        # - Extremely high-traffic (1000s requests/minute)\n        # - Ultra-low tolerance for degradation (banking, healthcare)\n        # - Excellent data quality (very low noise)\n        # - Willing to handle occasional false alarm pauses\n        #\n        # Use 4-5 failures if:\n        # - Very low-traffic (infrequent model updates)\n        # - High tolerance for transient degradation\n        # - Noisy data (imbalanced classes, sparse features)\n        # - Want to minimize circuit breaker trips\n        #\n        # Default 3 is optimal for most governance applications, providing statistical\n        # confirmation (0.4% false alarm rate) with reasonable detection speed (minutes\n        # to hours depending on traffic, not days).\n\n        if self._consecutive_failures >= self.consecutive_failures_limit:\n            # CIRCUIT BREAKER OPENS (CRITICAL \u2192 PAUSED)\n            #\n            # Consecutive failure limit reached - circuit breaker trips to prevent\n            # cascading degradation. Learning is halted to prevent bad model updates\n            # from corrupting future training data or reaching production.\n            #\n            # This is the \"open circuit\" state where no learning operations flow through.\n            # Manual intervention (force_resume) is required to close the circuit again.\n\n            if self._status != SafetyStatus.PAUSED:\n                # Transition to CRITICAL (tripping) then PAUSED (open)\n                self._status = SafetyStatus.CRITICAL\n\n                if self.enable_auto_pause:\n                    # AUTO-PAUSE MECHANISM: Preventing Bad Model Updates\n                    # ====================================================\n                    #\n                    # When enable_auto_pause=True, the circuit breaker automatically halts\n                    # learning after consecutive_failures_limit failures. This prevents\n                    # degraded models from corrupting production in several critical ways:\n                    #\n                    # 1. FEEDBACK LOOP PREVENTION:\n                    #    A degraded model makes incorrect predictions \u2192 those predictions\n                    #    generate incorrect feedback labels \u2192 model learns from its own\n                    #    mistakes \u2192 degradation accelerates.\n                    #\n                    #    Example in Governance:\n                    #    - Model incorrectly grants access (FP) \u2192 User accesses resource \u2192\n                    #      System logs \"successful access\" as positive feedback \u2192\n                    #      Model learns to grant more improper access \u2192 CASCADE\n                    #\n                    # 2. PRODUCTION CORRUPTION PREVENTION:\n                    #    Without auto-pause, consecutive failures would allow ModelManager\n                    #    to swap in progressively worse models, degrading the production\n                    #    system. Auto-pause blocks swaps until human review confirms fix.\n                    #\n                    #    Timeline without auto-pause:\n                    #    T0: Model at 90% accuracy (production)\n                    #    T1: New model at 84% fails check \u2192 WARNING (swap allowed)\n                    #    T2: New model at 82% fails check \u2192 WARNING (swap allowed)\n                    #    T3: New model at 79% fails check \u2192 WARNING (swap allowed)\n                    #    T4: Production now running 79% model \u2192 CRITICAL FAILURE\n                    #\n                    #    Timeline WITH auto-pause:\n                    #    T0: Model at 90% accuracy (production)\n                    #    T1: New model at 84% fails check \u2192 WARNING (swap allowed)\n                    #    T2: New model at 82% fails check \u2192 WARNING (swap allowed)\n                    #    T3: New model at 79% fails check \u2192 PAUSED (swap BLOCKED)\n                    #    T4: Production still running 90% model \u2192 STABLE\n                    #\n                    # 3. FORCED INVESTIGATION WINDOW:\n                    #    Auto-pause creates mandatory stop for root cause analysis:\n                    #    - Is training data corrupted? (bad labels, feature drift)\n                    #    - Is there distribution shift? (concept drift, covariate shift)\n                    #    - Are hyperparameters misconfigured? (learning rate too high)\n                    #    - Is there infrastructure failure? (broken pipeline)\n                    #\n                    # 4. GOVERNANCE SAFETY MARGIN:\n                    #    In access control, even a brief period with a degraded model can:\n                    #    - Grant unauthorized access (security breach, compliance violation)\n                    #    - Deny legitimate access (business disruption, user frustration)\n                    #    - Create audit inconsistencies (compliance risk)\n                    #    Auto-pause prevents these risks by blocking updates proactively.\n                    #\n                    # WHY ENABLE_AUTO_PAUSE IS CRITICAL:\n                    # When enable_auto_pause=False (NOT RECOMMENDED for production):\n                    # - Circuit breaker transitions to CRITICAL but does NOT pause\n                    # - Learning continues despite consecutive failures (DANGEROUS)\n                    # - Operator must manually monitor and pause (human-in-the-loop delay)\n                    # - Use only for testing/debugging where controlled degradation is acceptable\n                    #\n                    # When enable_auto_pause=True (RECOMMENDED for production):\n                    # - Circuit breaker automatically pauses learning (fail-safe)\n                    # - No human intervention delay (immediate protection)\n                    # - Forces operators to fix root cause before resume (deliberate recovery)\n                    # - Default: True for safety-critical governance applications\n                    #\n                    # This is the circuit breaker \"opening\" to prevent further damage.\n                    # Sets status to PAUSED and blocks all future learning operations.\n                    self._pause_learning()  # Opens circuit breaker\n\n                # Generate CRITICAL alert for operator intervention\n                # In production, this should trigger:\n                # - PagerDuty/OpsGenie alert for on-call engineer\n                # - Dashboard alert showing circuit breaker state\n                # - Audit log entry for compliance tracking\n                self._generate_alert(\n                    severity=\"critical\",\n                    message=f\"Learning paused after {self._consecutive_failures} consecutive failures\",\n                    accuracy=accuracy,\n                    action=\"paused_learning\",\n                )\n        else:\n            # CIRCUIT BREAKER WARNING (DEGRADED BUT STILL CLOSED)\n            #\n            # Failures detected but below consecutive limit - circuit is still closed\n            # but in a degraded/warning state. Learning continues but system is on alert.\n            #\n            # Example states:\n            # - 1/3 failures: First failure, could be transient\n            # - 2/3 failures: Second consecutive failure, concerning but not critical yet\n            #\n            # This warning state allows for:\n            # - Early detection of emerging issues\n            # - Operator awareness before circuit opens\n            # - Graceful handling of transient failures without circuit trip\n\n            self._status = SafetyStatus.WARNING\n\n            # Generate WARNING alert for monitoring\n            # Not critical yet, but operators should be aware of degradation\n            self._generate_alert(\n                severity=\"warning\",\n                message=f\"Safety check failed ({self._consecutive_failures}/{self.consecutive_failures_limit}): {message}\",\n                accuracy=accuracy,\n                action=\"none\",  # No action taken yet, still learning\n            )\n\n        logger.warning(\n            \"Safety check failed\",\n            extra={\n                \"result\": result.value,\n                \"accuracy\": accuracy,\n                \"consecutive_failures\": self._consecutive_failures,\n                \"status\": self._status.value,\n            },\n        )\n\n        return SafetyCheckResult(\n            passed=False,\n            result=result,\n            current_accuracy=accuracy,\n            threshold=self.accuracy_threshold,\n            message=message,\n            consecutive_failures=self._consecutive_failures,\n            safety_status=self._status,\n            metadata=metadata or {},\n        )\n\n    def _calculate_validation_accuracy(\n        self,\n        model: Any,\n        validation_data: List[Tuple[Dict[str, Any], int]],\n    ) -> float:\n        \"\"\"Calculate accuracy on a validation dataset.\n\n        Args:\n            model: The model to validate.\n            validation_data: List of (features, label) tuples.\n\n        Returns:\n            Accuracy on the validation set (0.0-1.0).\n        \"\"\"\n        if not validation_data:\n            return 0.0\n\n        correct = 0\n        total = len(validation_data)\n\n        for features, label in validation_data:\n            try:\n                result = model.predict_one(features)\n                prediction = result.prediction if hasattr(result, \"prediction\") else result\n                if prediction == label:\n                    correct += 1\n            except Exception as e:\n                logger.debug(f\"Validation prediction error: {e}\")\n                # Count as incorrect\n\n        return correct / total if total > 0 else 0.0\n\n    def _pause_learning(self) -> None:\n        \"\"\"Pause learning due to safety bounds violation.\n\n        CIRCUIT BREAKER OPENS (PAUSED STATE)\n        =====================================\n\n        This method opens the circuit breaker, halting all learning operations.\n        It represents the final safety action when consecutive failures indicate\n        a systematic problem that requires human intervention.\n\n        Circuit Breaker Open State:\n        - All learning operations are blocked (is_learning_allowed() returns False)\n        - Model updates are rejected to prevent further degradation\n        - System maintains current model until manual recovery\n        - Consecutive failure counter remains elevated until reset\n\n        Why Open the Circuit?\n        1. **Prevent Cascading Failures**: A degraded model learning from its own\n           bad predictions creates a feedback loop. Opening the circuit breaks this loop.\n\n        2. **Preserve System Stability**: Better to maintain a slightly degraded model\n           than risk further corruption through continued learning.\n\n        3. **Force Human Review**: Systematic failures (3+ consecutive) indicate issues\n           that automated systems cannot resolve:\n           - Data quality problems (garbage in, garbage out)\n           - Distribution shift requiring retraining from scratch\n           - Configuration errors (wrong thresholds, bad hyperparameters)\n           - Infrastructure issues (corrupted data pipeline)\n\n        4. **Governance Safety**: In access control, continued learning with a failing\n           model could grant improper permissions or deny legitimate access at scale.\n\n        Integration with ModelManager:\n        When the circuit opens, the ModelManager should:\n        - Reject new model updates (return SwapStatus.REJECTED_SAFETY)\n        - Continue serving the current (paused) model for predictions\n        - Alert operators for manual investigation\n        - Wait for force_resume() before accepting updates again\n\n        Recovery Process and Manual Intervention:\n        ==========================================\n\n        When the circuit breaker opens (PAUSED state), manual intervention via\n        force_resume() is required to restart learning. This deliberate human-in-the-loop\n        ensures systematic issues are resolved before resuming operations.\n\n        WHEN TO USE MANUAL INTERVENTION (force_resume):\n\n        Scenario 1: Root Cause Identified and Fixed\n        --------------------------------------------\n        If investigation reveals a fixable issue that has been resolved:\n\n        Example: Data Pipeline Corruption\n        - Investigation: Features were not being normalized correctly\n        - Fix: Repaired StandardScaler initialization in preprocessing\n        - Action: force_resume() after verifying fix on validation data\n        - Rationale: Root cause eliminated, safe to resume learning\n\n        Example: Hyperparameter Misconfiguration\n        - Investigation: Learning rate (0.1) was too high, causing divergence\n        - Fix: Reduced learning_rate to 0.01 in model configuration\n        - Action: force_resume() after testing with smaller learning rate\n        - Rationale: Configuration corrected, stable training expected\n\n        Example: Bad Training Batch\n        - Investigation: Single batch contained corrupted labels (flipped grant/deny)\n        - Fix: Removed corrupted batch from training queue, validated data quality\n        - Action: force_resume() after data quality checks pass\n        - Rationale: Corrupted data purged, clean training data restored\n\n        Scenario 2: Temporary Distribution Shift Resolved\n        --------------------------------------------------\n        If drift detection shows distribution has returned to normal:\n\n        Example: Organizational Restructure\n        - Investigation: Company acquisition caused temporary access pattern shift\n        - Resolution: Access patterns stabilized after integration completed\n        - Action: force_resume() after drift_score returns to baseline\n        - Rationale: Distribution shift was temporary, model is valid again\n\n        Example: Seasonal Policy Change\n        - Investigation: End-of-quarter access patterns deviated from norm\n        - Resolution: Quarter ended, access patterns returned to typical levels\n        - Action: force_resume() after monitoring shows pattern normalization\n        - Rationale: Seasonal drift resolved, model applicable again\n\n        Scenario 3: Threshold Adjustment After Review\n        ----------------------------------------------\n        If investigation shows thresholds were too conservative:\n\n        Example: Overly Strict Accuracy Threshold\n        - Investigation: Model consistently at 83-84% (below 85% threshold)\n        - Analysis: Domain research shows 80-85% is acceptable for this use case\n        - Action: Lower accuracy_threshold to 0.80, then force_resume()\n        - Rationale: Threshold was misconfigured, model is actually safe\n        - CAUTION: Only adjust thresholds after thorough domain analysis!\n\n        WHEN NOT TO USE MANUAL INTERVENTION:\n\n        Do NOT force_resume() if:\n\n        1. Root Cause Unknown:\n           - Consecutive failures detected but cause unclear\n           - Action: Continue investigation, review logs/metrics/drift\n           - Risk: Resuming without fix will trigger circuit breaker again\n\n        2. Systematic Data Quality Issues:\n           - Label noise, feature corruption, or missing values persist\n           - Action: Fix data pipeline, validate data quality first\n           - Risk: Model will learn from garbage data (garbage in, garbage out)\n\n        3. Fundamental Model Degradation:\n           - Model has catastrophically forgotten previous knowledge\n           - Distribution shift is permanent (concept drift)\n           - Action: Retrain model from scratch with new data\n           - Risk: Resuming learning won't fix fundamental model corruption\n\n        4. Infrastructure Instability:\n           - Database connections flapping, network issues, resource constraints\n           - Action: Stabilize infrastructure before resuming\n           - Risk: Unstable infrastructure will cause repeated failures\n\n        STANDARD RECOVERY WORKFLOW:\n\n        Step 1: INVESTIGATE\n        -------------------\n        - Check logs for error messages and stack traces\n        - Review accuracy metrics (current vs. historical)\n        - Analyze drift scores (check_drift() results)\n        - Inspect recent training batches (data quality)\n        - Verify infrastructure health (database, network, resources)\n\n        Step 2: DIAGNOSE\n        ----------------\n        - Identify root cause (data quality, drift, configuration, infrastructure)\n        - Determine if issue is transient or systematic\n        - Assess whether model can be salvaged or needs retraining\n\n        Step 3: FIX\n        -----------\n        - Repair data pipeline if corrupted\n        - Adjust hyperparameters if misconfigured\n        - Retrain model if fundamentally degraded\n        - Fix infrastructure if unstable\n\n        Step 4: VALIDATE\n        ----------------\n        - Test fix on validation data (ensure accuracy > threshold)\n        - Verify data quality checks pass\n        - Confirm drift scores are within acceptable range\n        - Run safety checks manually before force_resume()\n\n        Step 5: RESUME\n        --------------\n        - Call force_resume() to close circuit breaker\n        - Monitor initial checks closely (ensure they pass)\n        - Watch for consecutive failures (circuit may reopen)\n        - Document incident for post-mortem analysis\n\n        Step 6: MONITOR\n        ---------------\n        - Track accuracy metrics for next 24-48 hours\n        - Watch for circuit breaker state transitions\n        - Verify model performance remains stable\n        - Conduct post-mortem to prevent recurrence\n\n        ALTERNATIVE: AUTOMATIC RECOVERY\n\n        The circuit breaker can also close automatically if a safety check passes\n        after being PAUSED. This happens in _handle_success() when status == PAUSED.\n\n        Automatic recovery occurs when:\n        - Underlying issue self-resolves (temporary drift, transient error)\n        - Next model update happens to pass safety checks\n        - No manual intervention was needed\n\n        However, automatic recovery should NOT be relied upon for systematic issues.\n        If consecutive failures indicated a real problem, force_resume() with\n        investigation is the safer approach.\n\n        Callbacks:\n        Pause callbacks are invoked to notify integrated systems:\n        - ModelManager: Stop processing new model updates\n        - Monitoring: Trigger critical alerts (PagerDuty, Slack, etc.)\n        - Audit Log: Record circuit breaker state change for compliance\n        - Dashboard: Update UI to show PAUSED state\n        \"\"\"\n        # Set circuit breaker to OPEN (PAUSED) state\n        self._status = SafetyStatus.PAUSED\n        self._times_paused += 1\n\n        logger.warning(\n            \"Learning paused by safety bounds\",\n            extra={\n                \"consecutive_failures\": self._consecutive_failures,\n                \"times_paused\": self._times_paused,\n            },\n        )\n\n        # CIRCUIT BREAKER EVENT: Notify registered callbacks\n        # These callbacks should trigger operational response:\n        # - Alert on-call engineer (critical system state)\n        # - Update monitoring dashboards (circuit is now OPEN)\n        # - Log to audit trail (compliance requirement)\n        # - Block new model updates in ModelManager\n        for callback in self._pause_callbacks:\n            try:\n                callback()\n            except Exception as e:\n                # Don't let callback errors prevent circuit from opening\n                # Circuit breaker state change is more critical than callback success\n                logger.error(f\"Pause callback error: {e}\")\n\n    def _resume_learning(self) -> None:\n        \"\"\"Resume learning after safety bounds are satisfied.\n\n        CIRCUIT BREAKER CLOSES (OK STATE)\n        ==================================\n\n        This method closes the circuit breaker, resuming normal learning operations.\n        It is called when a safety check passes after the circuit was previously open,\n        or when an operator manually forces resume via force_resume().\n\n        Circuit Breaker Closed State:\n        - Learning operations are enabled (is_learning_allowed() returns True)\n        - Model updates are accepted if they pass safety checks\n        - Consecutive failure counter is reset to 0\n        - System transitions to OK state (normal operation)\n\n        When Does the Circuit Close?\n        1. **Automatic Recovery**: A safety check passes after the circuit was PAUSED\n           - This happens in _handle_success() when status == PAUSED\n           - Indicates the underlying issue has been resolved\n           - System automatically resumes normal operation\n\n        2. **Manual Recovery**: Operator calls force_resume() after investigation\n           - Used when operator has fixed root cause manually\n           - Resets failure counters and degradation tracking\n           - Allows system to return to normal operation under supervision\n\n        Why Automatic Closure is Safe:\n        - Circuit only closes if a safety check PASSES (accuracy above threshold)\n        - This means the model has recovered to acceptable performance\n        - Unlike traditional circuit breakers (which use timeout), ML circuit\n          breakers require evidence of recovery (passing check) before closing\n\n        Why Manual Intervention is Sometimes Required:\n        - If model cannot recover automatically (e.g., fundamental distribution shift)\n        - If data pipeline needs repair (no new valid data arriving)\n        - If thresholds need adjustment (overly conservative settings)\n        - If model needs complete retraining (not just continued learning)\n\n        Integration with ModelManager:\n        When the circuit closes, the ModelManager should:\n        - Resume accepting new model updates (if they pass safety checks)\n        - Continue monitoring for future failures\n        - Log the recovery event for audit trail\n        - Update dashboards to show OK state\n\n        Callbacks:\n        Resume callbacks are invoked to notify integrated systems:\n        - ModelManager: Resume processing model updates\n        - Monitoring: Clear critical alerts, send recovery notification\n        - Audit Log: Record circuit breaker recovery for compliance\n        - Dashboard: Update UI to show OK state\n        \"\"\"\n        # Increment resume counter for metrics tracking\n        self._times_resumed += 1\n\n        logger.info(\n            \"Learning resumed after safety bounds satisfied\",\n            extra={\n                \"times_resumed\": self._times_resumed,\n            },\n        )\n\n        # CIRCUIT BREAKER EVENT: Notify registered callbacks\n        # These callbacks should handle operational recovery:\n        # - Clear critical alerts (system has recovered)\n        # - Update monitoring dashboards (circuit is now CLOSED)\n        # - Log recovery to audit trail (compliance requirement)\n        # - Resume accepting model updates in ModelManager\n        for callback in self._resume_callbacks:\n            try:\n                callback()\n            except Exception as e:\n                # Don't let callback errors prevent circuit from closing\n                # Circuit breaker state change is more critical than callback success\n                logger.error(f\"Resume callback error: {e}\")\n\n    def _generate_alert(\n        self,\n        severity: str,\n        message: str,\n        accuracy: float,\n        action: str,\n    ) -> None:\n        \"\"\"Generate and dispatch a safety alert.\n\n        Args:\n            severity: Alert severity (\"warning\" or \"critical\").\n            message: Alert message.\n            accuracy: Current accuracy value.\n            action: Action taken (\"none\", \"paused_learning\").\n        \"\"\"\n        alert = SafetyAlert(\n            severity=severity,\n            message=message,\n            consecutive_failures=self._consecutive_failures,\n            current_accuracy=accuracy,\n            threshold=self.accuracy_threshold,\n            action_taken=action,\n            context={\n                \"status\": self._status.value,\n                \"total_checks\": self._total_checks,\n                \"failed_checks\": self._failed_checks,\n            },\n        )\n\n        # Add to history\n        self._alert_history.append(alert)\n        if len(self._alert_history) > self._max_alert_history:\n            self._alert_history = self._alert_history[-self._max_alert_history :]\n\n        # Dispatch to callbacks\n        for callback in self._alert_callbacks:\n            try:\n                callback(alert)\n            except Exception as e:\n                logger.error(f\"Alert callback error: {e}\")\n\n    def reset_failures(self) -> None:\n        \"\"\"Reset the consecutive failure counter.\n\n        Use with caution - this bypasses the safety circuit breaker.\n        Should only be called after manual review and intervention.\n        \"\"\"\n        with self._lock:\n            old_failures = self._consecutive_failures\n            self._consecutive_failures = 0\n            self._status = SafetyStatus.OK\n\n            logger.info(\n                \"Safety failures reset manually\",\n                extra={\"old_failures\": old_failures},\n            )\n\n            # Resume if was paused\n            if old_failures >= self.consecutive_failures_limit:\n                self._resume_learning()\n\n    def force_resume(self) -> None:\n        \"\"\"Force resume learning after manual intervention.\n\n        Should only be called after manual review.\n        \"\"\"\n        with self._lock:\n            self.reset_failures()\n            self._last_accuracy = None  # Reset degradation tracking\n\n            logger.info(\"Learning force-resumed after manual intervention\")\n\n    def is_learning_allowed(self) -> bool:\n        \"\"\"Check if learning is currently allowed.\n\n        Returns:\n            True if learning is allowed, False if paused.\n        \"\"\"\n        with self._lock:\n            return self._status != SafetyStatus.PAUSED\n\n    def get_status(self) -> SafetyStatus:\n        \"\"\"Get the current safety status.\n\n        Returns:\n            Current SafetyStatus enum value.\n        \"\"\"\n        with self._lock:\n            return self._status\n\n    def get_consecutive_failures(self) -> int:\n        \"\"\"Get the current consecutive failure count.\n\n        Returns:\n            Number of consecutive failures.\n        \"\"\"\n        with self._lock:\n            return self._consecutive_failures\n\n    def get_metrics(self) -> SafetyMetrics:\n        \"\"\"Get current safety metrics.\n\n        Returns:\n            SafetyMetrics dataclass with all current metrics.\n        \"\"\"\n        with self._lock:\n            return SafetyMetrics(\n                total_checks=self._total_checks,\n                passed_checks=self._passed_checks,\n                failed_checks=self._failed_checks,\n                consecutive_failures=self._consecutive_failures,\n                max_consecutive_failures=self._max_consecutive_failures,\n                times_paused=self._times_paused,\n                times_resumed=self._times_resumed,\n                current_status=self._status,\n                last_check_time=self._last_check_time,\n                last_failure_time=self._last_failure_time,\n            )\n\n    def get_alert_history(self, limit: int = 10) -> List[SafetyAlert]:\n        \"\"\"Get recent safety alerts.\n\n        Args:\n            limit: Maximum number of alerts to return.\n\n        Returns:\n            List of recent SafetyAlert objects.\n        \"\"\"\n        with self._lock:\n            return list(self._alert_history[-limit:])\n\n    def get_config(self) -> Dict[str, Any]:\n        \"\"\"Get the checker configuration.\n\n        Returns:\n            Dictionary with current configuration.\n        \"\"\"\n        return {\n            \"accuracy_threshold\": self.accuracy_threshold,\n            \"consecutive_failures_limit\": self.consecutive_failures_limit,\n            \"min_samples_for_check\": self.min_samples_for_check,\n            \"enable_auto_pause\": self.enable_auto_pause,\n            \"degradation_threshold\": self.degradation_threshold,\n        }\n\n    def update_threshold(self, new_threshold: float) -> None:\n        \"\"\"Update the accuracy threshold.\n\n        Args:\n            new_threshold: New accuracy threshold (0.0-1.0).\n        \"\"\"\n        if not 0.0 <= new_threshold <= 1.0:\n            raise ValueError(f\"accuracy_threshold must be between 0 and 1, got {new_threshold}\")\n\n        with self._lock:\n            old_threshold = self.accuracy_threshold\n            self.accuracy_threshold = new_threshold\n\n            logger.info(\n                \"Safety threshold updated\",\n                extra={\n                    \"old_threshold\": old_threshold,\n                    \"new_threshold\": new_threshold,\n                },\n            )\n\n    def register_alert_callback(self, callback: Callable[[SafetyAlert], None]) -> None:\n        \"\"\"Register a callback for safety alerts.\n\n        Callbacks are invoked when safety violations occur.\n\n        Args:\n            callback: Function to call with SafetyAlert object.\n        \"\"\"\n        with self._lock:\n            self._alert_callbacks.append(callback)\n\n    def unregister_alert_callback(self, callback: Callable[[SafetyAlert], None]) -> bool:\n        \"\"\"Unregister an alert callback.\n\n        Args:\n            callback: The callback to remove.\n\n        Returns:\n            True if callback was found and removed.\n        \"\"\"\n        with self._lock:\n            try:\n                self._alert_callbacks.remove(callback)\n                return True\n            except ValueError:\n                return False\n\n    def register_pause_callback(self, callback: Callable[[], None]) -> None:\n        \"\"\"Register a callback for when learning is paused.\n\n        Args:\n            callback: Function to call when learning is paused.\n        \"\"\"\n        with self._lock:\n            self._pause_callbacks.append(callback)\n\n    def unregister_pause_callback(self, callback: Callable[[], None]) -> bool:\n        \"\"\"Unregister a pause callback.\n\n        Args:\n            callback: The callback to remove.\n\n        Returns:\n            True if callback was found and removed.\n        \"\"\"\n        with self._lock:\n            try:\n                self._pause_callbacks.remove(callback)\n                return True\n            except ValueError:\n                return False\n\n    def register_resume_callback(self, callback: Callable[[], None]) -> None:\n        \"\"\"Register a callback for when learning is resumed.\n\n        Args:\n            callback: Function to call when learning is resumed.\n        \"\"\"\n        with self._lock:\n            self._resume_callbacks.append(callback)\n\n    def unregister_resume_callback(self, callback: Callable[[], None]) -> bool:\n        \"\"\"Unregister a resume callback.\n\n        Args:\n            callback: The callback to remove.\n\n        Returns:\n            True if callback was found and removed.\n        \"\"\"\n        with self._lock:\n            try:\n                self._resume_callbacks.remove(callback)\n                return True\n            except ValueError:\n                return False\n\n    def reset(self) -> None:\n        \"\"\"Reset the checker to initial state.\n\n        Warning: This clears all state and metrics.\n        \"\"\"\n        with self._lock:\n            self._consecutive_failures = 0\n            self._status = SafetyStatus.OK\n            self._last_accuracy = None\n            self._total_checks = 0\n            self._passed_checks = 0\n            self._failed_checks = 0\n            self._max_consecutive_failures = 0\n            self._times_paused = 0\n            self._times_resumed = 0\n            self._last_check_time = None\n            self._last_failure_time = None\n            self._alert_history.clear()\n\n            logger.info(\"SafetyBoundsChecker reset to initial state\")\n\n    def __repr__(self) -> str:\n        \"\"\"String representation of the checker.\"\"\"\n        return (\n            f\"SafetyBoundsChecker(\"\n            f\"threshold={self.accuracy_threshold:.2f}, \"\n            f\"failures={self._consecutive_failures}/{self.consecutive_failures_limit}, \"\n            f\"status={self._status.value})\"\n        )\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.196763",
  "last_updated": "2026-01-04T05:35:59.105960"
}