{
  "file_path": "src/core/shared/tiered_cache.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Tiered Cache Manager\nConstitutional Hash: cdd01ef066bc6cf2\n\nCoordinates L1 (in-process), L2 (Redis), and L3 (distributed) caches with\nintelligent tier promotion/demotion based on access patterns.\n\nTier Architecture:\n- L1: In-process TTLCache for ultra-hot data (<0.1ms target latency)\n- L2: Redis for shared caching across instances (1-50ms latency)\n- L3: Distributed cache for cold data and fallback (10-1000ms latency)\n\nPromotion Logic:\n- Data accessed >10 times/minute automatically promotes to L1\n- Data accessed <1 time/hour demotes to L3\n\nGraceful Degradation:\n- When Redis (L2) is unavailable, system falls back to L1 + L3\n- No exceptions are raised; degraded mode is logged\n\nUsage:\n    from shared.tiered_cache import TieredCacheManager, get_tiered_cache\n\n    # Direct usage (synchronous L1+L3)\n    mgr = TieredCacheManager()\n    mgr.get('key')  # Fast sync access to L1 and L3\n\n    # Full async usage (L1+L2+L3)\n    mgr = TieredCacheManager()\n    await mgr.initialize()  # Initialize Redis connection\n    await mgr.set('key', 'value')\n    value = await mgr.get_async('key')  # Includes Redis L2 tier\n\n    # Singleton pattern\n    mgr = get_tiered_cache()\n\nAccess Frequency Tracking:\n    Keys accessed frequently (>10 times/minute) are automatically promoted\n    to L1 tier for optimal performance. Use get_tier(key) to check current tier.\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport threading\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, TypeVar\n\nfrom shared.cache_metrics import (\n    CACHE_ENTRIES,\n    CACHE_HITS_TOTAL,\n    CACHE_MISSES_TOTAL,\n    L1_LATENCY,\n    L2_LATENCY,\n    L3_LATENCY,\n    record_cache_hit,\n    record_cache_latency,\n    record_cache_miss,\n    record_demotion,\n    record_fallback,\n    record_promotion,\n    set_tier_health,\n    update_cache_size,\n)\nfrom shared.l1_cache import L1Cache\nfrom shared.metrics import (\n    _get_or_create_counter,\n    _get_or_create_gauge,\n)\nfrom shared.redis_config import (\n    CONSTITUTIONAL_HASH,\n    RedisConfig,\n    RedisHealthState,\n    get_redis_config,\n)\n\n# ============================================================================\n# Tiered Cache Prometheus Metrics\n# ============================================================================\n\n# Redis failure counter\nTIERED_CACHE_REDIS_FAILURES = _get_or_create_counter(\n    \"tiered_cache_redis_failures_total\",\n    \"Total Redis failures in tiered cache\",\n    [\"cache_name\"],\n)\n\n# Degraded mode gauge (1 = degraded, 0 = normal)\nTIERED_CACHE_DEGRADED = _get_or_create_gauge(\n    \"tiered_cache_degraded\",\n    \"Whether tiered cache is running in degraded mode (1=degraded, 0=normal)\",\n    [\"cache_name\"],\n)\n\n# ============================================================================\n# Backward Compatibility Aliases\n# These provide backward compatibility for any code using the old metric names\n# ============================================================================\n\n# Per-tier hit/miss counters - now use cache_metrics.py centralized metrics\nTIERED_CACHE_HITS = CACHE_HITS_TOTAL\nTIERED_CACHE_MISSES = CACHE_MISSES_TOTAL\n\n# Per-tier size gauges - now use cache_metrics.py centralized metrics\nTIERED_CACHE_SIZE = CACHE_ENTRIES\n\n# Tier-specific latency histograms from cache_metrics.py\n# These have optimized bucket configurations for each tier's latency profile\nTIERED_CACHE_LATENCY = {\n    \"L1\": L1_LATENCY,  # Buckets: [0.00001, 0.00005, 0.0001, 0.0005, 0.001] (sub-ms)\n    \"L2\": L2_LATENCY,  # Buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1] (1-100ms)\n    \"L3\": L3_LATENCY,  # Buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1.0] (10-1000ms)\n}\n\ntry:\n    import redis.asyncio as aioredis\nexcept ImportError:\n    aioredis = None\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\nclass CacheTier(Enum):\n    \"\"\"Cache tier identifiers.\"\"\"\n\n    L1 = \"L1\"  # In-process cache (fastest)\n    L2 = \"L2\"  # Redis cache (shared)\n    L3 = \"L3\"  # Distributed/persistent cache (slowest)\n    NONE = \"NONE\"  # Not cached in any tier\n\n\n@dataclass\nclass TieredCacheConfig:\n    \"\"\"Configuration for tiered cache manager.\"\"\"\n\n    # L1 configuration\n    l1_maxsize: int = 1024\n    l1_ttl: int = 300  # 5 minutes (must be <= L2 TTL)\n\n    # L2 configuration\n    l2_ttl: int = 3600  # 1 hour\n    redis_url: Optional[str] = None\n\n    # L3 configuration\n    l3_ttl: int = 86400  # 24 hours\n    l3_enabled: bool = True\n\n    # Promotion/demotion thresholds\n    promotion_threshold: int = 10  # Accesses per minute to promote to L1\n    demotion_threshold_hours: float = 1.0  # Hours without access to demote to L3\n\n    # Serialization\n    serialize: bool = True  # JSON serialize for type consistency across tiers\n\n\n@dataclass\nclass TieredCacheStats:\n    \"\"\"Statistics for tiered cache operations.\"\"\"\n\n    l1_hits: int = 0\n    l1_misses: int = 0\n    l2_hits: int = 0\n    l2_misses: int = 0\n    l3_hits: int = 0\n    l3_misses: int = 0\n    promotions: int = 0\n    demotions: int = 0\n    redis_failures: int = 0\n\n    @property\n    def total_hits(self) -> int:\n        \"\"\"Total hits across all tiers.\"\"\"\n        return self.l1_hits + self.l2_hits + self.l3_hits\n\n    @property\n    def total_misses(self) -> int:\n        \"\"\"Total misses across all tiers.\"\"\"\n        return self.l1_misses + self.l2_misses + self.l3_misses\n\n    @property\n    def hit_ratio(self) -> float:\n        \"\"\"Overall hit ratio.\"\"\"\n        total = self.total_hits + self.total_misses\n        return self.total_hits / total if total > 0 else 0.0\n\n    @property\n    def l1_hit_ratio(self) -> float:\n        \"\"\"L1 hit ratio.\"\"\"\n        total = self.l1_hits + self.l1_misses\n        return self.l1_hits / total if total > 0 else 0.0\n\n\n@dataclass\nclass AccessRecord:\n    \"\"\"Tracks access patterns for a cache key.\"\"\"\n\n    key: str\n    access_times: List[float] = field(default_factory=list)\n    last_access: float = field(default_factory=time.time)\n    current_tier: CacheTier = CacheTier.NONE\n\n    def record_access(self) -> None:\n        \"\"\"Record a new access.\"\"\"\n        now = time.time()\n        self.last_access = now\n        # Keep only accesses from the last minute for promotion calculation\n        cutoff = now - 60\n        self.access_times = [t for t in self.access_times if t >= cutoff]\n        self.access_times.append(now)\n\n    @property\n    def accesses_per_minute(self) -> int:\n        \"\"\"Count of accesses in the last minute.\"\"\"\n        now = time.time()\n        cutoff = now - 60\n        return sum(1 for t in self.access_times if t >= cutoff)\n\n    @property\n    def hours_since_access(self) -> float:\n        \"\"\"Hours since last access.\"\"\"\n        return (time.time() - self.last_access) / 3600\n\n\nclass TieredCacheManager:\n    \"\"\"\n    Multi-tier cache manager coordinating L1, L2, and L3 caches.\n\n    Provides intelligent tier management with:\n    - Automatic promotion of hot data to L1\n    - Demotion of cold data to L3\n    - Graceful degradation when Redis is unavailable\n    - Per-tier statistics and metrics\n\n    Thread Safety:\n    - L1 cache uses internal threading.Lock\n    - L2 (Redis) operations are async and connection-pooled\n    - Access tracking uses its own lock\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[TieredCacheConfig] = None,\n        name: str = \"default\",\n    ):\n        \"\"\"\n        Initialize tiered cache manager.\n\n        Args:\n            config: Optional TieredCacheConfig for customization\n            name: Cache name for metrics and logging\n        \"\"\"\n        self.config = config or TieredCacheConfig()\n        self.name = name\n\n        # Ensure L1 TTL <= L2 TTL (requirement from spec)\n        if self.config.l1_ttl > self.config.l2_ttl:\n            self.config.l1_ttl = self.config.l2_ttl\n            logger.warning(\n                f\"[{CONSTITUTIONAL_HASH}] L1 TTL adjusted to {self.config.l1_ttl}s \"\n                f\"to not exceed L2 TTL\"\n            )\n\n        # L1 cache (in-process)\n        self._l1_cache = L1Cache(\n            maxsize=self.config.l1_maxsize,\n            ttl=self.config.l1_ttl,\n            serialize=self.config.serialize,\n        )\n\n        # L2 Redis client (initialized async)\n        self._l2_client: Optional[Any] = None\n        self._redis_config: RedisConfig = get_redis_config()\n\n        # L3 cache (in-memory simulation for now, to be extended)\n        # In production, this could be backed by a distributed store like Memcached\n        # or a persistent store like PostgreSQL\n        self._l3_cache: Dict[str, Dict[str, Any]] = {}\n        self._l3_lock = threading.Lock()\n\n        # Access tracking for promotion/demotion\n        self._access_records: Dict[str, AccessRecord] = {}\n        self._access_lock = threading.Lock()\n\n        # Statistics\n        self._stats = TieredCacheStats()\n        self._stats_lock = threading.Lock()\n\n        # Degraded mode tracking\n        self._l2_degraded = False\n        self._last_l2_failure: float = 0.0\n        self._l2_recovery_interval: float = 30.0  # Try to recover every 30 seconds\n\n        # Register for Redis health changes\n        self._redis_config.register_health_callback(self._on_redis_health_change)\n\n        # Initialize tier health metrics - L1 and L3 always healthy at startup\n        set_tier_health(\"L1\", True)\n        if self.config.l3_enabled:\n            set_tier_health(\"L3\", True)\n\n        logger.info(\n            f\"[{CONSTITUTIONAL_HASH}] TieredCacheManager '{name}' initialized: \"\n            f\"L1(maxsize={self.config.l1_maxsize}, ttl={self.config.l1_ttl}s), \"\n            f\"L2(ttl={self.config.l2_ttl}s), L3(enabled={self.config.l3_enabled})\"\n        )\n\n    async def initialize(self) -> bool:\n        \"\"\"\n        Initialize async components (Redis L2 connection).\n\n        Returns:\n            True if all tiers initialized successfully\n        \"\"\"\n        l2_ready = await self._initialize_l2()\n\n        if not l2_ready:\n            logger.warning(\n                f\"[{CONSTITUTIONAL_HASH}] L2 (Redis) not available, \"\n                f\"running in degraded mode (L1 + L3 only)\"\n            )\n            self._l2_degraded = True\n            # Set L2 tier health to unhealthy\n            set_tier_health(\"L2\", False)\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n        else:\n            # Set L2 tier health to healthy\n            set_tier_health(\"L2\", True)\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(0)\n\n        return l2_ready\n\n    async def _initialize_l2(self) -> bool:\n        \"\"\"Initialize L2 Redis connection.\"\"\"\n        if aioredis is None:\n            logger.warning(\"redis-py[async] not installed, L2 cache disabled\")\n            return False\n\n        try:\n            redis_url = self.config.redis_url or RedisConfig.get_url()\n            self._l2_client = aioredis.from_url(\n                redis_url,\n                decode_responses=True,  # Avoid bytes/str confusion\n            )\n            await self._l2_client.ping()\n            logger.info(f\"[{CONSTITUTIONAL_HASH}] L2 Redis connection established\")\n            return True\n        except Exception as e:\n            logger.warning(f\"[{CONSTITUTIONAL_HASH}] L2 Redis initialization failed: {e}\")\n            self._l2_client = None\n            return False\n\n    async def close(self) -> None:\n        \"\"\"Close connections and cleanup resources.\"\"\"\n        if self._l2_client:\n            try:\n                await self._l2_client.close()\n            except Exception as e:\n                logger.warning(f\"Error closing L2 Redis: {e}\")\n            self._l2_client = None\n\n        logger.info(f\"[{CONSTITUTIONAL_HASH}] TieredCacheManager '{self.name}' closed\")\n\n    def _on_redis_health_change(\n        self, old_state: RedisHealthState, new_state: RedisHealthState\n    ) -> None:\n        \"\"\"\n        Handle Redis health state changes for graceful degradation.\n\n        Args:\n            old_state: Previous Redis health state\n            new_state: New Redis health state\n        \"\"\"\n        if new_state == RedisHealthState.UNHEALTHY:\n            self._l2_degraded = True\n            self._last_l2_failure = time.time()\n            # Update degraded mode gauge and tier health\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n            set_tier_health(\"L2\", False)\n            # Record fallback event\n            record_fallback(\"L2\", \"L3\", self.name)\n            logger.warning(\n                f\"[{CONSTITUTIONAL_HASH}] Redis unhealthy, switching to degraded mode (L1 + L3)\"\n            )\n        elif new_state == RedisHealthState.HEALTHY and self._l2_degraded:\n            self._l2_degraded = False\n            self._last_l2_failure = 0.0\n            # Update degraded mode gauge and tier health\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(0)\n            set_tier_health(\"L2\", True)\n            logger.info(\n                f\"[{CONSTITUTIONAL_HASH}] Redis recovered, resuming normal tiered operation\"\n            )\n\n    def _should_try_l2_recovery(self) -> bool:\n        \"\"\"\n        Check if enough time has passed to attempt L2 recovery.\n\n        Returns:\n            True if recovery should be attempted\n        \"\"\"\n        if not self._l2_degraded:\n            return False\n        if self._l2_client is None:\n            return False\n        return time.time() - self._last_l2_failure >= self._l2_recovery_interval\n\n    async def _try_l2_recovery(self) -> bool:\n        \"\"\"\n        Attempt to recover L2 Redis connection.\n\n        Called periodically when in degraded mode to check if Redis is back.\n\n        Returns:\n            True if recovery succeeded\n        \"\"\"\n        if not self._l2_degraded or self._l2_client is None:\n            return not self._l2_degraded\n\n        try:\n            await self._l2_client.ping()\n            self._l2_degraded = False\n            self._last_l2_failure = 0.0\n            # Update degraded mode gauge and tier health\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(0)\n            set_tier_health(\"L2\", True)\n            logger.info(\n                f\"[{CONSTITUTIONAL_HASH}] L2 Redis connection recovered, \"\n                f\"resuming normal tiered operation\"\n            )\n            return True\n        except Exception as e:\n            # Still down, update failure time for next recovery attempt\n            self._last_l2_failure = time.time()\n            logger.debug(f\"[{CONSTITUTIONAL_HASH}] L2 recovery attempt failed: {e}\")\n            return False\n\n    def get(\n        self,\n        key: str,\n        default: Optional[T] = None,\n    ) -> Optional[T]:\n        \"\"\"\n        Get a value from the tiered cache (synchronous version).\n\n        Lookup order: L1 -> L3 (skips L2 Redis for sync access)\n        Side effects: May promote data to L1 if access frequency threshold met.\n\n        For full async access including L2 Redis, use get_async().\n\n        Args:\n            key: Cache key\n            default: Default value if key not found in any tier\n\n        Returns:\n            Cached value or default\n        \"\"\"\n        # Track access for promotion decisions\n        self._record_access(key)\n\n        # Try L1 first (fastest)\n        value = self._get_from_l1(key)\n        if value is not None:\n            self._check_and_promote(key, value)\n            return value\n\n        # Try L3 (distributed/persistent) - skip L2 for sync\n        value = self._get_from_l3(key)\n        if value is not None:\n            # Check if should promote to L1\n            self._check_and_promote(key, value)\n            return value\n\n        # Check for promotion based on access frequency even for misses\n        # This allows tier tracking to reflect access patterns\n        self._check_and_promote_tier_only(key)\n\n        # Miss in all tiers\n        return default\n\n    async def get_async(\n        self,\n        key: str,\n        default: Optional[T] = None,\n    ) -> Optional[T]:\n        \"\"\"\n        Get a value from the tiered cache (async version with L2 Redis).\n\n        Lookup order: L1 -> L2 -> L3\n        Side effects: May promote data to L1 if access frequency threshold met.\n\n        Args:\n            key: Cache key\n            default: Default value if key not found in any tier\n\n        Returns:\n            Cached value or default\n        \"\"\"\n        # Track access for promotion decisions\n        self._record_access(key)\n\n        # Try L1 first (fastest)\n        value = self._get_from_l1(key)\n        if value is not None:\n            self._check_and_promote(key, value)\n            return value\n\n        # Try L2 (Redis)\n        value = await self._get_from_l2(key)\n        if value is not None:\n            # Check if should promote to L1\n            self._check_and_promote(key, value)\n            return value\n\n        # Try L3 (distributed/persistent)\n        value = self._get_from_l3(key)\n        if value is not None:\n            # Check if should promote\n            self._check_and_promote(key, value)\n            return value\n\n        # Check for promotion based on access frequency even for misses\n        self._check_and_promote_tier_only(key)\n\n        # Miss in all tiers\n        return default\n\n    async def set(\n        self,\n        key: str,\n        value: Any,\n        ttl: Optional[int] = None,\n        tier: Optional[CacheTier] = None,\n    ) -> None:\n        \"\"\"\n        Set a value in the tiered cache.\n\n        By default, writes to L2 (Redis) and selectively to L1 for hot data.\n\n        Args:\n            key: Cache key\n            value: Value to cache\n            ttl: Optional TTL override\n            tier: Optional specific tier to write to (for cache warming)\n        \"\"\"\n        # Record access\n        self._record_access(key)\n\n        # Serialize value if needed\n        serialized = self._serialize(value)\n\n        # Determine target tier\n        if tier == CacheTier.L1:\n            self._set_in_l1(key, serialized)\n        elif tier == CacheTier.L2:\n            await self._set_in_l2(key, serialized, ttl)\n        elif tier == CacheTier.L3:\n            self._set_in_l3(key, serialized, ttl)\n        else:\n            # Default: write to L2, and L1 if hot\n            await self._set_in_l2(key, serialized, ttl)\n\n            # Also set in L1 if this is a hot key\n            if self._should_promote_to_l1(key):\n                self._set_in_l1(key, serialized)\n                self._update_tier(key, CacheTier.L1)\n\n    async def delete(self, key: str) -> bool:\n        \"\"\"\n        Delete a key from all cache tiers.\n\n        Gracefully handles Redis unavailability - continues to delete\n        from L1 and L3 even if L2 fails.\n\n        Args:\n            key: Cache key to delete\n\n        Returns:\n            True if key was deleted from at least one tier\n        \"\"\"\n        deleted = False\n\n        # Delete from L1\n        if self._l1_cache.delete(key):\n            deleted = True\n\n        # Delete from L2 (graceful degradation)\n        if self._l2_client and not self._l2_degraded:\n            try:\n                result = await self._l2_client.delete(key)\n                if result:\n                    deleted = True\n            except Exception as e:\n                logger.warning(f\"[{CONSTITUTIONAL_HASH}] L2 delete failed for key '{key}': {e}\")\n                with self._stats_lock:\n                    self._stats.redis_failures += 1\n                # Emit Redis failure metric\n                TIERED_CACHE_REDIS_FAILURES.labels(cache_name=self.name).inc()\n                self._l2_degraded = True\n                self._last_l2_failure = time.time()\n                # Update degraded mode gauge\n                TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n\n        # Delete from L3\n        with self._l3_lock:\n            if key in self._l3_cache:\n                del self._l3_cache[key]\n                deleted = True\n\n        # Clear access record\n        with self._access_lock:\n            self._access_records.pop(key, None)\n\n        return deleted\n\n    async def exists(self, key: str) -> bool:\n        \"\"\"\n        Check if a key exists in any cache tier.\n\n        Gracefully handles Redis unavailability - continues to check\n        L1 and L3 even if L2 fails.\n\n        Args:\n            key: Cache key\n\n        Returns:\n            True if key exists in any tier\n        \"\"\"\n        # Check L1\n        if self._l1_cache.exists(key):\n            return True\n\n        # Check L2 (graceful degradation)\n        if self._l2_client and not self._l2_degraded:\n            try:\n                exists = await self._l2_client.exists(key)\n                if exists:\n                    return True\n            except Exception as e:\n                logger.warning(f\"[{CONSTITUTIONAL_HASH}] L2 exists check failed: {e}\")\n                with self._stats_lock:\n                    self._stats.redis_failures += 1\n                # Emit Redis failure metric\n                TIERED_CACHE_REDIS_FAILURES.labels(cache_name=self.name).inc()\n                self._l2_degraded = True\n                self._last_l2_failure = time.time()\n                # Update degraded mode gauge\n                TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n\n        # Check L3\n        with self._l3_lock:\n            if key in self._l3_cache:\n                cached = self._l3_cache[key]\n                if time.time() - cached.get(\"timestamp\", 0) < self.config.l3_ttl:\n                    return True\n\n        return False\n\n    def get_tier(self, key: str) -> str:\n        \"\"\"\n        Get the current tier for a key.\n\n        Args:\n            key: Cache key\n\n        Returns:\n            Tier name (L1, L2, L3, or NONE)\n        \"\"\"\n        with self._access_lock:\n            record = self._access_records.get(key)\n            if record:\n                return record.current_tier.value\n        return CacheTier.NONE.value\n\n    def get_access_stats(self, key: str) -> Dict[str, Any]:\n        \"\"\"\n        Get access statistics for a key.\n\n        Useful for debugging and monitoring tier promotion decisions.\n\n        Args:\n            key: Cache key\n\n        Returns:\n            Dictionary with access frequency and tier info\n        \"\"\"\n        with self._access_lock:\n            record = self._access_records.get(key)\n            if record:\n                return {\n                    \"key\": key,\n                    \"accesses_per_minute\": record.accesses_per_minute,\n                    \"hours_since_access\": record.hours_since_access,\n                    \"current_tier\": record.current_tier.value,\n                    \"promotion_threshold\": self.config.promotion_threshold,\n                    \"would_promote\": record.accesses_per_minute >= self.config.promotion_threshold,\n                }\n        return {\n            \"key\": key,\n            \"accesses_per_minute\": 0,\n            \"hours_since_access\": None,\n            \"current_tier\": CacheTier.NONE.value,\n            \"promotion_threshold\": self.config.promotion_threshold,\n            \"would_promote\": False,\n        }\n\n    async def clear(self) -> None:\n        \"\"\"Clear all caches.\"\"\"\n        # Clear L1\n        self._l1_cache.clear()\n\n        # Clear L2 (careful - only clear our keys, not entire Redis)\n        # In production, use a key prefix pattern\n        if self._l2_client and not self._l2_degraded:\n            logger.warning(\"L2 clear not implemented - would clear all Redis keys\")\n\n        # Clear L3\n        with self._l3_lock:\n            self._l3_cache.clear()\n\n        # Clear access records\n        with self._access_lock:\n            self._access_records.clear()\n\n        logger.info(f\"[{CONSTITUTIONAL_HASH}] TieredCacheManager '{self.name}' cleared\")\n\n    # -------------------------------------------------------------------------\n    # Internal tier operations\n    # -------------------------------------------------------------------------\n\n    def _get_from_l1(self, key: str) -> Optional[Any]:\n        \"\"\"Get from L1 (in-process cache).\"\"\"\n        start_time = time.perf_counter()\n        value = self._l1_cache.get(key)\n        duration = time.perf_counter() - start_time\n\n        # Record operation duration using tier-specific histogram with optimized buckets\n        record_cache_latency(\"L1\", self.name, \"get\", duration)\n\n        with self._stats_lock:\n            if value is not None:\n                self._stats.l1_hits += 1\n                self._update_tier(key, CacheTier.L1)\n                # Emit hit metric using cache_metrics helper\n                record_cache_hit(\"L1\", self.name, \"get\")\n            else:\n                self._stats.l1_misses += 1\n                # Emit miss metric using cache_metrics helper\n                record_cache_miss(\"L1\", self.name, \"get\")\n\n        # Update size gauge using cache_metrics helper\n        update_cache_size(\"L1\", self.name, 0, self._l1_cache.size)\n\n        return self._deserialize(value) if value is not None else None\n\n    async def _get_from_l2(self, key: str) -> Optional[Any]:\n        \"\"\"Get from L2 (Redis cache) with graceful degradation.\"\"\"\n        if not self._l2_client:\n            return None\n\n        # If degraded, check if we should try recovery\n        if self._l2_degraded:\n            if self._should_try_l2_recovery():\n                await self._try_l2_recovery()\n            if self._l2_degraded:\n                # Record fallback event when L2 is unavailable\n                record_fallback(\"L2\", \"L3\", self.name)\n                return None\n\n        start_time = time.perf_counter()\n        try:\n            cached_json = await self._l2_client.get(key)\n            duration = time.perf_counter() - start_time\n\n            # Record operation duration using tier-specific histogram\n            record_cache_latency(\"L2\", self.name, \"get\", duration)\n\n            if cached_json:\n                cached = json.loads(cached_json)\n                with self._stats_lock:\n                    self._stats.l2_hits += 1\n                self._update_tier(key, CacheTier.L2)\n                # Emit hit metric using cache_metrics helper\n                record_cache_hit(\"L2\", self.name, \"get\")\n                return cached.get(\"data\")\n            else:\n                with self._stats_lock:\n                    self._stats.l2_misses += 1\n                # Emit miss metric using cache_metrics helper\n                record_cache_miss(\"L2\", self.name, \"get\")\n        except Exception as e:\n            duration = time.perf_counter() - start_time\n            # Record operation duration even on failure\n            record_cache_latency(\"L2\", self.name, \"get\", duration)\n\n            logger.warning(f\"[{CONSTITUTIONAL_HASH}] L2 get failed for key '{key}': {e}\")\n            with self._stats_lock:\n                self._stats.redis_failures += 1\n            # Emit Redis failure metric\n            TIERED_CACHE_REDIS_FAILURES.labels(cache_name=self.name).inc()\n            # Enter degraded mode - operations continue via L1 + L3\n            self._l2_degraded = True\n            self._last_l2_failure = time.time()\n            # Update degraded mode gauge and tier health\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n            set_tier_health(\"L2\", False)\n            # Record fallback event\n            record_fallback(\"L2\", \"L3\", self.name)\n\n        return None\n\n    def _get_from_l3(self, key: str) -> Optional[Any]:\n        \"\"\"Get from L3 (distributed/persistent cache).\"\"\"\n        if not self.config.l3_enabled:\n            return None\n\n        start_time = time.perf_counter()\n        with self._l3_lock:\n            if key in self._l3_cache:\n                cached = self._l3_cache[key]\n                # Check TTL\n                if time.time() - cached.get(\"timestamp\", 0) < self.config.l3_ttl:\n                    duration = time.perf_counter() - start_time\n                    # Record operation duration using tier-specific histogram\n                    record_cache_latency(\"L3\", self.name, \"get\", duration)\n\n                    with self._stats_lock:\n                        self._stats.l3_hits += 1\n                    self._update_tier(key, CacheTier.L3)\n                    # Emit hit metric using cache_metrics helper\n                    record_cache_hit(\"L3\", self.name, \"get\")\n                    # Update size gauge using cache_metrics helper\n                    update_cache_size(\"L3\", self.name, 0, len(self._l3_cache))\n                    return cached.get(\"data\")\n                else:\n                    # Expired, remove\n                    del self._l3_cache[key]\n\n            duration = time.perf_counter() - start_time\n            # Record operation duration using tier-specific histogram\n            record_cache_latency(\"L3\", self.name, \"get\", duration)\n\n            with self._stats_lock:\n                self._stats.l3_misses += 1\n            # Emit miss metric using cache_metrics helper\n            record_cache_miss(\"L3\", self.name, \"get\")\n            # Update size gauge using cache_metrics helper\n            update_cache_size(\"L3\", self.name, 0, len(self._l3_cache))\n\n        return None\n\n    def _set_in_l1(self, key: str, value: Any) -> None:\n        \"\"\"Set in L1 (in-process cache).\"\"\"\n        start_time = time.perf_counter()\n        self._l1_cache.set(key, value)\n        duration = time.perf_counter() - start_time\n\n        # Record operation duration using tier-specific histogram\n        record_cache_latency(\"L1\", self.name, \"set\", duration)\n\n        # Update size gauge using cache_metrics helper\n        update_cache_size(\"L1\", self.name, 0, self._l1_cache.size)\n\n        self._update_tier(key, CacheTier.L1)\n\n    async def _set_in_l2(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n        \"\"\"Set in L2 (Redis cache) with graceful degradation to L3.\"\"\"\n        if not self._l2_client:\n            # No Redis client, fall back to L3\n            record_fallback(\"L2\", \"L3\", self.name)\n            self._set_in_l3(key, value, ttl)\n            return\n\n        # If degraded, check if we should try recovery\n        if self._l2_degraded:\n            if self._should_try_l2_recovery():\n                await self._try_l2_recovery()\n            if self._l2_degraded:\n                # Still degraded, fall back to L3\n                record_fallback(\"L2\", \"L3\", self.name)\n                self._set_in_l3(key, value, ttl)\n                return\n\n        effective_ttl = ttl or self.config.l2_ttl\n        cache_data = {\"data\": value, \"timestamp\": time.time()}\n\n        start_time = time.perf_counter()\n        try:\n            await self._l2_client.setex(key, effective_ttl, json.dumps(cache_data))\n            duration = time.perf_counter() - start_time\n\n            # Record operation duration using tier-specific histogram\n            record_cache_latency(\"L2\", self.name, \"set\", duration)\n\n            self._update_tier(key, CacheTier.L2)\n        except Exception as e:\n            duration = time.perf_counter() - start_time\n            # Record operation duration even on failure\n            record_cache_latency(\"L2\", self.name, \"set\", duration)\n\n            logger.warning(f\"[{CONSTITUTIONAL_HASH}] L2 set failed for key '{key}': {e}\")\n            with self._stats_lock:\n                self._stats.redis_failures += 1\n            # Emit Redis failure metric\n            TIERED_CACHE_REDIS_FAILURES.labels(cache_name=self.name).inc()\n            # Enter degraded mode and fall back to L3\n            self._l2_degraded = True\n            self._last_l2_failure = time.time()\n            # Update degraded mode gauge and tier health\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n            set_tier_health(\"L2\", False)\n            # Record fallback event\n            record_fallback(\"L2\", \"L3\", self.name)\n            self._set_in_l3(key, value, ttl)\n\n    def _set_in_l3(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n        \"\"\"Set in L3 (distributed/persistent cache).\"\"\"\n        if not self.config.l3_enabled:\n            return\n\n        cache_data = {\n            \"data\": value,\n            \"timestamp\": time.time(),\n            \"ttl\": ttl or self.config.l3_ttl,\n        }\n\n        start_time = time.perf_counter()\n        with self._l3_lock:\n            self._l3_cache[key] = cache_data\n            l3_size = len(self._l3_cache)\n        duration = time.perf_counter() - start_time\n\n        # Record operation duration using tier-specific histogram\n        record_cache_latency(\"L3\", self.name, \"set\", duration)\n\n        # Update size gauge using cache_metrics helper\n        update_cache_size(\"L3\", self.name, 0, l3_size)\n\n        self._update_tier(key, CacheTier.L3)\n\n    # -------------------------------------------------------------------------\n    # Promotion/demotion logic\n    # -------------------------------------------------------------------------\n\n    def _record_access(self, key: str) -> None:\n        \"\"\"Record an access for promotion tracking.\"\"\"\n        with self._access_lock:\n            if key not in self._access_records:\n                self._access_records[key] = AccessRecord(key=key)\n            self._access_records[key].record_access()\n\n    def _should_promote_to_l1(self, key: str) -> bool:\n        \"\"\"Check if a key should be promoted to L1.\"\"\"\n        with self._access_lock:\n            record = self._access_records.get(key)\n            if record:\n                return record.accesses_per_minute >= self.config.promotion_threshold\n        return False\n\n    def _check_and_promote(self, key: str, value: Any) -> None:\n        \"\"\"Check if key should be promoted to L1 and do so if needed.\"\"\"\n        with self._access_lock:\n            record = self._access_records.get(key)\n            if not record:\n                return\n\n            # Promote to L1 if threshold met and not already there\n            if (\n                record.accesses_per_minute >= self.config.promotion_threshold\n                and record.current_tier != CacheTier.L1\n            ):\n                from_tier = record.current_tier.value\n                self._set_in_l1(key, self._serialize(value))\n                record.current_tier = CacheTier.L1\n                with self._stats_lock:\n                    self._stats.promotions += 1\n                # Emit promotion metric using cache_metrics helper\n                record_promotion(from_tier, \"L1\", self.name)\n                logger.debug(\n                    f\"[{CONSTITUTIONAL_HASH}] Promoted key '{key}' to L1 \"\n                    f\"(accesses/min: {record.accesses_per_minute})\"\n                )\n\n    def _check_and_promote_tier_only(self, key: str) -> None:\n        \"\"\"\n        Check if key should be promoted to L1 tier based on access frequency.\n\n        Unlike _check_and_promote, this only updates tier tracking without\n        storing a value. Used for tracking hot keys that don't yet have data.\n        \"\"\"\n        with self._access_lock:\n            record = self._access_records.get(key)\n            if not record:\n                return\n\n            # Mark as L1 tier if threshold met (for future sets and tier queries)\n            if (\n                record.accesses_per_minute >= self.config.promotion_threshold\n                and record.current_tier != CacheTier.L1\n            ):\n                from_tier = record.current_tier.value\n                record.current_tier = CacheTier.L1\n                with self._stats_lock:\n                    self._stats.promotions += 1\n                # Emit promotion metric using cache_metrics helper\n                record_promotion(from_tier, \"L1\", self.name)\n                logger.debug(\n                    f\"[{CONSTITUTIONAL_HASH}] Key '{key}' marked for L1 tier \"\n                    f\"(accesses/min: {record.accesses_per_minute})\"\n                )\n\n    def _update_tier(self, key: str, tier: CacheTier) -> None:\n        \"\"\"Update the tier record for a key.\"\"\"\n        with self._access_lock:\n            if key in self._access_records:\n                self._access_records[key].current_tier = tier\n            else:\n                self._access_records[key] = AccessRecord(key=key, current_tier=tier)\n\n    async def run_demotion_check(self) -> int:\n        \"\"\"\n        Run demotion check to move cold data to L3.\n\n        Returns:\n            Number of keys demoted\n        \"\"\"\n        demoted = 0\n        keys_to_demote = []\n\n        with self._access_lock:\n            for key, record in self._access_records.items():\n                if (\n                    record.hours_since_access >= self.config.demotion_threshold_hours\n                    and record.current_tier in (CacheTier.L1, CacheTier.L2)\n                ):\n                    keys_to_demote.append((key, record))\n\n        for key, record in keys_to_demote:\n            # Get value from current tier and track original tier for metrics\n            value = None\n            from_tier = record.current_tier.value\n            if record.current_tier == CacheTier.L1:\n                value = self._l1_cache.get(key)\n                if value is not None:\n                    self._l1_cache.delete(key)\n            elif record.current_tier == CacheTier.L2 and self._l2_client:\n                try:\n                    cached = await self._l2_client.get(key)\n                    if cached:\n                        value = json.loads(cached).get(\"data\")\n                        await self._l2_client.delete(key)\n                except Exception as e:\n                    logger.warning(f\"Failed to demote key '{key}' from L2: {e}\")\n\n            # Move to L3\n            if value is not None:\n                self._set_in_l3(key, value)\n                with self._access_lock:\n                    if key in self._access_records:\n                        self._access_records[key].current_tier = CacheTier.L3\n                with self._stats_lock:\n                    self._stats.demotions += 1\n                # Emit demotion metric using cache_metrics helper\n                record_demotion(from_tier, \"L3\", self.name)\n                demoted += 1\n\n        if demoted > 0:\n            logger.info(\n                f\"[{CONSTITUTIONAL_HASH}] Demoted {demoted} keys to L3 \"\n                f\"(threshold: {self.config.demotion_threshold_hours}h)\"\n            )\n\n        return demoted\n\n    # -------------------------------------------------------------------------\n    # Serialization\n    # -------------------------------------------------------------------------\n\n    def _serialize(self, value: Any) -> Any:\n        \"\"\"Serialize value if configured.\"\"\"\n        if self.config.serialize and not isinstance(value, str):\n            try:\n                return json.dumps(value)\n            except (TypeError, ValueError):\n                return value\n        return value\n\n    def _deserialize(self, value: Any) -> Any:\n        \"\"\"Deserialize value if configured.\"\"\"\n        if self.config.serialize and isinstance(value, str):\n            try:\n                return json.loads(value)\n            except (TypeError, ValueError, json.JSONDecodeError):\n                return value\n        return value\n\n    # -------------------------------------------------------------------------\n    # Statistics and monitoring\n    # -------------------------------------------------------------------------\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get comprehensive cache statistics.\n\n        Returns:\n            Dictionary with per-tier and aggregate stats\n        \"\"\"\n        with self._stats_lock:\n            return {\n                \"name\": self.name,\n                \"constitutional_hash\": CONSTITUTIONAL_HASH,\n                \"tiers\": {\n                    \"l1\": {\n                        \"hits\": self._stats.l1_hits,\n                        \"misses\": self._stats.l1_misses,\n                        \"hit_ratio\": self._stats.l1_hit_ratio,\n                        \"size\": self._l1_cache.size,\n                        \"maxsize\": self.config.l1_maxsize,\n                    },\n                    \"l2\": {\n                        \"hits\": self._stats.l2_hits,\n                        \"misses\": self._stats.l2_misses,\n                        \"available\": self._l2_client is not None,\n                        \"degraded\": self._l2_degraded,\n                        \"failures\": self._stats.redis_failures,\n                    },\n                    \"l3\": {\n                        \"hits\": self._stats.l3_hits,\n                        \"misses\": self._stats.l3_misses,\n                        \"enabled\": self.config.l3_enabled,\n                        \"size\": len(self._l3_cache),\n                    },\n                },\n                \"aggregate\": {\n                    \"total_hits\": self._stats.total_hits,\n                    \"total_misses\": self._stats.total_misses,\n                    \"hit_ratio\": self._stats.hit_ratio,\n                    \"promotions\": self._stats.promotions,\n                    \"demotions\": self._stats.demotions,\n                },\n                \"config\": {\n                    \"l1_ttl\": self.config.l1_ttl,\n                    \"l2_ttl\": self.config.l2_ttl,\n                    \"l3_ttl\": self.config.l3_ttl,\n                    \"promotion_threshold\": self.config.promotion_threshold,\n                    \"demotion_threshold_hours\": self.config.demotion_threshold_hours,\n                },\n            }\n\n    @property\n    def stats(self) -> TieredCacheStats:\n        \"\"\"Get raw statistics object.\"\"\"\n        return self._stats\n\n    @property\n    def is_l2_available(self) -> bool:\n        \"\"\"Check if L2 (Redis) is available.\"\"\"\n        return self._l2_client is not None and not self._l2_degraded\n\n    @property\n    def is_degraded(self) -> bool:\n        \"\"\"Check if cache is running in degraded mode (L2 unavailable).\"\"\"\n        return self._l2_degraded\n\n    async def check_l2_health(self) -> bool:\n        \"\"\"\n        Explicitly check L2 (Redis) health and attempt recovery if degraded.\n\n        This method can be called periodically or on-demand to check if\n        Redis has recovered and restore normal operation.\n\n        Returns:\n            True if L2 is healthy and available\n        \"\"\"\n        if self._l2_client is None:\n            return False\n\n        if self._l2_degraded:\n            return await self._try_l2_recovery()\n\n        # Not degraded, verify connection is still good\n        try:\n            await self._l2_client.ping()\n            set_tier_health(\"L2\", True)\n            return True\n        except Exception as e:\n            logger.warning(f\"[{CONSTITUTIONAL_HASH}] L2 health check failed: {e}\")\n            with self._stats_lock:\n                self._stats.redis_failures += 1\n            # Emit Redis failure metric\n            TIERED_CACHE_REDIS_FAILURES.labels(cache_name=self.name).inc()\n            self._l2_degraded = True\n            self._last_l2_failure = time.time()\n            # Update degraded mode gauge and tier health\n            TIERED_CACHE_DEGRADED.labels(cache_name=self.name).set(1)\n            set_tier_health(\"L2\", False)\n            # Record fallback event\n            record_fallback(\"L2\", \"L3\", self.name)\n            return False\n\n    def __repr__(self) -> str:\n        \"\"\"String representation.\"\"\"\n        return (\n            f\"TieredCacheManager(name='{self.name}', \"\n            f\"l1_size={self._l1_cache.size}/{self.config.l1_maxsize}, \"\n            f\"l2_available={self.is_l2_available}, \"\n            f\"hit_ratio={self._stats.hit_ratio:.2%})\"\n        )\n\n\n# -----------------------------------------------------------------------------\n# Singleton pattern for shared usage\n# -----------------------------------------------------------------------------\n\n_default_manager: Optional[TieredCacheManager] = None\n_singleton_lock = threading.Lock()\n\n\ndef get_tiered_cache(\n    config: Optional[TieredCacheConfig] = None,\n    name: str = \"default\",\n) -> TieredCacheManager:\n    \"\"\"\n    Get or create the singleton TieredCacheManager instance.\n\n    Thread-safe singleton pattern for shared cache access across the application.\n\n    Args:\n        config: Optional TieredCacheConfig (used only on first call)\n        name: Cache name (used only on first call)\n\n    Returns:\n        TieredCacheManager singleton instance\n    \"\"\"\n    global _default_manager\n\n    if _default_manager is None:\n        with _singleton_lock:\n            if _default_manager is None:\n                _default_manager = TieredCacheManager(config=config, name=name)\n                logger.info(f\"[{CONSTITUTIONAL_HASH}] TieredCacheManager singleton created\")\n\n    return _default_manager\n\n\ndef reset_tiered_cache() -> None:\n    \"\"\"\n    Reset the singleton TieredCacheManager instance.\n\n    Useful for testing or when configuration needs to change.\n    \"\"\"\n    global _default_manager\n\n    with _singleton_lock:\n        if _default_manager is not None:\n            # Close is async, so we run it in an event loop if possible\n            try:\n                loop = asyncio.get_event_loop()\n                if loop.is_running():\n                    asyncio.create_task(_default_manager.close())\n                else:\n                    loop.run_until_complete(_default_manager.close())\n            except RuntimeError:\n                # No event loop, just clear the reference\n                pass\n            _default_manager = None\n            logger.info(f\"[{CONSTITUTIONAL_HASH}] TieredCacheManager singleton reset\")\n\n\n__all__ = [\n    # Constants\n    \"CONSTITUTIONAL_HASH\",\n    # Enums\n    \"CacheTier\",\n    # Classes\n    \"TieredCacheManager\",\n    \"TieredCacheConfig\",\n    \"TieredCacheStats\",\n    \"AccessRecord\",\n    # Functions\n    \"get_tiered_cache\",\n    \"reset_tiered_cache\",\n]\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.248053",
  "last_updated": "2026-01-04T05:35:59.097381"
}