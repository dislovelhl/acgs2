{
  "file_path": "src/core/integrations/nemo_agent_toolkit/constitutional_guardrails.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Constitutional Guardrails for NeMo-Agent-Toolkit\nConstitutional Hash: cdd01ef066bc6cf2\n\nProvides constitutional validation guardrails that integrate with\nNeMo-Agent-Toolkit's agent optimization pipeline.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport json\nimport logging\nfrom dataclasses import dataclass, field\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar\n\nimport httpx\n\nif TYPE_CHECKING:\n    from collections.abc import Awaitable\n\nCONSTITUTIONAL_HASH: str = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\nclass GuardrailAction(str, Enum):\n    \"\"\"Actions a guardrail can take.\"\"\"\n\n    ALLOW = \"allow\"\n    BLOCK = \"block\"\n    MODIFY = \"modify\"\n    ESCALATE = \"escalate\"\n    AUDIT = \"audit\"\n\n\nclass ViolationType(str, Enum):\n    \"\"\"Types of constitutional violations.\"\"\"\n\n    PRIVACY = \"privacy\"\n    SAFETY = \"safety\"\n    ETHICS = \"ethics\"\n    COMPLIANCE = \"compliance\"\n    SECURITY = \"security\"\n    TRANSPARENCY = \"transparency\"\n    FAIRNESS = \"fairness\"\n    ACCOUNTABILITY = \"accountability\"\n\n\n@dataclass\nclass GuardrailConfig:\n    \"\"\"Configuration for constitutional guardrails.\"\"\"\n\n    enabled: bool = True\n    strict_mode: bool = False\n    max_retries: int = 3\n    timeout_seconds: float = 5.0\n    colang_version: str = \"2.x\"  # Support for latest Colang 2.x\n    audit_all_requests: bool = True\n    block_on_violation: bool = True\n    escalation_threshold: float = 0.8\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n\n    # Policy configuration\n    privacy_protection: bool = True\n    safety_checks: bool = True\n    ethics_validation: bool = True\n    compliance_enforcement: bool = True\n\n    # PII patterns to detect\n    pii_patterns: list[str] = field(\n        default_factory=lambda: [\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN\n            r\"\\b\\d{16}\\b\",  # Credit card\n            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone\n        ]\n    )\n\n    # NIM Integration\n    use_nim: bool = False\n    nim_endpoint: str = \"http://localhost:8000/v1\"\n    nim_model: str = \"nvidia/nemotron-3-8b-steerlm\"\n    nim_guardrails_enabled: bool = True\n\n    # Bot Reasoning Monitoring\n    monitor_reasoning: bool = True\n    reasoning_validation_threshold: float = 0.9\n\n    def validate(self) -> None:\n        \"\"\"Validate configuration.\"\"\"\n        if self.constitutional_hash != CONSTITUTIONAL_HASH:\n            raise ValueError(\n                f\"Invalid constitutional hash. Expected {CONSTITUTIONAL_HASH}, \"\n                f\"got {self.constitutional_hash}\"\n            )\n\n\n@dataclass\nclass GuardrailResult:\n    \"\"\"Result of a guardrail check.\"\"\"\n\n    action: GuardrailAction\n    allowed: bool\n    violations: list[dict[str, Any]] = field(default_factory=list)\n    modified_content: str | None = None\n    reasoning: str = \"\"\n    confidence: float = 1.0\n    timestamp: datetime = field(default_factory=lambda: datetime.now(UTC))\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n    trace_id: str = \"\"\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"action\": self.action.value,\n            \"allowed\": self.allowed,\n            \"violations\": self.violations,\n            \"modified_content\": self.modified_content,\n            \"reasoning\": self.reasoning,\n            \"confidence\": self.confidence,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"constitutional_hash\": self.constitutional_hash,\n            \"trace_id\": self.trace_id,\n        }\n\n\nclass ConstitutionalGuardrails:\n    \"\"\"\n    Constitutional guardrails for NeMo-Agent-Toolkit integration.\n\n    Provides input/output validation, PII protection, safety checks,\n    and constitutional compliance enforcement for AI agents.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: GuardrailConfig | None = None,\n        acgs2_client: Any | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize constitutional guardrails.\n\n        Args:\n            config: Guardrail configuration\n            acgs2_client: Optional ACGS-2 SDK client for policy validation\n        \"\"\"\n        self.config = config or GuardrailConfig()\n        self.config.validate()\n        self._client = acgs2_client\n        self._violation_handlers: dict[ViolationType, list[Callable]] = {}\n        self._input_validators: list[Callable] = []\n        self._output_validators: list[Callable] = []\n        self._audit_log: list[dict[str, Any]] = []\n        self._compiled_patterns: list[Any] = []\n        self._colang_flows: dict[str, str] = {}\n        self._reasoning_traces: list[dict[str, Any]] = []\n        self._http_client: httpx.AsyncClient | None = None\n\n        self._compile_patterns()\n        self._init_colang_flows()\n\n    async def __aenter__(self):\n        \"\"\"Async context manager entry.\"\"\"\n        if self._http_client is None:\n            self._http_client = httpx.AsyncClient(\n                base_url=self.config.nim_endpoint, timeout=self.config.timeout_seconds\n            )\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Async context manager exit.\"\"\"\n        if self._http_client:\n            await self._http_client.aclose()\n            self._http_client = None\n\n    def _init_colang_flows(self) -> None:\n        \"\"\"Initialize Colang 2.0 flows for constitutional principles.\"\"\"\n        if self.config.colang_version.startswith(\"2\"):\n            colang_path = Path(__file__).parent / \"colang\" / \"constitutional_principles.co\"\n            if colang_path.exists():\n                try:\n                    with open(colang_path, \"r\") as f:\n                        self._colang_flows[\"constitutional_principles\"] = f.read()\n                    logger.info(f\"Loaded external Colang flows from {colang_path}\")\n                except Exception as e:\n                    logger.error(f\"Failed to load external Colang flows: {e}\")\n            else:\n                # Fallback to defaults if file missing\n                self._colang_flows = {\n                    \"privacy_flow\": \"\"\"\nflow monitor privacy check\n    when bot said $text\n    if len(regex_findall(\"pii_patterns\", $text)) > 0\n        abort \"Constitutional violation: Privacy leak detected.\"\n                    \"\"\",\n                    \"safety_flow\": \"\"\"\nflow monitor safety check\n    when bot said $text\n    if contains_unsafe_content($text)\n        abort \"Constitutional violation: Unsafe content detected.\"\n                    \"\"\",\n                }\n\n    def _compile_patterns(self) -> None:\n        \"\"\"Compile PII detection patterns.\"\"\"\n        import re\n\n        self._compiled_patterns = [re.compile(pattern) for pattern in self.config.pii_patterns]\n\n    def _generate_trace_id(self) -> str:\n        \"\"\"Generate a trace ID for audit purposes.\"\"\"\n        timestamp = datetime.now(UTC).isoformat()\n        data = f\"{timestamp}-{CONSTITUTIONAL_HASH}\"\n        return hashlib.sha256(data.encode()).hexdigest()[:16]\n\n    def add_input_validator(\n        self,\n        validator: Callable[[str], Awaitable[GuardrailResult] | GuardrailResult],\n    ) -> None:\n        \"\"\"Add a custom input validator.\"\"\"\n        self._input_validators.append(validator)\n\n    def add_output_validator(\n        self,\n        validator: Callable[[str], Awaitable[GuardrailResult] | GuardrailResult],\n    ) -> None:\n        \"\"\"Add a custom output validator.\"\"\"\n        self._output_validators.append(validator)\n\n    def on_violation(\n        self,\n        violation_type: ViolationType,\n        handler: Callable[[dict[str, Any]], None],\n    ) -> None:\n        \"\"\"Register a violation handler.\"\"\"\n        if violation_type not in self._violation_handlers:\n            self._violation_handlers[violation_type] = []\n        self._violation_handlers[violation_type].append(handler)\n\n    async def check_input(\n        self,\n        content: str,\n        context: dict[str, Any] | None = None,\n    ) -> GuardrailResult:\n        \"\"\"\n        Check input content against constitutional guardrails.\n\n        Args:\n            content: Input content to validate\n            context: Optional context for validation\n\n        Returns:\n            GuardrailResult with validation outcome\n        \"\"\"\n        if not self.config.enabled:\n            return GuardrailResult(\n                action=GuardrailAction.ALLOW,\n                allowed=True,\n                trace_id=self._generate_trace_id(),\n            )\n\n        trace_id = self._generate_trace_id()\n        violations: list[dict[str, Any]] = []\n\n        # Check for PII\n        if self.config.privacy_protection:\n            pii_violations = self._detect_pii(content)\n            violations.extend(pii_violations)\n\n        # Check for safety issues\n        if self.config.safety_checks:\n            safety_violations = await self._check_safety(content)\n            violations.extend(safety_violations)\n\n        # Delegate to NIM if enabled\n        if self.config.use_nim and self.config.nim_guardrails_enabled:\n            nim_violations = await self._check_with_nim_guardrails(content)\n            violations.extend(nim_violations)\n\n        # Run custom validators\n        for validator in self._input_validators:\n            result = validator(content)\n            if asyncio.iscoroutine(result):\n                result = await result\n            if not result.allowed:\n                violations.extend(result.violations)\n\n        # Validate with ACGS-2 if client available\n        if self._client and self.config.compliance_enforcement:\n            compliance_result = await self._validate_with_acgs2(content, context)\n            if not compliance_result.allowed:\n                violations.extend(compliance_result.violations)\n\n        # Determine action\n        if violations:\n            self._handle_violations(violations)\n            if self.config.block_on_violation:\n                action = GuardrailAction.BLOCK\n                allowed = False\n            else:\n                action = GuardrailAction.AUDIT\n                allowed = True\n        else:\n            action = GuardrailAction.ALLOW\n            allowed = True\n\n        result = GuardrailResult(\n            action=action,\n            allowed=allowed,\n            violations=violations,\n            reasoning=self._generate_reasoning(violations),\n            trace_id=trace_id,\n        )\n\n        # Audit logging\n        if self.config.audit_all_requests:\n            self._audit(trace_id, \"input\", content, result)\n\n        return result\n\n    async def check_output(\n        self,\n        content: str,\n        context: dict[str, Any] | None = None,\n        reasoning: str | None = None,\n    ) -> GuardrailResult:\n        \"\"\"\n        Check output content against constitutional guardrails.\n\n        Args:\n            content: Output content to validate\n            context: Optional context for validation\n            reasoning: Optional LLM thinking/reasoning trace to validate\n\n        Returns:\n            GuardrailResult with validation outcome\n        \"\"\"\n        if not self.config.enabled:\n            return GuardrailResult(\n                action=GuardrailAction.ALLOW,\n                allowed=True,\n                trace_id=self._generate_trace_id(),\n            )\n\n        trace_id = self._generate_trace_id()\n        violations: list[dict[str, Any]] = []\n        modified_content: str | None = None\n\n        # Validate reasoning trace if provided\n        if reasoning and self.config.monitor_reasoning:\n            reasoning_violations = await self._validate_reasoning(reasoning, trace_id)\n            violations.extend(reasoning_violations)\n\n        # Check for PII in output\n        if self.config.privacy_protection:\n            pii_violations = self._detect_pii(content)\n            if pii_violations:\n                # Attempt to redact PII from output\n                modified_content = self._redact_pii(content)\n                violations.extend(pii_violations)\n\n        # Check for harmful content\n        if self.config.safety_checks:\n            safety_violations = await self._check_output_safety(content)\n            violations.extend(safety_violations)\n\n        # Run custom validators\n        for validator in self._output_validators:\n            result = validator(content)\n            if asyncio.iscoroutine(result):\n                result = await result\n            if not result.allowed:\n                violations.extend(result.violations)\n                if result.modified_content:\n                    modified_content = result.modified_content\n\n        # Determine action\n        if violations:\n            self._handle_violations(violations)\n            if modified_content:\n                action = GuardrailAction.MODIFY\n                allowed = True\n            elif self.config.block_on_violation:\n                action = GuardrailAction.BLOCK\n                allowed = False\n            else:\n                action = GuardrailAction.AUDIT\n                allowed = True\n        else:\n            action = GuardrailAction.ALLOW\n            allowed = True\n\n        result = GuardrailResult(\n            action=action,\n            allowed=allowed,\n            violations=violations,\n            modified_content=modified_content,\n            reasoning=self._generate_reasoning(violations),\n            trace_id=trace_id,\n        )\n\n        # Audit logging\n        if self.config.audit_all_requests:\n            self._audit(trace_id, \"output\", content, result)\n\n        return result\n\n    def _detect_pii(self, content: str) -> list[dict[str, Any]]:\n        \"\"\"Detect PII in content.\"\"\"\n        violations: list[dict[str, Any]] = []\n        for i, pattern in enumerate(self._compiled_patterns):\n            matches = pattern.findall(content)\n            if matches:\n                violations.append(\n                    {\n                        \"type\": ViolationType.PRIVACY.value,\n                        \"pattern_index\": i,\n                        \"match_count\": len(matches),\n                        \"message\": f\"PII detected: {len(matches)} potential matches\",\n                    }\n                )\n        return violations\n\n    def _redact_pii(self, content: str) -> str:\n        \"\"\"Redact PII from content.\"\"\"\n        redacted = content\n        for pattern in self._compiled_patterns:\n            redacted = pattern.sub(\"[REDACTED]\", redacted)\n        return redacted\n\n    async def _check_safety(self, content: str) -> list[dict[str, Any]]:\n        \"\"\"Check content for safety issues.\"\"\"\n        violations: list[dict[str, Any]] = []\n\n        # Basic safety patterns\n        unsafe_patterns = [\n            (r\"\\b(hack|exploit|attack)\\s+(system|server|database)\\b\", \"security_threat\"),\n            (r\"\\b(delete|drop|truncate)\\s+all\\b\", \"destructive_action\"),\n            (r\"\\bpassword\\s*[:=]\\s*\\S+\", \"credential_exposure\"),\n        ]\n\n        import re\n\n        for pattern, violation_type in unsafe_patterns:\n            if re.search(pattern, content, re.IGNORECASE):\n                violations.append(\n                    {\n                        \"type\": ViolationType.SECURITY.value,\n                        \"subtype\": violation_type,\n                        \"message\": f\"Safety concern detected: {violation_type}\",\n                    }\n                )\n\n        return violations\n\n    async def _check_output_safety(self, content: str) -> list[dict[str, Any]]:\n        \"\"\"Check output content for safety issues.\"\"\"\n        violations: list[dict[str, Any]] = []\n\n        # Check for potential harmful output patterns\n        harmful_patterns = [\n            (r\"\\b(instructions?\\s+to|how\\s+to)\\s+(hack|exploit|attack)\\b\", \"harmful_instructions\"),\n            (r\"\\b(malware|virus|trojan|ransomware)\\s+(code|script)\\b\", \"malicious_content\"),\n        ]\n\n        import re\n\n        for pattern, violation_type in harmful_patterns:\n            if re.search(pattern, content, re.IGNORECASE):\n                violations.append(\n                    {\n                        \"type\": ViolationType.SAFETY.value,\n                        \"subtype\": violation_type,\n                        \"message\": f\"Harmful content detected: {violation_type}\",\n                    }\n                )\n\n        return violations\n\n    async def _check_with_nim_guardrails(self, content: str) -> list[dict[str, Any]]:\n        \"\"\"Check content using NemoGuard NIM microservices.\"\"\"\n        if not self._http_client:\n            logger.warning(\"HTTP client not initialized. Call __aenter__ or initialize manually.\")\n            return []\n\n        violations = []\n        try:\n            response = await self._http_client.post(\n                \"/guardrails\",\n                json={\n                    \"model\": self.config.nim_model,\n                    \"text\": content,\n                    \"guardrails\": [\"content_safety\", \"jailbreak_detection\"],\n                },\n            )\n            response.raise_for_status()\n            result = response.json()\n\n            if not result.get(\"allowed\", True):\n                violations.append(\n                    {\n                        \"type\": ViolationType.SAFETY.value,\n                        \"message\": \"NIM Guardrail identified a violation\",\n                        \"details\": result.get(\"violations\", []),\n                    }\n                )\n        except Exception as e:\n            logger.error(f\"NIM communication error: {e}\")\n            if self.config.strict_mode:\n                violations.append(\n                    {\n                        \"type\": ViolationType.SECURITY.value,\n                        \"message\": f\"NIM service unavailable: {e}\",\n                    }\n                )\n\n        return violations\n\n    async def _validate_reasoning(self, reasoning: str, trace_id: str) -> list[dict[str, Any]]:\n        \"\"\"Validate LLM thinking/reasoning traces.\"\"\"\n        violations = []\n\n        # Capture trace for audit\n        self._reasoning_traces.append(\n            {\n                \"trace_id\": trace_id,\n                \"reasoning\": reasoning,\n                \"timestamp\": datetime.now(UTC).isoformat(),\n            }\n        )\n\n        # Check for suspicious patterns in reasoning (e.g., trying to bypass guardrails)\n        bypass_patterns = [\n            r\"ignore\\s+previous\\s+instructions\",\n            r\"bypass\\s+safety\",\n            r\"constitutional\\s+override\",\n        ]\n\n        import re\n\n        for pattern in bypass_patterns:\n            if re.search(pattern, reasoning, re.IGNORECASE):\n                violations.append(\n                    {\n                        \"type\": ViolationType.SECURITY.value,\n                        \"subtype\": \"reasoning_bypass_attempt\",\n                        \"message\": \"Potential guardrail bypass attempt detected in reasoning trace\",\n                    }\n                )\n\n        return violations\n\n    async def _validate_with_acgs2(\n        self,\n        content: str,\n        context: dict[str, Any] | None,\n    ) -> GuardrailResult:\n        \"\"\"Validate content with ACGS-2 compliance service.\"\"\"\n        if not self._client:\n            return GuardrailResult(\n                action=GuardrailAction.ALLOW,\n                allowed=True,\n            )\n\n        try:\n            # Use ACGS-2 compliance validation\n            from acgs2_sdk import ComplianceService\n\n            compliance = ComplianceService(self._client)\n            result = await compliance.validate_action(\n                agent_id=context.get(\"agent_id\", \"nemo-agent\") if context else \"nemo-agent\",\n                action=\"process_content\",\n                context={\n                    \"content_hash\": hashlib.sha256(content.encode()).hexdigest()[:16],\n                    \"content_length\": len(content),\n                    **(context or {}),\n                },\n            )\n\n            if not result.get(\"compliant\", True):\n                return GuardrailResult(\n                    action=GuardrailAction.BLOCK,\n                    allowed=False,\n                    violations=[\n                        {\n                            \"type\": ViolationType.COMPLIANCE.value,\n                            \"message\": \"ACGS-2 compliance validation failed\",\n                            \"details\": result,\n                        }\n                    ],\n                )\n\n        except Exception as e:\n            logger.warning(f\"ACGS-2 validation failed: {e}\")\n            if self.config.strict_mode:\n                return GuardrailResult(\n                    action=GuardrailAction.BLOCK,\n                    allowed=False,\n                    violations=[\n                        {\n                            \"type\": ViolationType.COMPLIANCE.value,\n                            \"message\": f\"ACGS-2 validation error: {e}\",\n                        }\n                    ],\n                )\n\n        return GuardrailResult(action=GuardrailAction.ALLOW, allowed=True)\n\n    def _handle_violations(self, violations: list[dict[str, Any]]) -> None:\n        \"\"\"Handle detected violations.\"\"\"\n        for violation in violations:\n            violation_type = ViolationType(violation.get(\"type\", \"compliance\"))\n            handlers = self._violation_handlers.get(violation_type, [])\n            for handler in handlers:\n                try:\n                    handler(violation)\n                except Exception as e:\n                    logger.error(f\"Violation handler error: {e}\")\n\n    def _generate_reasoning(self, violations: list[dict[str, Any]]) -> str:\n        \"\"\"Generate human-readable reasoning for the result.\"\"\"\n        if not violations:\n            return \"Content passed all constitutional guardrail checks.\"\n\n        reasons = []\n        for v in violations:\n            reasons.append(f\"- {v.get('type', 'unknown')}: {v.get('message', 'No details')}\")\n\n        return \"Constitutional violations detected:\\n\" + \"\\n\".join(reasons)\n\n    def _audit(\n        self,\n        trace_id: str,\n        direction: str,\n        content: str,\n        result: GuardrailResult,\n    ) -> None:\n        \"\"\"Log audit entry.\"\"\"\n        entry = {\n            \"trace_id\": trace_id,\n            \"direction\": direction,\n            \"content_hash\": hashlib.sha256(content.encode()).hexdigest()[:16],\n            \"content_length\": len(content),\n            \"action\": result.action.value,\n            \"allowed\": result.allowed,\n            \"violation_count\": len(result.violations),\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n        self._audit_log.append(entry)\n        logger.info(f\"Guardrail audit: {json.dumps(entry)}\")\n\n    def get_audit_log(self) -> list[dict[str, Any]]:\n        \"\"\"Get the audit log.\"\"\"\n        return self._audit_log.copy()\n\n    def clear_audit_log(self) -> None:\n        \"\"\"Clear the audit log.\"\"\"\n        self._audit_log.clear()\n\n    async def get_metrics(self) -> dict[str, Any]:\n        \"\"\"Get guardrail metrics.\"\"\"\n        total = len(self._audit_log)\n        if total == 0:\n            return {\n                \"total_checks\": 0,\n                \"allowed_rate\": 1.0,\n                \"violation_rate\": 0.0,\n                \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            }\n\n        allowed = sum(1 for entry in self._audit_log if entry.get(\"allowed\", False))\n        violations = sum(entry.get(\"violation_count\", 0) for entry in self._audit_log)\n\n        return {\n            \"total_checks\": total,\n            \"allowed_count\": allowed,\n            \"blocked_count\": total - allowed,\n            \"allowed_rate\": allowed / total,\n            \"violation_rate\": violations / total if total > 0 else 0.0,\n            \"input_checks\": sum(1 for e in self._audit_log if e.get(\"direction\") == \"input\"),\n            \"output_checks\": sum(1 for e in self._audit_log if e.get(\"direction\") == \"output\"),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.179991",
  "last_updated": "2026-01-04T05:35:59.187318"
}