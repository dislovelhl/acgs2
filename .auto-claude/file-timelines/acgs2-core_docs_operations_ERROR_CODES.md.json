{
  "file_path": "acgs2-core/docs/operations/ERROR_CODES.md",
  "main_branch_history": [],
  "task_views": {
    "060-document-error-codes-and-troubleshooting-for-commo": {
      "task_id": "060-document-error-codes-and-troubleshooting-for-commo",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "# ACGS-2 Error Code Reference\n\n**Constitutional Hash:** cdd01ef066bc6cf2\n**Version:** 1.0.0\n**Created:** 2026-01-03\n**Status:** Production\n**Purpose:** Comprehensive error code reference for operators and developers\n\n---\n\n## Table of Contents\n\n1. [Quick Reference](#quick-reference)\n2. [How to Use This Guide](#how-to-use-this-guide)\n3. [Error Code Structure](#error-code-structure)\n4. [Severity Levels](#severity-levels)\n5. [ACGS-1xxx: Configuration Errors](#acgs-1xxx-configuration-errors)\n6. [ACGS-2xxx: Authentication/Authorization Errors](#acgs-2xxx-authenticationauthorization-errors)\n7. [ACGS-3xxx: Deployment/Infrastructure Errors](#acgs-3xxx-deploymentinfrastructure-errors)\n8. [ACGS-4xxx: Service Integration Errors](#acgs-4xxx-service-integration-errors)\n9. [ACGS-5xxx: Runtime Errors](#acgs-5xxx-runtime-errors)\n10. [ACGS-6xxx: Constitutional/Governance Errors](#acgs-6xxx-constitutionalgovernance-errors)\n11. [ACGS-7xxx: Performance/Resource Errors](#acgs-7xxx-performanceresource-errors)\n12. [ACGS-8xxx: Platform-Specific Errors](#acgs-8xxx-platform-specific-errors)\n13. [Related Documentation](#related-documentation)\n14. [Getting Help](#getting-help)\n\n---\n\n## Quick Reference\n\n### Most Common Errors\n\n| Error Code | Description | Severity | Quick Fix |\n|------------|-------------|----------|-----------|\n| **ACGS-1101** | Missing environment variable | CRITICAL | Add required env var to `.env` file |\n| **ACGS-1301** | Constitutional hash mismatch | CRITICAL | Verify `CONSTITUTIONAL_HASH=cdd01ef066bc6cf2` |\n| **ACGS-2403** | Cannot connect to OPA | CRITICAL | Start OPA container: `docker-compose up -d opa` |\n| **ACGS-3101** | Docker daemon not running | CRITICAL | Start Docker Desktop or `systemctl start docker` |\n| **ACGS-3301** | Port already in use | CRITICAL | Kill process on port or change port config |\n| **ACGS-4101** | Redis connection failed | MEDIUM | Start Redis: `docker-compose up -d redis` |\n| **ACGS-4201** | Kafka connection failed | HIGH | Start Kafka: `docker-compose up -d kafka` |\n| **ACGS-4301** | Database connection failed | CRITICAL | Verify DATABASE_URL and start PostgreSQL |\n\n### Severity Response Times\n\n| Severity | Response Time | Examples |\n|----------|---------------|----------|\n| **CRITICAL** | Immediate (page on-call) | System down, security breach, data loss |\n| **HIGH** | < 1 hour | Service degraded, auth failures |\n| **MEDIUM** | < 4 hours (business hours) | Minor functionality impaired |\n| **LOW** | Best effort | Informational, platform quirks |\n\n---\n\n## How to Use This Guide\n\n### Finding an Error Code\n\n1. **By Error Code**: Use browser search (Ctrl+F / Cmd+F) to find `ACGS-XXXX`\n2. **By Category**: Navigate to the appropriate category section (1xxx, 2xxx, etc.)\n3. **By Service**: Check the service-specific sections for errors in that service\n4. **By Symptom**: Search for keywords like \"connection refused\", \"timeout\", etc.\n\n### Understanding an Error Entry\n\nEach error code entry provides:\n- **Error Code**: ACGS-XXXX identifier\n- **Severity**: CRITICAL, HIGH, MEDIUM, or LOW\n- **Impact**: Effect on system (Deployment-Blocking, Service-Unavailable, etc.)\n- **Description**: What the error means\n- **Common Causes**: Why this error occurs\n- **Symptoms**: How to recognize this error\n- **Resolution**: Step-by-step fix instructions\n- **Example**: Real-world scenario\n- **Related Errors**: Similar or related error codes\n\n### Diagnostic Workflow\n\n```\n1. Identify error code in logs (ACGS-XXXX)\n   \u2193\n2. Look up error code in this document\n   \u2193\n3. Check severity and impact\n   \u2193\n4. Review common causes\n   \u2193\n5. Follow resolution steps\n   \u2193\n6. Verify fix with diagnostic commands\n   \u2193\n7. Document resolution for team\n```\n\n---\n\n## Error Code Structure\n\n### Format\n\n```\nACGS-XYZZ\n```\n\nWhere:\n- `ACGS` = Platform identifier (Agentic Constitutional Governance System)\n- `X` = Major category (1-8)\n- `Y` = Subcategory (0-9)\n- `ZZ` = Specific error (01-99)\n\n### Category Overview\n\n| Code Range | Category | Common Severity | Description |\n|------------|----------|-----------------|-------------|\n| **ACGS-1xxx** | Configuration | CRITICAL-HIGH | Environment vars, config files, constitutional hash |\n| **ACGS-2xxx** | Authentication/Authorization | CRITICAL-HIGH | OPA, webhooks, SSO, RBAC |\n| **ACGS-3xxx** | Deployment/Infrastructure | CRITICAL | Docker, K8s, network, ports |\n| **ACGS-4xxx** | Service Integration | CRITICAL-HIGH | Redis, Kafka, PostgreSQL, OPA |\n| **ACGS-5xxx** | Runtime | HIGH-MEDIUM | Approvals, webhooks, messages |\n| **ACGS-6xxx** | Constitutional/Governance | CRITICAL | Hash validation, MACI, deliberation |\n| **ACGS-7xxx** | Performance/Resource | HIGH-MEDIUM | Latency, exhaustion, throughput |\n| **ACGS-8xxx** | Platform-Specific | LOW-MEDIUM | Windows, macOS, Linux issues |\n\n---\n\n## Severity Levels\n\n### CRITICAL\n\n**When to Escalate**: Immediately (page on-call engineer)\n\n**Characteristics**:\n- System completely down or deployment blocked\n- Security breach or constitutional violation\n- Data loss risk\n- No viable workaround\n\n**Examples**: Constitutional hash mismatch, OPA unavailable, database down, MACI violations\n\n---\n\n### HIGH\n\n**When to Escalate**: < 1 hour (alert engineering team)\n\n**Characteristics**:\n- Core functionality impaired\n- Service degraded but partially operational\n- Authentication/authorization failures\n- Workarounds exist but complex\n\n**Examples**: Webhook auth failures, Redis down (with fallback), Kafka issues, approval chain errors\n\n---\n\n### MEDIUM\n\n**When to Escalate**: < 4 hours (business hours)\n\n**Characteristics**:\n- Non-essential functionality affected\n- Performance degraded within acceptable limits\n- Clear workarounds available\n- Automatic retries working\n\n**Examples**: Cache misses, config warnings, delayed notifications, latency warnings\n\n---\n\n### LOW\n\n**When to Escalate**: Best effort (backlog)\n\n**Characteristics**:\n- No functional impact\n- Platform-specific behavior (not errors)\n- Informational logging\n- Development/debugging information\n\n**Examples**: Platform quirks, deprecation warnings, successful fallbacks\n\n---\n\n## ACGS-1xxx: Configuration Errors\n\n**Category Description**: Errors related to system configuration, environment variables, config files, and constitutional hash validation.\n\n**Common Severity**: HIGH to CRITICAL (deployment-blocking)\n\n**Related Files**: `.env`, `config.yaml`, configuration validators\n\n---\n\n### ACGS-1001: ConfigurationError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: `ConfigurationError` (enhanced-agent-bus)\n\n**Description**: Generic configuration error, base class for configuration issues.\n\n**Common Causes**:\n- Invalid configuration structure\n- Missing required configuration sections\n- Configuration validation failures\n\n**Resolution**:\n1. Review configuration file syntax\n2. Check for missing required fields\n3. Validate against schema/documentation\n4. Restart service after fixing configuration\n\n---\n\n### ACGS-1101: MissingEnvironmentVariableError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n**Exception**: `MissingEnvironmentVariableError` (integration-service)\n\n**Description**: Required environment variable not set. This is the most common deployment error.\n\n**Common Causes**:\n- `.env` file missing or not loaded\n- Required variable not exported\n- Typo in variable name\n- Docker Compose not loading env file\n\n**Symptoms**:\n```\nError: Required environment variable not set: DATABASE_URL\nContainer exited with code 1\n```\n\n**Critical Variables**:\n- `CONSTITUTIONAL_HASH` (must be `cdd01ef066bc6cf2`)\n- `OPA_URL` (e.g., `http://opa:8181`)\n- `REDIS_URL` (e.g., `redis://redis:6379`)\n- `KAFKA_BOOTSTRAP_SERVERS` (e.g., `kafka:19092`)\n- `DATABASE_URL` (PostgreSQL connection string)\n\n**Resolution**:\n1. **Check `.env` file exists**:\n   ```bash\n   ls -la .env\n   ```\n\n2. **Copy from example if missing**:\n   ```bash\n   cp .env.example .env\n   ```\n\n3. **Verify required variables**:\n   ```bash\n   grep -E \"CONSTITUTIONAL_HASH|OPA_URL|REDIS_URL|KAFKA_BOOTSTRAP_SERVERS|DATABASE_URL\" .env\n   ```\n\n4. **Add missing variable**:\n   ```bash\n   echo \"DATABASE_URL=postgresql://user:password@localhost:5432/acgs2\" >> .env\n   ```\n\n5. **Restart service**:\n   ```bash\n   docker-compose restart <service-name>\n   ```\n\n**Example**:\n```bash\n# Missing DATABASE_URL\nERROR: MissingEnvironmentVariableError: DATABASE_URL\n\n# Fix:\necho \"DATABASE_URL=postgresql://acgs2:password@postgres:5432/acgs2_db\" >> .env\ndocker-compose restart hitl-approvals\n```\n\n**Related Errors**: ACGS-1102, ACGS-1103, ACGS-1202\n\n---\n\n### ACGS-1102: InvalidEnvironmentVariableError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n\n**Description**: Environment variable has invalid format or value.\n\n**Common Causes**:\n- Invalid URL format (missing protocol, wrong port)\n- Type mismatch (string vs integer)\n- Value outside acceptable range\n- Wrong connection string format\n\n**Symptoms**:\n```\nError: Invalid environment variable: OPA_URL (must start with http:// or https://)\nInvalid Redis URL format\n```\n\n**Resolution**:\n1. **Check variable format**:\n   ```bash\n   # Wrong: OPA_URL=localhost:8181\n   # Right: OPA_URL=http://opa:8181\n   ```\n\n2. **Verify URL scheme**:\n   - HTTP/HTTPS URLs: `http://` or `https://`\n   - Redis: `redis://` or `rediss://` (SSL)\n   - PostgreSQL: `postgresql://` or `postgres://`\n\n3. **Check for Docker network names**:\n   ```bash\n   # In Docker Compose, use service names not localhost\n   # Wrong: REDIS_URL=redis://localhost:6379\n   # Right: REDIS_URL=redis://redis:6379\n   ```\n\n**Example**:\n```bash\n# Wrong OPA URL (missing protocol)\nOPA_URL=opa:8181\n\n# Fix:\nsed -i 's|OPA_URL=opa:8181|OPA_URL=http://opa:8181|' .env\ndocker-compose restart enhanced-agent-bus\n```\n\n**Related Errors**: ACGS-1101, ACGS-1401, ACGS-1402, ACGS-1403\n\n---\n\n### ACGS-1103: EnvironmentVariableTypeError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n\n**Description**: Environment variable type does not match expected type.\n\n**Common Causes**:\n- String provided where integer expected (e.g., port numbers)\n- Boolean value incorrectly formatted (must be \"true\"/\"false\", not \"yes\"/\"no\")\n- Numeric values with extra characters\n- List/array values not properly comma-separated\n\n**Symptoms**:\n```\nTypeError: Cannot convert 'yes' to boolean\nValueError: invalid literal for int() with base 10: '8080port'\nExpected integer for REDIS_MAX_CONNECTIONS, got 'unlimited'\n```\n\n**Resolution**:\n1. **Check expected type in documentation**:\n   - Integers: Port numbers, timeouts, connection limits\n   - Booleans: Feature flags, debug modes (use \"true\"/\"false\")\n   - URLs: Must include protocol\n   - Lists: Comma-separated values\n\n2. **Common type fixes**:\n   ```bash\n   # Wrong: DEBUG_MODE=yes\n   # Right: DEBUG_MODE=true\n\n   # Wrong: REDIS_PORT=6379port\n   # Right: REDIS_PORT=6379\n\n   # Wrong: REDIS_MAX_CONNECTIONS=unlimited\n   # Right: REDIS_MAX_CONNECTIONS=100\n   ```\n\n3. **Verify numeric values**:\n   ```bash\n   # Ensure no extra characters\n   grep -E \"PORT|TIMEOUT|MAX|LIMIT\" .env\n   ```\n\n**Example**:\n```bash\n# Wrong type\nREDIS_MAX_CONNECTIONS=unlimited\n\n# Fix:\nsed -i 's/REDIS_MAX_CONNECTIONS=unlimited/REDIS_MAX_CONNECTIONS=100/' .env\ndocker-compose restart integration-service\n```\n\n**Related Errors**: ACGS-1101, ACGS-1102\n\n---\n\n### ACGS-1201: ConfigValidationError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: `ConfigValidationError` (integration-service)\n**Location**: `integration-service/src/config/validation.py`\n\n**Description**: Configuration validation failed against defined schema or rules.\n\n**Common Causes**:\n- Invalid URL format in configuration\n- Missing required configuration fields\n- Invalid enum values (e.g., wrong log level)\n- Regex pattern mismatch (e.g., invalid email format)\n- Value constraints violated (min/max ranges)\n\n**Symptoms**:\n```\nConfigValidationError: Invalid configuration\n  - Field 'log_level' must be one of: DEBUG, INFO, WARNING, ERROR\n  - Field 'webhook_url' must match pattern: https?://.*\n  - Field 'timeout' must be between 1 and 300 seconds\n```\n\n**Resolution**:\n1. **Review validation error message** - it will specify which field(s) failed\n\n2. **Check configuration schema**:\n   ```bash\n   # Look for validation rules in code\n   grep -r \"ConfigValidationError\" integration-service/src/config/\n   ```\n\n3. **Common validation fixes**:\n   ```bash\n   # Log level must be valid enum\n   # Wrong: LOG_LEVEL=verbose\n   # Right: LOG_LEVEL=INFO\n\n   # URLs must have protocol\n   # Wrong: WEBHOOK_URL=example.com/hook\n   # Right: WEBHOOK_URL=https://example.com/hook\n\n   # Timeouts must be in valid range\n   # Wrong: REQUEST_TIMEOUT=0\n   # Right: REQUEST_TIMEOUT=30\n   ```\n\n4. **Validate configuration before deployment**:\n   ```bash\n   # Run validation check\n   docker-compose run --rm integration-service python -c \"\n   from src.config.validation import validate_config\n   validate_config()\n   \"\n   ```\n\n**Example**:\n```bash\n# Invalid log level\nLOG_LEVEL=verbose\n\n# Fix:\nsed -i 's/LOG_LEVEL=verbose/LOG_LEVEL=INFO/' .env\ndocker-compose restart integration-service\n```\n\n**Related Errors**: ACGS-1001, ACGS-1202, ACGS-1203\n\n---\n\n### ACGS-1202: ConfigFileNotFoundError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n\n**Description**: Required configuration file not found.\n\n**Common Causes**:\n- `.env` file missing (not created from `.env.example`)\n- Configuration file in wrong directory\n- File path typo in startup script\n- File not mounted in Docker container\n\n**Symptoms**:\n```\nError: Configuration file not found: .env\nFileNotFoundError: [Errno 2] No such file or directory: '/app/.env'\nContainer exits immediately after start\n```\n\n**Resolution**:\n1. **Check if .env file exists**:\n   ```bash\n   ls -la .env\n   ```\n\n2. **Create from example if missing**:\n   ```bash\n   cp .env.example .env\n   ```\n\n3. **Verify file location**:\n   ```bash\n   # .env should be in repository root\n   pwd\n   ls .env\n   ```\n\n4. **For Docker containers, check volume mount**:\n   ```yaml\n   # In docker-compose.yml:\n   services:\n     service-name:\n       volumes:\n         - ./.env:/app/.env  # Ensure this line exists\n   ```\n\n5. **Check file permissions**:\n   ```bash\n   chmod 644 .env\n   ```\n\n**Example**:\n```bash\n# .env missing\nls .env\n# ls: cannot access '.env': No such file or directory\n\n# Fix:\ncp .env.example .env\n# Edit required values\nnano .env\n# Restart services\ndocker-compose up -d\n```\n\n**Related Errors**: ACGS-1101, ACGS-1201\n\n---\n\n### ACGS-1203: ConfigSchemaValidationError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n\n**Description**: Configuration does not match expected schema version or structure.\n\n**Common Causes**:\n- Schema version mismatch (old config with new code)\n- Required fields missing from configuration\n- Invalid field types in config file\n- Nested configuration structure incorrect\n- YAML/JSON syntax errors\n\n**Symptoms**:\n```\nSchemaValidationError: Configuration schema mismatch\n  Expected schema version: 2.0\n  Found schema version: 1.5\n  Missing required fields: ['constitutional_hash', 'opa_config']\n```\n\n**Resolution**:\n1. **Check schema version**:\n   ```bash\n   # Look for version field in config\n   grep -i version config.yaml\n   ```\n\n2. **Update configuration to match schema**:\n   ```bash\n   # Compare with example config\n   diff config.yaml config.example.yaml\n   ```\n\n3. **Add missing required fields**:\n   ```yaml\n   # config.yaml\n   version: \"2.0\"\n   constitutional_hash: \"cdd01ef066bc6cf2\"\n   opa_config:\n     url: \"http://opa:8181\"\n     timeout: 30\n   ```\n\n4. **Validate YAML/JSON syntax**:\n   ```bash\n   # For YAML\n   python -c \"import yaml; yaml.safe_load(open('config.yaml'))\"\n\n   # For JSON\n   python -c \"import json; json.load(open('config.json'))\"\n   ```\n\n5. **Check for common syntax errors**:\n   - Incorrect indentation (YAML)\n   - Missing commas (JSON)\n   - Unquoted strings with special characters\n\n**Example**:\n```bash\n# Old config missing required fields\n# Fix by adding constitutional_hash\necho \"constitutional_hash: cdd01ef066bc6cf2\" >> config.yaml\ndocker-compose restart\n```\n\n**Related Errors**: ACGS-1001, ACGS-1201, ACGS-1202\n\n---\n\n### ACGS-1301: ConstitutionalHashMismatchError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking + Security-Violation\n**Exception**: `ConstitutionalHashMismatchError` (enhanced-agent-bus, sdk)\n\n**Description**: Constitutional hash validation failed. This is a critical safety mechanism that prevents deployment of unapproved code.\n\n**Expected Hash**: `cdd01ef066bc6cf2`\n\n**Common Causes**:\n- Wrong hash in `.env` file\n- Hash not set in environment\n- Kubernetes ConfigMap not updated\n- Typo in hash value\n\n**Symptoms**:\n```\nCRITICAL: Constitutional hash mismatch\nExpected: cdd01ef0...\nReceived: <null> or <wrong-hash>\nDeployment blocked for safety\n```\n\n**Resolution**:\n1. **Verify constitutional hash**:\n   ```bash\n   grep CONSTITUTIONAL_HASH .env\n   ```\n\n2. **Set correct hash**:\n   ```bash\n   # Correct value:\n   CONSTITUTIONAL_HASH=cdd01ef066bc6cf2\n   ```\n\n3. **Update .env file**:\n   ```bash\n   sed -i 's/CONSTITUTIONAL_HASH=.*/CONSTITUTIONAL_HASH=cdd01ef066bc6cf2/' .env\n   ```\n\n4. **For Kubernetes deployments**, update ConfigMap:\n   ```bash\n   kubectl create configmap acgs2-config \\\n     --from-literal=CONSTITUTIONAL_HASH=cdd01ef066bc6cf2 \\\n     --dry-run=client -o yaml | kubectl apply -f -\n   ```\n\n5. **Restart all services**:\n   ```bash\n   docker-compose restart\n   # or\n   kubectl rollout restart deployment\n   ```\n\n**\u26a0\ufe0f Important**: Never bypass this check. If you're getting this error, it means the code doesn't match the approved constitutional requirements.\n\n**Example**:\n```bash\n# Check current hash\necho $CONSTITUTIONAL_HASH\n\n# Fix wrong hash\nexport CONSTITUTIONAL_HASH=cdd01ef066bc6cf2\necho \"CONSTITUTIONAL_HASH=cdd01ef066bc6cf2\" >> .env\n\n# Verify\ndocker-compose logs enhanced-agent-bus | grep \"Constitutional hash validated\"\n```\n\n**Related Errors**: ACGS-1302, ACGS-6101\n\n---\n\n### ACGS-1302: ConstitutionalValidationError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: `ConstitutionalValidationError` (enhanced-agent-bus)\n**Location**: `acgs2-core/enhanced_agent_bus/exceptions.py`\n\n**Description**: Constitutional validation check failed during runtime. The system enforces constitutional compliance for all operations.\n\n**Common Causes**:\n- Operation violates constitutional rules\n- Alignment check failures\n- Policy compliance violations\n- Constitutional principles not satisfied\n- Safety constraints violated\n\n**Symptoms**:\n```\nConstitutionalValidationError: Operation violates constitutional requirements\nConstitutional alignment check failed\nSafety constraints not satisfied\nPolicy compliance validation failed\n```\n\n**Resolution**:\n1. **Review the validation error** - it will specify which constitutional requirement failed\n\n2. **Check constitutional logs**:\n   ```bash\n   docker-compose logs enhanced-agent-bus | grep \"Constitutional\"\n   ```\n\n3. **Verify operation meets constitutional requirements**:\n   - Check if operation aligns with defined policies\n   - Ensure all safety constraints are met\n   - Verify alignment principles are satisfied\n\n4. **Review constitutional documentation**:\n   ```bash\n   # Check constitutional hash and principles\n   grep CONSTITUTIONAL_HASH .env\n   cat acgs2-core/docs/operations/constitutional_principles.md\n   ```\n\n5. **If legitimate operation is being rejected**:\n   - Review constitutional policies in OPA\n   - Check for overly restrictive rules\n   - Consult with governance team\n\n**\u26a0\ufe0f Important**: Do not bypass constitutional validation. These checks are critical safety mechanisms.\n\n**Example**:\n```bash\n# Check constitutional validation logs\ndocker-compose logs enhanced-agent-bus | grep -A 5 \"ConstitutionalValidationError\"\n\n# Verify constitutional hash is correct\necho $CONSTITUTIONAL_HASH\n# Should be: cdd01ef066bc6cf2\n```\n\n**Related Errors**: ACGS-1301, ACGS-6101, ACGS-6201\n\n---\n\n### ACGS-1401: OPAConfigurationError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n\n**Description**: OPA service configuration invalid or incorrect.\n\n**Common Causes**:\n- Using `localhost` instead of Docker network service name\n- Incorrect port number (should be 8181)\n- Missing protocol in URL (`http://`)\n- Wrong OPA endpoint path\n- OPA_URL not set in environment\n\n**Symptoms**:\n```\nOPAConfigurationError: Invalid OPA_URL\nCannot parse OPA URL: localhost:8181\nOPA_URL must include protocol (http:// or https://)\n```\n\n**Resolution**:\n1. **Check OPA_URL format**:\n   ```bash\n   grep OPA_URL .env\n   # Should be: OPA_URL=http://opa:8181\n   ```\n\n2. **Common configuration mistakes**:\n   ```bash\n   # Wrong: OPA_URL=localhost:8181\n   # Right: OPA_URL=http://localhost:8181  (from host)\n\n   # Wrong: OPA_URL=opa:8181\n   # Right: OPA_URL=http://opa:8181  (from container)\n   ```\n\n3. **Fix OPA URL**:\n   ```bash\n   # For Docker Compose (from within containers):\n   sed -i 's|OPA_URL=.*|OPA_URL=http://opa:8181|' .env\n\n   # For host access:\n   sed -i 's|OPA_URL=.*|OPA_URL=http://localhost:8181|' .env\n   ```\n\n4. **Verify OPA is accessible**:\n   ```bash\n   # From host:\n   curl http://localhost:8181/health\n\n   # From container:\n   docker-compose exec enhanced-agent-bus curl http://opa:8181/health\n   ```\n\n**Example**:\n```bash\n# Wrong OPA URL\nOPA_URL=opa:8181\n\n# Fix:\necho \"OPA_URL=http://opa:8181\" >> .env\ndocker-compose restart enhanced-agent-bus\n\n# Verify:\ndocker-compose logs enhanced-agent-bus | grep \"Connected to OPA\"\n```\n\n**Related Errors**: ACGS-1102, ACGS-2403, ACGS-4401\n\n---\n\n### ACGS-1402: KafkaConfigurationError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n\n**Description**: Kafka configuration invalid or bootstrap servers incorrect.\n\n**Common Causes**:\n- Wrong Kafka port (9092 vs 19092 vs 29092 confusion)\n- Using localhost instead of Docker service name\n- Incorrect listener configuration\n- Multiple broker addresses with typos\n\n**Symptoms**:\n```\nKafkaConfigurationError: Invalid KAFKA_BOOTSTRAP_SERVERS\nCannot connect to broker: localhost:9092\nKafka listener confusion: Use kafka:19092 from containers\n```\n\n**Kafka Port Guide**:\n- **9092**: External access from host (localhost:9092)\n- **19092**: Internal Docker network (kafka:19092)\n- **29092**: Additional internal listener (if configured)\n\n**Resolution**:\n1. **Check Kafka bootstrap servers**:\n   ```bash\n   grep KAFKA_BOOTSTRAP_SERVERS .env\n   ```\n\n2. **Fix common port confusion**:\n   ```bash\n   # From within Docker containers (most services):\n   # Right: KAFKA_BOOTSTRAP_SERVERS=kafka:19092\n\n   # From host machine (development tools):\n   # Right: KAFKA_BOOTSTRAP_SERVERS=localhost:9092\n\n   # Wrong: KAFKA_BOOTSTRAP_SERVERS=localhost:19092\n   # Wrong: KAFKA_BOOTSTRAP_SERVERS=kafka:9092\n   ```\n\n3. **Update configuration**:\n   ```bash\n   # For container-based services:\n   sed -i 's|KAFKA_BOOTSTRAP_SERVERS=.*|KAFKA_BOOTSTRAP_SERVERS=kafka:19092|' .env\n   ```\n\n4. **Verify Kafka is accessible**:\n   ```bash\n   # From host:\n   docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092\n\n   # From container:\n   docker-compose exec enhanced-agent-bus nc -zv kafka 19092\n   ```\n\n**Example**:\n```bash\n# Wrong port from container\nKAFKA_BOOTSTRAP_SERVERS=kafka:9092\n\n# Fix:\nsed -i 's|KAFKA_BOOTSTRAP_SERVERS=kafka:9092|KAFKA_BOOTSTRAP_SERVERS=kafka:19092|' .env\ndocker-compose restart hitl-approvals\n\n# Verify:\ndocker-compose logs hitl-approvals | grep \"Connected to Kafka\"\n```\n\n**Related Errors**: ACGS-1102, ACGS-4201, ACGS-4202\n\n---\n\n### ACGS-1403: DatabaseConfigurationError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n\n**Description**: Database connection string invalid or credentials incorrect.\n\n**Common Causes**:\n- Invalid PostgreSQL URL format\n- Wrong credentials (username/password)\n- Incorrect database name\n- Using localhost instead of Docker service name\n- Missing database driver in URL (`postgresql://` vs `postgres://`)\n\n**Symptoms**:\n```\nDatabaseConfigurationError: Invalid DATABASE_URL\nInvalid connection string format\nCould not parse DATABASE_URL\nMissing password in connection string\n```\n\n**Connection String Format**:\n```\npostgresql://username:password@host:port/database_name\n```\n\n**Resolution**:\n1. **Check DATABASE_URL format**:\n   ```bash\n   grep DATABASE_URL .env\n   # Should look like: postgresql://user:pass@postgres:5432/dbname\n   ```\n\n2. **Common format issues**:\n   ```bash\n   # Missing protocol:\n   # Wrong: DATABASE_URL=postgres:5432/acgs2_db\n   # Right: DATABASE_URL=postgresql://acgs2:password@postgres:5432/acgs2_db\n\n   # Missing database name:\n   # Wrong: DATABASE_URL=postgresql://user:pass@postgres:5432\n   # Right: DATABASE_URL=postgresql://user:pass@postgres:5432/acgs2_db\n\n   # Wrong host (from container):\n   # Wrong: DATABASE_URL=postgresql://user:pass@localhost:5432/db\n   # Right: DATABASE_URL=postgresql://user:pass@postgres:5432/db\n   ```\n\n3. **Fix DATABASE_URL**:\n   ```bash\n   # Standard format for Docker Compose:\n   echo \"DATABASE_URL=postgresql://acgs2:acgs2password@postgres:5432/acgs2_db\" >> .env\n   ```\n\n4. **Verify database is accessible**:\n   ```bash\n   # From host (if port 5432 is exposed):\n   psql postgresql://acgs2:acgs2password@localhost:5432/acgs2_db -c \"SELECT 1\"\n\n   # From container:\n   docker-compose exec hitl-approvals pg_isready -h postgres -p 5432\n   ```\n\n5. **Check credentials match docker-compose.yml**:\n   ```bash\n   # Verify username, password, database name match\n   grep -A 5 \"POSTGRES_\" docker-compose.yml\n   ```\n\n**Example**:\n```bash\n# Wrong: missing protocol\nDATABASE_URL=postgres:5432/acgs2_db\n\n# Fix:\nsed -i 's|DATABASE_URL=.*|DATABASE_URL=postgresql://acgs2:acgs2password@postgres:5432/acgs2_db|' .env\ndocker-compose restart hitl-approvals\n\n# Verify connection:\ndocker-compose logs hitl-approvals | grep \"Database connected\"\n```\n\n**Related Errors**: ACGS-1102, ACGS-4301, ACGS-4302\n\n---\n\n### ACGS-1501: TLSConfigurationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n\n**Description**: TLS/SSL configuration invalid or certificates missing.\n\n**Common Causes**:\n- Invalid or expired certificate\n- Certificate chain incomplete\n- Private key doesn't match certificate\n- Certificate file permissions incorrect\n- Self-signed certificate not trusted\n\n**Symptoms**:\n```\nTLSConfigurationError: Invalid certificate\nSSL handshake failed\nCertificate verification failed\nPrivate key does not match certificate\n```\n\n**Resolution**:\n1. **Check certificate validity**:\n   ```bash\n   # Check certificate expiration\n   openssl x509 -in cert.pem -noout -dates\n\n   # Verify certificate chain\n   openssl verify -CAfile ca.pem cert.pem\n   ```\n\n2. **Verify private key matches certificate**:\n   ```bash\n   # Get certificate modulus\n   openssl x509 -in cert.pem -noout -modulus | md5sum\n\n   # Get private key modulus (should match)\n   openssl rsa -in key.pem -noout -modulus | md5sum\n   ```\n\n3. **Check file permissions**:\n   ```bash\n   # Certificates should be readable\n   chmod 644 cert.pem ca.pem\n\n   # Private keys should be restricted\n   chmod 600 key.pem\n   ```\n\n4. **For self-signed certificates in development**:\n   ```bash\n   # Disable SSL verification (DEVELOPMENT ONLY)\n   export SSL_VERIFY=false\n\n   # Or add to .env:\n   echo \"SSL_VERIFY=false\" >> .env\n   ```\n\n**Example**:\n```bash\n# Certificate doesn't match key\n# Fix: regenerate certificate or use correct key\n\n# For development, generate self-signed cert:\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes\n\n# Update configuration:\necho \"TLS_CERT_PATH=/path/to/cert.pem\" >> .env\necho \"TLS_KEY_PATH=/path/to/key.pem\" >> .env\n```\n\n**Related Errors**: ACGS-1502, ACGS-1503\n\n---\n\n### ACGS-1502: CORSConfigurationError\n\n**Severity**: CRITICAL (Security)\n**Impact**: Security-Vulnerability\n**Location**: `acgs2-core/services/compliance_docs/src/main.py:25`\n\n**Description**: CORS policy misconfigured allowing all origins. This is a known TODO that creates a security vulnerability.\n\n**Current Behavior**: CORS is configured to allow all origins (`*`), which allows any website to access the API.\n\n**Security Impact**:\n- Cross-site request forgery (CSRF) attacks possible\n- Unauthorized API access from malicious sites\n- Data exposure to untrusted origins\n\n**Resolution**:\n1. **Review TODO comment** in `compliance_docs/src/main.py:25`\n\n2. **Configure proper CORS policy**:\n   ```python\n   # Replace wildcard with specific origins\n   app.add_middleware(\n       CORSMiddleware,\n       allow_origins=[\n           \"https://app.example.com\",\n           \"https://dashboard.example.com\"\n       ],\n       allow_credentials=True,\n       allow_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n       allow_headers=[\"*\"],\n   )\n   ```\n\n3. **Use environment variable for origins**:\n   ```bash\n   # In .env:\n   ALLOWED_ORIGINS=https://app.example.com,https://dashboard.example.com\n   ```\n\n4. **Implement origin validation**:\n   ```python\n   import os\n   allowed_origins = os.getenv(\"ALLOWED_ORIGINS\", \"\").split(\",\")\n   ```\n\n**TODO Reference**: See `TODO_CATALOG.md` - CRITICAL priority\n\n**Related Errors**: ACGS-1501, ACGS-1503\n\n---\n\n### ACGS-1503: SecretNotFoundError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n\n**Description**: Required secret not found in secret store or environment.\n\n**Common Causes**:\n- Kubernetes secret not created\n- Secret key name mismatch\n- Vault/AWS Secrets Manager misconfiguration\n- Environment variable for secret not set\n- Secret not mounted in container\n\n**Symptoms**:\n```\nSecretNotFoundError: Required secret not found: WEBHOOK_SECRET\nKubernetes secret 'acgs2-secrets' does not exist\nFailed to fetch secret from Vault: path not found\n```\n\n**Resolution**:\n1. **For Kubernetes deployments, create secret**:\n   ```bash\n   # Create secret from literals\n   kubectl create secret generic acgs2-secrets \\\n     --from-literal=WEBHOOK_SECRET=your-secret-here \\\n     --from-literal=DATABASE_PASSWORD=db-password\n\n   # Or from file\n   kubectl create secret generic acgs2-secrets \\\n     --from-file=.env\n   ```\n\n2. **Verify secret exists**:\n   ```bash\n   kubectl get secret acgs2-secrets\n   kubectl describe secret acgs2-secrets\n   ```\n\n3. **Check secret is mounted in pod**:\n   ```yaml\n   # In deployment.yaml:\n   env:\n     - name: WEBHOOK_SECRET\n       valueFrom:\n         secretKeyRef:\n           name: acgs2-secrets\n           key: WEBHOOK_SECRET\n   ```\n\n4. **For Vault, check path and policies**:\n   ```bash\n   # Test Vault access\n   vault kv get secret/acgs2/webhooks\n\n   # Verify policy allows read\n   vault policy read acgs2-read-policy\n   ```\n\n5. **For AWS Secrets Manager**:\n   ```bash\n   # Verify secret exists\n   aws secretsmanager describe-secret --secret-id acgs2/production\n\n   # Test retrieval\n   aws secretsmanager get-secret-value --secret-id acgs2/production\n   ```\n\n**Example**:\n```bash\n# Kubernetes: create missing secret\nkubectl create secret generic acgs2-secrets \\\n  --from-literal=WEBHOOK_SECRET=$(openssl rand -hex 32) \\\n  --from-literal=JWT_SECRET=$(openssl rand -hex 32)\n\n# Restart pods to pick up secret\nkubectl rollout restart deployment/integration-service\n```\n\n**Related Errors**: ACGS-1101, ACGS-1501, ACGS-1502\n\n---\n\n### ACGS-1504: OIDCConfigurationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n**Exception**: `OIDCConfigurationError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/oidc_handler.py`\n\n**Description**: OIDC (OpenID Connect) provider configuration error.\n\n**Common Causes**:\n- Missing client ID or client secret\n- Invalid OIDC discovery URL\n- Provider not properly registered\n- Redirect URI mismatch\n- Incorrect scopes configuration\n\n**Symptoms**:\n```\nOIDCConfigurationError: Missing OIDC_CLIENT_ID\nFailed to fetch OIDC discovery document from https://auth.example.com/.well-known/openid-configuration\nInvalid redirect URI: must match configured value\n```\n\n**Resolution**:\n1. **Check required OIDC environment variables**:\n   ```bash\n   grep -E \"OIDC_\" .env\n   # Required variables:\n   # OIDC_PROVIDER_URL=https://auth.example.com\n   # OIDC_CLIENT_ID=your-client-id\n   # OIDC_CLIENT_SECRET=your-client-secret\n   # OIDC_REDIRECT_URI=https://yourapp.example.com/auth/callback\n   ```\n\n2. **Test OIDC discovery endpoint**:\n   ```bash\n   curl https://auth.example.com/.well-known/openid-configuration\n   # Should return JSON with issuer, authorization_endpoint, token_endpoint, etc.\n   ```\n\n3. **Verify client ID and secret with provider**:\n   - Log into your OIDC provider (Auth0, Okta, Keycloak, etc.)\n   - Check client application settings\n   - Regenerate secret if needed\n\n4. **Fix redirect URI mismatch**:\n   ```bash\n   # Redirect URI must exactly match provider configuration\n   # Including protocol (https://) and path\n   OIDC_REDIRECT_URI=https://app.example.com/auth/callback\n   ```\n\n5. **Verify scopes**:\n   ```bash\n   # Common OIDC scopes\n   OIDC_SCOPES=openid,profile,email\n   ```\n\n**Example**:\n```bash\n# Missing OIDC client ID\n# Fix by adding to .env:\ncat >> .env << EOF\nOIDC_PROVIDER_URL=https://auth.example.com\nOIDC_CLIENT_ID=acgs2-client-id\nOIDC_CLIENT_SECRET=your-secret-here\nOIDC_REDIRECT_URI=https://app.example.com/auth/callback\nOIDC_SCOPES=openid,profile,email\nEOF\n\n# Restart service\ndocker-compose restart hitl-approvals\n```\n\n**Related Errors**: ACGS-1102, ACGS-1505, ACGS-2201, ACGS-2202\n\n---\n\n### ACGS-1505: SAMLConfigurationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n**Exception**: `SAMLConfigurationError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/saml_config.py`\n\n**Description**: SAML (Security Assertion Markup Language) configuration error.\n\n**Common Causes**:\n- Invalid IdP (Identity Provider) metadata\n- Certificate issues (expired, missing, wrong format)\n- SP (Service Provider) entity ID mismatch\n- Assertion Consumer Service (ACS) URL incorrect\n- Missing or invalid signing certificate\n\n**Symptoms**:\n```\nSAMLConfigurationError: Invalid IdP metadata\nFailed to parse SAML certificate\nSP entity ID mismatch: expected 'https://app.example.com', got 'http://localhost'\nACS URL not configured in IdP\n```\n\n**Resolution**:\n1. **Check required SAML environment variables**:\n   ```bash\n   grep -E \"SAML_\" .env\n   # Required variables:\n   # SAML_IDP_METADATA_URL=https://idp.example.com/metadata\n   # SAML_SP_ENTITY_ID=https://app.example.com\n   # SAML_ACS_URL=https://app.example.com/saml/acs\n   # SAML_CERT_PATH=/path/to/cert.pem\n   # SAML_KEY_PATH=/path/to/key.pem\n   ```\n\n2. **Verify IdP metadata is accessible**:\n   ```bash\n   curl https://idp.example.com/metadata\n   # Should return XML with IdP configuration\n   ```\n\n3. **Check certificate validity**:\n   ```bash\n   # Verify certificate hasn't expired\n   openssl x509 -in saml-cert.pem -noout -dates\n\n   # Check certificate matches key\n   openssl x509 -in saml-cert.pem -noout -modulus | md5sum\n   openssl rsa -in saml-key.pem -noout -modulus | md5sum\n   # The two hashes should match\n   ```\n\n4. **Verify SP entity ID matches IdP configuration**:\n   ```bash\n   # Entity ID must exactly match what's configured in IdP\n   # Usually the base URL of your application\n   SAML_SP_ENTITY_ID=https://app.example.com\n   ```\n\n5. **Check ACS URL is registered with IdP**:\n   - Log into IdP admin console\n   - Verify ACS URL is in allowed callback URLs\n   - Ensure it matches exactly (including https://)\n\n6. **Validate SAML response**:\n   ```bash\n   # Use SAML tracer browser extension to debug\n   # Check for common issues:\n   # - Clock skew (NotBefore/NotOnOrAfter)\n   # - Signature validation failures\n   # - Attribute mapping issues\n   ```\n\n**Example**:\n```bash\n# Configure SAML authentication\ncat >> .env << EOF\nSAML_IDP_METADATA_URL=https://idp.example.com/metadata\nSAML_SP_ENTITY_ID=https://app.example.com\nSAML_ACS_URL=https://app.example.com/saml/acs\nSAML_CERT_PATH=/app/certs/saml-cert.pem\nSAML_KEY_PATH=/app/certs/saml-key.pem\nEOF\n\n# Mount certificates in docker-compose.yml:\n# volumes:\n#   - ./certs/saml-cert.pem:/app/certs/saml-cert.pem:ro\n#   - ./certs/saml-key.pem:/app/certs/saml-key.pem:ro\n\n# Restart service\ndocker-compose restart hitl-approvals\n```\n\n**Related Errors**: ACGS-1102, ACGS-1501, ACGS-1504, ACGS-2211, ACGS-2214\n\n---\n\n## ACGS-2xxx: Authentication/Authorization Errors\n\n**Category Description**: Errors related to authentication, authorization, policy evaluation, and access control.\n\n**Common Severity**: HIGH to CRITICAL (security-critical)\n\n**Related Services**: OPA, webhooks, OIDC, SAML, RBAC\n\n---\n\n### ACGS-2101: InvalidSignatureError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `InvalidSignatureError` (integration-service)\n\n**Description**: HMAC signature verification failed for incoming webhook.\n\n**Common Causes**:\n- Incorrect shared secret key\n- Signature algorithm mismatch (SHA256 vs SHA512)\n- Payload tampering or modification\n- Timestamp mismatch in signed payload\n- Header name mismatch (X-Signature vs X-Hub-Signature)\n\n**Symptoms**:\n```\n401 Unauthorized: Invalid signature\nWebhook rejected: signature verification failed\nExpected signature: sha256=abc...\nReceived signature: sha256=xyz...\n```\n\n**Resolution**:\n1. **Verify shared secret matches sender**:\n   ```bash\n   # Check webhook configuration\n   grep WEBHOOK_SECRET .env\n   ```\n\n2. **Check signature header name**:\n   ```python\n   # Common headers:\n   # GitHub: X-Hub-Signature-256\n   # Generic: X-Signature\n   # Custom: check integration docs\n   ```\n\n3. **Verify signature algorithm**:\n   ```bash\n   # Ensure both sides use same algorithm\n   # Common: HMAC-SHA256, HMAC-SHA512\n   ```\n\n4. **Test signature generation**:\n   ```python\n   import hmac\n   import hashlib\n\n   secret = b\"your-secret-key\"\n   payload = b'{\"event\": \"test\"}'\n   signature = hmac.new(secret, payload, hashlib.sha256).hexdigest()\n   print(f\"Expected: sha256={signature}\")\n   ```\n\n5. **Check timestamp tolerance**:\n   ```bash\n   # Default: 300 seconds (5 minutes)\n   # Increase if clock skew is issue\n   SIGNATURE_TIMESTAMP_TOLERANCE=600\n   ```\n\n**Example**:\n```bash\n# Debug webhook signature\ncurl -X POST http://localhost:8080/webhooks/github \\\n  -H \"X-Hub-Signature-256: sha256=<calculated-signature>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"event\":\"test\"}'\n\n# If fails, check secret:\necho -n '{\"event\":\"test\"}' | openssl dgst -sha256 -hmac \"your-secret\"\n```\n\n**Related Errors**: ACGS-2102, ACGS-2105, ACGS-2106, ACGS-2107\n\n---\n\n### ACGS-2001: AuthenticationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n**Exception**: `AuthenticationError` (integration-service, sdk)\n**Location**: `integration-service/src/integrations/base.py`, `sdk/python/acgs2_sdk/exceptions.py`\n\n**Description**: Generic authentication failure when authenticating with external services or SDK.\n\n**Common Causes**:\n- Invalid credentials provided\n- Authentication token expired\n- Authentication service unavailable\n- Incorrect authentication method\n\n**Symptoms**:\n```\nAuthenticationError: Authentication failed\n401 Unauthorized\nInvalid credentials\nAuthentication service returned error\n```\n\n**Resolution**:\n1. **Verify credentials are correct**:\n   ```bash\n   # Check authentication configuration\n   grep -E \"API_KEY|AUTH_TOKEN|CLIENT_ID|CLIENT_SECRET\" .env\n   ```\n\n2. **Check credential expiration**:\n   - Verify tokens haven't expired\n   - Refresh OAuth tokens if needed\n   - Check API key validity\n\n3. **Test authentication endpoint**:\n   ```bash\n   # Test service is accepting auth\n   curl -H \"Authorization: Bearer <token>\" https://api.example.com/test\n   ```\n\n4. **Check service logs** for specific error details:\n   ```bash\n   docker-compose logs integration-service | grep -i auth\n   ```\n\n**Example**:\n```bash\n# API key invalid\nERROR: AuthenticationError: Invalid API key\n\n# Fix: Update API key in .env\necho \"API_KEY=your-valid-api-key-here\" >> .env\ndocker-compose restart integration-service\n```\n\n**Related Errors**: ACGS-2002, ACGS-2003, ACGS-2101\n\n---\n\n### ACGS-2002: AuthorizationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n**Exception**: `AuthorizationError` (sdk)\n**Location**: `sdk/python/acgs2_sdk/exceptions.py`\n\n**Description**: User is authenticated but not authorized to perform the requested operation.\n\n**Common Causes**:\n- Insufficient permissions for operation\n- Role not assigned to user\n- Resource access control list (ACL) denies access\n- Organization/tenant membership required\n\n**Symptoms**:\n```\n403 Forbidden: You do not have permission to access this resource\nAuthorizationError: Insufficient permissions\nUser lacks required role: admin\n```\n\n**Resolution**:\n1. **Verify user roles and permissions**:\n   ```bash\n   # Check user roles in database\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT user_id, roles FROM users WHERE email='user@example.com';\"\n   ```\n\n2. **Review required permissions** for the operation:\n   - Check OPA policy requirements\n   - Verify RBAC role assignments\n   - Confirm tenant/organization membership\n\n3. **Assign required roles**:\n   ```bash\n   # Example: Assign admin role\n   docker-compose exec hitl-approvals python -c \"\n   from app.models import User\n   user = User.query.filter_by(email='user@example.com').first()\n   user.roles.append('admin')\n   db.session.commit()\n   \"\n   ```\n\n4. **Check OPA policies** allow the operation:\n   ```bash\n   # Query OPA directly\n   curl -X POST http://localhost:8181/v1/data/acgs2/rbac/allow \\\n     -d '{\"input\": {\"user\": \"user@example.com\", \"action\": \"read\", \"resource\": \"/api/approvals\"}}'\n   ```\n\n**Example**:\n```bash\n# User lacks approval role\nERROR: AuthorizationError: User lacks required role: approver\n\n# Fix: Add approver role\n# Via admin UI or database update\n```\n\n**Related Errors**: ACGS-2001, ACGS-2003, ACGS-2302, ACGS-2401\n\n---\n\n### ACGS-2003: AccessDeniedError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `AccessDeniedError` (tenant-management, tenant-integration)\n\n**Description**: Access denied to tenant resource or cross-tenant operation.\n\n**Common Causes**:\n- User not member of target tenant\n- Tenant disabled or suspended\n- Cross-tenant access not permitted\n- Tenant isolation policy violation\n\n**Symptoms**:\n```\nAccessDeniedError: User not authorized for tenant 'org-123'\n403 Forbidden: Tenant access denied\nUser does not belong to this tenant\n```\n\n**Resolution**:\n1. **Verify tenant membership**:\n   ```bash\n   # Check user's tenants\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT user_id, tenant_id, role FROM tenant_memberships \\\n      WHERE user_id='<user-id>';\"\n   ```\n\n2. **Check tenant status**:\n   ```bash\n   # Verify tenant is active\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT id, name, status FROM tenants WHERE id='<tenant-id>';\"\n   ```\n\n3. **Add user to tenant** if appropriate:\n   ```bash\n   # Via tenant admin API or database\n   curl -X POST http://localhost:8080/api/tenants/<tenant-id>/members \\\n     -H \"Authorization: Bearer <admin-token>\" \\\n     -d '{\"user_id\": \"<user-id>\", \"role\": \"member\"}'\n   ```\n\n**Example**:\n```bash\n# User tries to access tenant they don't belong to\nERROR: AccessDeniedError: User not authorized for tenant 'acme-corp'\n\n# Fix: Add user to tenant (requires tenant admin permissions)\n```\n\n**Related Errors**: ACGS-2001, ACGS-2002, ACGS-2302\n\n---\n\n### ACGS-2102: InvalidApiKeyError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `InvalidApiKeyError` (integration-service)\n**Location**: `integration-service/src/webhooks/auth.py`\n\n**Description**: API key validation failed for webhook authentication.\n\n**Common Causes**:\n- Missing API key header\n- Invalid or revoked API key\n- API key not registered in handler\n- Wrong API key format\n\n**Symptoms**:\n```\n401 Unauthorized: Invalid API key\nMissing required header: X-API-Key\nAPI key not found or revoked\n```\n\n**Resolution**:\n1. **Verify API key header is sent**:\n   ```bash\n   # Test webhook with API key\n   curl -X POST http://localhost:8080/webhooks/integration \\\n     -H \"X-API-Key: your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"event\":\"test\"}'\n   ```\n\n2. **Check API key is registered**:\n   ```bash\n   # Verify API key in configuration\n   docker-compose exec integration-service python -c \"\n   from src.webhooks.auth import validate_api_key\n   print(validate_api_key('your-api-key'))\n   \"\n   ```\n\n3. **Generate new API key** if needed:\n   ```bash\n   # Generate secure API key\n   openssl rand -hex 32\n   ```\n\n4. **Update webhook configuration** with correct API key:\n   ```bash\n   # Update in external service sending webhooks\n   # and in ACGS webhook handler configuration\n   ```\n\n**Example**:\n```bash\n# API key mismatch\nERROR: InvalidApiKeyError: API key validation failed\n\n# Debug: Check what key is being sent\ncurl -v http://localhost:8080/webhooks/test \\\n  -H \"X-API-Key: test-key\" 2>&1 | grep X-API-Key\n\n# Fix: Use correct API key from .env or generate new one\n```\n\n**Related Errors**: ACGS-2101, ACGS-2103, ACGS-2106\n\n---\n\n### ACGS-2103: InvalidBearerTokenError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `InvalidBearerTokenError` (integration-service)\n**Location**: `integration-service/src/webhooks/auth.py`\n\n**Description**: Bearer token authentication failed for webhook.\n\n**Common Causes**:\n- Expired OAuth token\n- Invalid token format\n- Token not in token store/database\n- Token signature validation failed\n\n**Symptoms**:\n```\n401 Unauthorized: Invalid bearer token\nToken expired or not found\nInvalid Authorization header format\nExpected: Bearer <token>\n```\n\n**Resolution**:\n1. **Verify token format**:\n   ```bash\n   # Correct format: Authorization: Bearer <token>\n   curl -X POST http://localhost:8080/webhooks/integration \\\n     -H \"Authorization: Bearer eyJhbGc...\" \\\n     -d '{\"event\":\"test\"}'\n   ```\n\n2. **Check token hasn't expired**:\n   ```bash\n   # Decode JWT to check expiration (if JWT)\n   echo \"eyJhbGc...\" | cut -d. -f2 | base64 -d | jq .exp\n   # Compare with current time: date +%s\n   ```\n\n3. **Refresh token** if expired:\n   ```bash\n   # Use OAuth refresh token flow\n   curl -X POST https://auth.example.com/oauth/token \\\n     -d \"grant_type=refresh_token\" \\\n     -d \"refresh_token=<refresh-token>\" \\\n     -d \"client_id=<client-id>\"\n   ```\n\n4. **Verify token is registered** in system:\n   ```bash\n   # Check token store\n   docker-compose exec redis redis-cli GET \"webhook_token:<token-prefix>\"\n   ```\n\n**Example**:\n```bash\n# Expired token\nERROR: InvalidBearerTokenError: Token expired\n\n# Fix: Get new token via OAuth flow or refresh existing token\n```\n\n**Related Errors**: ACGS-2104, ACGS-2501, ACGS-2502\n\n---\n\n### ACGS-2104: TokenExpiredError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `TokenExpiredError` (integration-service)\n**Location**: `integration-service/src/webhooks/auth.py`\n\n**Description**: OAuth or authentication token has expired.\n\n**Common Causes**:\n- Token TTL exceeded\n- System clock drift between systems\n- Token not refreshed before expiration\n- Refresh token also expired\n\n**Symptoms**:\n```\nTokenExpiredError: Token expired at 2026-01-03T10:00:00Z\n401 Unauthorized: Token no longer valid\nToken expired 2 hours ago\n```\n\n**Resolution**:\n1. **Use refresh token** to get new access token:\n   ```bash\n   curl -X POST https://auth.example.com/oauth/token \\\n     -d \"grant_type=refresh_token\" \\\n     -d \"refresh_token=<refresh-token>\" \\\n     -d \"client_id=<client-id>\" \\\n     -d \"client_secret=<client-secret>\"\n   ```\n\n2. **Check system clock synchronization**:\n   ```bash\n   # Verify time is synchronized\n   timedatectl status\n   # Or on macOS:\n   sudo sntp -sS time.apple.com\n   ```\n\n3. **Configure token refresh** before expiration:\n   ```bash\n   # Set token refresh margin (e.g., refresh 5 min before expiry)\n   TOKEN_REFRESH_MARGIN_SECONDS=300\n   ```\n\n4. **Re-authenticate** if refresh token also expired:\n   ```bash\n   # Initiate new OAuth flow\n   # Or re-authenticate via SSO\n   ```\n\n**Example**:\n```bash\n# Token expired\nERROR: TokenExpiredError: Token expired\n\n# Automatic fix: System should auto-refresh\n# Manual fix: Re-authenticate or use refresh token\n```\n\n**Related Errors**: ACGS-2103, ACGS-2501\n\n---\n\n### ACGS-2105: SignatureTimestampError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `SignatureTimestampError` (integration-service)\n**Location**: `integration-service/src/webhooks/auth.py`\n\n**Description**: Webhook signature timestamp outside acceptable window (default 300 seconds). This protects against replay attacks.\n\n**Common Causes**:\n- Request replay attack attempt\n- Significant clock skew between systems (>5 minutes)\n- Network delay exceeding tolerance\n- Timestamp not included in signature\n- Incorrect timestamp format\n\n**Symptoms**:\n```\nSignatureTimestampError: Timestamp outside acceptable range\nRequest timestamp too old: 2026-01-03T09:00:00Z\nCurrent time: 2026-01-03T09:10:00Z\nTolerance: 300 seconds\n```\n\n**Resolution**:\n1. **Check system time synchronization**:\n   ```bash\n   # On Linux:\n   timedatectl status\n   sudo systemctl status systemd-timesyncd\n\n   # On macOS:\n   sudo sntp -sS time.apple.com\n\n   # Verify time matches:\n   date -u\n   ```\n\n2. **Increase timestamp tolerance** if legitimate delay:\n   ```bash\n   # Default: 300 seconds (5 minutes)\n   # Increase for high-latency networks\n   echo \"SIGNATURE_TIMESTAMP_TOLERANCE=600\" >> .env\n   docker-compose restart integration-service\n   ```\n\n3. **Check webhook sender configuration**:\n   - Ensure sender includes timestamp in signature\n   - Verify timestamp format (Unix epoch vs ISO 8601)\n   - Confirm sender's clock is synchronized\n\n4. **Investigate replay attack** if frequent:\n   ```bash\n   # Check for duplicate requests\n   docker-compose logs integration-service | \\\n     grep SignatureTimestampError | \\\n     grep -o \"Request ID: [^\\\"]*\" | \\\n     sort | uniq -d\n   ```\n\n**Example**:\n```bash\n# Clock skew detected\nERROR: SignatureTimestampError: Timestamp too old\n\n# Fix 1: Synchronize clocks\nsudo timedatectl set-ntp true\n\n# Fix 2: Increase tolerance temporarily\necho \"SIGNATURE_TIMESTAMP_TOLERANCE=600\" >> .env\ndocker-compose restart integration-service\n\n# Verify:\ncurl -X POST http://localhost:8080/webhooks/github \\\n  -H \"X-Hub-Signature-256: sha256=<sig>\" \\\n  -H \"X-Hub-Timestamp: $(date +%s)\" \\\n  -d '{\"event\":\"test\"}'\n```\n\n**Security Note**: Do not set tolerance too high (>900s/15min) as it weakens replay attack protection.\n\n**Related Errors**: ACGS-2101, ACGS-2107\n\n---\n\n### ACGS-2106: MissingAuthHeaderError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `MissingAuthHeaderError` (integration-service)\n**Location**: `integration-service/src/webhooks/auth.py`\n\n**Description**: Required authentication header is missing from webhook request.\n\n**Common Causes**:\n- Webhook sender not configured to send auth header\n- Header name mismatch (e.g., X-Signature vs X-Hub-Signature)\n- Proxy stripping authentication headers\n- Incorrect webhook endpoint configuration\n\n**Symptoms**:\n```\n401 Unauthorized: Missing required header: X-Signature\nMissingAuthHeaderError: No authentication header found\nExpected header: X-API-Key or Authorization\n```\n\n**Resolution**:\n1. **Verify required header name**:\n   ```bash\n   # Check webhook handler configuration\n   docker-compose logs integration-service | grep \"Expected header\"\n   ```\n\n2. **Common authentication headers**:\n   - `X-Signature`: HMAC signature\n   - `X-Hub-Signature-256`: GitHub webhooks (SHA256)\n   - `X-API-Key`: API key authentication\n   - `Authorization`: Bearer token or Basic auth\n\n3. **Test with correct header**:\n   ```bash\n   # Test different auth header types:\n\n   # HMAC signature:\n   curl -X POST http://localhost:8080/webhooks/integration \\\n     -H \"X-Signature: sha256=abc...\" \\\n     -d '{\"event\":\"test\"}'\n\n   # API key:\n   curl -X POST http://localhost:8080/webhooks/integration \\\n     -H \"X-API-Key: your-api-key\" \\\n     -d '{\"event\":\"test\"}'\n\n   # Bearer token:\n   curl -X POST http://localhost:8080/webhooks/integration \\\n     -H \"Authorization: Bearer <token>\" \\\n     -d '{\"event\":\"test\"}'\n   ```\n\n4. **Configure webhook sender** to include header:\n   - Check webhook configuration in external service\n   - Verify header name matches what ACGS expects\n   - Ensure authentication credentials are set\n\n5. **Check for proxy interference**:\n   ```bash\n   # If behind nginx/proxy, ensure headers are forwarded\n   # nginx.conf should include:\n   # proxy_set_header X-Signature $http_x_signature;\n   ```\n\n**Example**:\n```bash\n# Missing GitHub signature header\nERROR: MissingAuthHeaderError: No X-Hub-Signature-256 header\n\n# Fix: Configure GitHub webhook to include secret\n# GitHub Repo \u2192 Settings \u2192 Webhooks \u2192 Edit webhook \u2192 Secret\n```\n\n**Related Errors**: ACGS-2101, ACGS-2102, ACGS-2107\n\n---\n\n### ACGS-2107: WebhookAuthError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `WebhookAuthError` (integration-service)\n**Location**: `integration-service/src/webhooks/auth.py`\n\n**Description**: Base exception for webhook authentication errors. Generic failure when specific error type not determined.\n\n**Common Causes**:\n- Authentication mechanism misconfigured\n- Unknown authentication method requested\n- Multiple authentication failures\n- Invalid authentication configuration\n\n**Symptoms**:\n```\nWebhookAuthError: Authentication failed\nUnable to authenticate webhook request\nAuthentication configuration error\n```\n\n**Resolution**:\n1. **Check webhook authentication configuration**:\n   ```bash\n   # Review webhook auth settings\n   grep -E \"WEBHOOK_AUTH|AUTH_METHOD\" .env\n   ```\n\n2. **Verify authentication method** is supported:\n   - HMAC signature (recommended)\n   - API key\n   - Bearer token\n   - Basic auth\n\n3. **Review webhook handler logs** for specific error:\n   ```bash\n   docker-compose logs integration-service | grep -A 10 WebhookAuthError\n   ```\n\n4. **Test with curl** to isolate issue:\n   ```bash\n   # Minimal test request\n   curl -v -X POST http://localhost:8080/webhooks/test \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"test\":\"data\"}'\n   # Check what authentication is expected in response\n   ```\n\n**Example**:\n```bash\n# Generic auth failure\nERROR: WebhookAuthError: Authentication failed\n\n# Diagnosis:\ndocker-compose logs integration-service | tail -50\n\n# Common fixes:\n# 1. Check auth method configured\n# 2. Verify credentials match\n# 3. Ensure correct headers sent\n```\n\n**Related Errors**: ACGS-2101, ACGS-2102, ACGS-2103, ACGS-2105, ACGS-2106, ACGS-2108\n\n---\n\n### ACGS-2108: WebhookAuthenticationError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `WebhookAuthenticationError` (integration-service)\n**Location**: `integration-service/src/webhooks/delivery.py`\n\n**Description**: Webhook delivery authentication failed when ACGS sends webhook to external endpoint.\n\n**Common Causes**:\n- Invalid credentials for outbound webhook\n- External endpoint requires different auth method\n- Certificate validation failed (HTTPS)\n- Authentication endpoint unreachable\n\n**Symptoms**:\n```\nWebhookAuthenticationError: Failed to authenticate with external endpoint\n401 Unauthorized from https://example.com/webhook\nCertificate verification failed\n```\n\n**Resolution**:\n1. **Verify external endpoint credentials**:\n   ```bash\n   # Check webhook delivery configuration\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT id, url, auth_type FROM webhook_handlers;\"\n   ```\n\n2. **Test endpoint authentication manually**:\n   ```bash\n   # Test with same credentials ACGS uses\n   curl -X POST https://example.com/webhook \\\n     -H \"Authorization: Bearer <token>\" \\\n     -d '{\"test\":\"data\"}'\n   ```\n\n3. **Check certificate if HTTPS**:\n   ```bash\n   # Verify SSL certificate\n   openssl s_client -connect example.com:443 -servername example.com\n\n   # If self-signed, may need to disable verification (not recommended for production)\n   WEBHOOK_VERIFY_SSL=false  # In .env\n   ```\n\n4. **Update webhook credentials**:\n   ```bash\n   # Via API or database\n   curl -X PATCH http://localhost:8080/api/webhooks/<webhook-id> \\\n     -H \"Authorization: Bearer <admin-token>\" \\\n     -d '{\"auth_token\": \"new-token\"}'\n   ```\n\n**Example**:\n```bash\n# External endpoint returns 401\nERROR: WebhookAuthenticationError: External endpoint rejected auth\n\n# Fix: Update webhook credentials\ncurl -X PATCH http://localhost:8080/api/webhooks/<id> \\\n  -d '{\"auth_token\": \"updated-token\"}'\n```\n\n**Related Errors**: ACGS-2107, ACGS-5201, ACGS-5202\n\n---\n\n### ACGS-2201: OIDCAuthenticationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n**Exception**: `OIDCAuthenticationError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/oidc_handler.py`\n\n**Description**: OpenID Connect (OIDC) authentication failed during SSO login.\n\n**Common Causes**:\n- Invalid authorization code\n- State parameter mismatch (CSRF protection)\n- User denied consent at identity provider\n- Redirect URI mismatch\n- Client credentials invalid\n\n**Symptoms**:\n```\nOIDCAuthenticationError: Authentication failed\nInvalid authorization code\nState mismatch: CSRF protection triggered\nUser denied authorization\nerror=access_denied&error_description=User cancelled login\n```\n\n**Resolution**:\n1. **Verify OIDC configuration**:\n   ```bash\n   # Check OIDC settings in .env\n   grep -E \"OIDC_|OPENID_\" .env\n\n   # Required variables:\n   # OIDC_CLIENT_ID=<client-id>\n   # OIDC_CLIENT_SECRET=<client-secret>\n   # OIDC_DISCOVERY_URL=https://idp.example.com/.well-known/openid-configuration\n   # OIDC_REDIRECT_URI=http://localhost:8080/auth/callback\n   ```\n\n2. **Verify redirect URI** matches identity provider configuration:\n   ```bash\n   # Redirect URI must exactly match what's registered\n   # Common mistakes:\n   # - http vs https\n   # - trailing slash\n   # - localhost vs 127.0.0.1\n   # - port number\n   ```\n\n3. **Test OIDC discovery endpoint**:\n   ```bash\n   curl https://idp.example.com/.well-known/openid-configuration\n   # Should return JSON with endpoints\n   ```\n\n4. **Check identity provider logs** for rejection reason\n\n5. **Clear browser state** and retry:\n   ```bash\n   # Clear cookies and retry login\n   # State mismatch often caused by stale state parameter\n   ```\n\n**Example**:\n```bash\n# State mismatch error\nERROR: OIDCAuthenticationError: State mismatch\n\n# Common cause: User navigated back/forward during OAuth flow\n# Fix: Clear browser cookies and retry login\n\n# Configuration error:\nERROR: OIDCAuthenticationError: Redirect URI mismatch\n\n# Fix: Update redirect URI in identity provider to match:\necho \"OIDC_REDIRECT_URI=http://localhost:8080/auth/callback\" >> .env\n# And register same URI in IdP configuration\n```\n\n**Related Errors**: ACGS-2202, ACGS-2203, ACGS-2204, ACGS-1505\n\n---\n\n### ACGS-2202: OIDCTokenError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `OIDCTokenError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/oidc_handler.py`\n\n**Description**: OIDC token exchange or validation failed.\n\n**Common Causes**:\n- Invalid ID token signature\n- Token issuer (iss) mismatch\n- Token audience (aud) doesn't match client ID\n- Token expired\n- Token nonce mismatch\n- JWK keys not accessible\n\n**Symptoms**:\n```\nOIDCTokenError: Token validation failed\nInvalid signature: Token signature verification failed\nIssuer mismatch: expected 'https://idp.example.com', got 'https://other.com'\nAudience mismatch: token aud claim doesn't match client ID\nToken expired at 2026-01-03T10:00:00Z\n```\n\n**Resolution**:\n1. **Verify token issuer** matches OIDC provider:\n   ```bash\n   # Decode ID token (JWT) to inspect claims\n   # token format: header.payload.signature\n   echo \"<token-payload-part>\" | base64 -d | jq .\n\n   # Check iss (issuer) matches OIDC_DISCOVERY_URL domain\n   # Check aud (audience) matches OIDC_CLIENT_ID\n   # Check exp (expiration) is in future\n   ```\n\n2. **Check JWK keys are accessible**:\n   ```bash\n   # OIDC provider must expose public keys\n   curl https://idp.example.com/.well-known/jwks.json\n   # Should return JSON with keys array\n   ```\n\n3. **Verify client ID configuration**:\n   ```bash\n   # Ensure client ID matches what's registered\n   grep OIDC_CLIENT_ID .env\n   ```\n\n4. **Check clock synchronization**:\n   ```bash\n   # Token validation includes timestamp checks\n   timedatectl status\n   sudo timedatectl set-ntp true\n   ```\n\n5. **Test token endpoint**:\n   ```bash\n   # Exchange authorization code for tokens\n   curl -X POST https://idp.example.com/token \\\n     -d \"grant_type=authorization_code\" \\\n     -d \"code=<auth-code>\" \\\n     -d \"client_id=<client-id>\" \\\n     -d \"client_secret=<client-secret>\" \\\n     -d \"redirect_uri=<redirect-uri>\"\n   ```\n\n**Example**:\n```bash\n# Issuer mismatch\nERROR: OIDCTokenError: Issuer mismatch\n\n# Fix: Verify OIDC discovery URL\ncurl https://idp.example.com/.well-known/openid-configuration | jq .issuer\n# Update .env to match:\necho \"OIDC_DISCOVERY_URL=https://correct-idp.example.com/.well-known/openid-configuration\" >> .env\n```\n\n**Related Errors**: ACGS-2201, ACGS-2104\n\n---\n\n### ACGS-2203: OIDCProviderError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `OIDCProviderError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/oidc_handler.py`\n\n**Description**: Error communicating with OIDC identity provider.\n\n**Common Causes**:\n- Network connectivity issues to identity provider\n- Identity provider is down or unavailable\n- DNS resolution failure for provider domain\n- Firewall blocking access to provider\n- Provider returned error response\n- TLS/SSL certificate issues\n\n**Symptoms**:\n```\nOIDCProviderError: Cannot connect to identity provider\nConnection refused: https://idp.example.com\nTimeout connecting to OIDC provider\nDNS resolution failed for idp.example.com\nSSL certificate verification failed\nProvider returned 500 Internal Server Error\n```\n\n**Resolution**:\n1. **Test connectivity to provider**:\n   ```bash\n   # Test DNS resolution\n   nslookup idp.example.com\n   dig idp.example.com\n\n   # Test HTTPS connectivity\n   curl -v https://idp.example.com/.well-known/openid-configuration\n\n   # Test from inside Docker network\n   docker-compose exec enhanced-agent-bus curl -v https://idp.example.com\n   ```\n\n2. **Check provider status**:\n   - Visit provider's status page\n   - Check for maintenance windows\n   - Verify provider is operational\n\n3. **Verify firewall rules** allow outbound HTTPS:\n   ```bash\n   # Check if port 443 accessible\n   telnet idp.example.com 443\n   nc -zv idp.example.com 443\n   ```\n\n4. **Check certificate validation**:\n   ```bash\n   # Test SSL certificate\n   openssl s_client -connect idp.example.com:443 -servername idp.example.com\n\n   # If corporate proxy with SSL inspection:\n   # May need to add CA certificate\n   ```\n\n5. **Configure HTTP proxy** if required:\n   ```bash\n   # If behind corporate proxy\n   echo \"HTTPS_PROXY=http://proxy.corporate.com:8080\" >> .env\n   echo \"NO_PROXY=localhost,127.0.0.1\" >> .env\n   docker-compose restart\n   ```\n\n**Example**:\n```bash\n# Provider unreachable\nERROR: OIDCProviderError: Cannot connect to https://idp.example.com\n\n# Diagnosis:\ncurl -v https://idp.example.com/.well-known/openid-configuration\n\n# Common fixes:\n# 1. Check VPN connection if required\n# 2. Verify DNS resolution\n# 3. Check firewall rules\n# 4. Wait for provider to recover if down\n```\n\n**Related Errors**: ACGS-2201, ACGS-2202, ACGS-4501\n\n---\n\n### ACGS-2204: OIDCError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `OIDCError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/oidc_handler.py`\n\n**Description**: Base exception for OIDC-related errors. Generic OIDC failure when specific type not determined.\n\n**Resolution**: See related errors for specific OIDC issues:\n- ACGS-2201: OIDCAuthenticationError\n- ACGS-2202: OIDCTokenError\n- ACGS-2203: OIDCProviderError\n\nCheck logs for more specific error details:\n```bash\ndocker-compose logs | grep -i oidc | tail -50\n```\n\n**Related Errors**: ACGS-2201, ACGS-2202, ACGS-2203\n\n---\n\n### ACGS-2211: SAMLAuthenticationError\n\n**Severity**: HIGH\n**Impact**: Service-Unavailable\n**Exception**: `SAMLAuthenticationError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/saml_handler.py`\n\n**Description**: SAML 2.0 authentication failed during SSO login.\n\n**Common Causes**:\n- Invalid SAML assertion\n- SAML response signature validation failed\n- Required attributes missing from assertion\n- Name ID format mismatch\n- Assertion conditions not met (time bounds, audience)\n\n**Symptoms**:\n```\nSAMLAuthenticationError: Authentication failed\nInvalid SAML response signature\nRequired attribute 'email' missing from assertion\nAssertion not yet valid\nAssertion expired\nAudience restriction failed\n```\n\n**Resolution**:\n1. **Verify SAML configuration**:\n   ```bash\n   # Check SAML settings\n   grep -E \"SAML_\" .env\n\n   # Required:\n   # SAML_IDP_METADATA_URL=https://idp.example.com/metadata\n   # SAML_SP_ENTITY_ID=https://acgs2.example.com/saml/metadata\n   # SAML_ACS_URL=https://acgs2.example.com/saml/acs\n   ```\n\n2. **Download and inspect IdP metadata**:\n   ```bash\n   curl https://idp.example.com/metadata > idp-metadata.xml\n   # Verify certificate, endpoints, entity ID\n   ```\n\n3. **Check SP metadata** is registered with IdP:\n   ```bash\n   # Generate SP metadata\n   curl http://localhost:8080/saml/metadata > sp-metadata.xml\n   # Upload to IdP or provide SP metadata URL\n   ```\n\n4. **Verify clock synchronization** (critical for SAML):\n   ```bash\n   # SAML is very sensitive to time skew\n   timedatectl status\n   sudo timedatectl set-ntp true\n   ```\n\n5. **Check required SAML attributes** are provided:\n   ```bash\n   # Common required attributes:\n   # - email (urn:oid:0.9.2342.19200300.100.1.3)\n   # - firstName/givenName\n   # - lastName/surname\n   # - groups (for role mapping)\n   ```\n\n**Example**:\n```bash\n# Missing email attribute\nERROR: SAMLAuthenticationError: Required attribute 'email' missing\n\n# Fix: Configure IdP to release email attribute\n# or map different attribute to email in ACGS:\necho \"SAML_ATTR_EMAIL=http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\" >> .env\n```\n\n**Related Errors**: ACGS-2212, ACGS-2213, ACGS-2214, ACGS-2215\n\n---\n\n### ACGS-2212: SAMLValidationError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `SAMLValidationError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/saml_handler.py`\n\n**Description**: SAML assertion validation failed.\n\n**Common Causes**:\n- SAML response signature invalid\n- Assertion signature invalid\n- Certificate mismatch or expired\n- Assertion expired or not yet valid\n- Audience restriction failed\n- Recipient URL mismatch\n\n**Symptoms**:\n```\nSAMLValidationError: Assertion validation failed\nSignature verification failed\nCertificate expired\nAssertion audience doesn't match SP entity ID\nAssertion condition: NotBefore/NotOnOrAfter failed\nRecipient mismatch: expected https://acgs2.example.com/saml/acs\n```\n\n**Resolution**:\n1. **Verify IdP certificate**:\n   ```bash\n   # Extract certificate from metadata\n   xmllint --xpath \"//X509Certificate/text()\" idp-metadata.xml | \\\n     base64 -d | \\\n     openssl x509 -text -noout\n\n   # Check expiration date\n   ```\n\n2. **Check Assertion Consumer Service (ACS) URL**:\n   ```bash\n   # Must exactly match what IdP has configured\n   grep SAML_ACS_URL .env\n   # Should be: https://your-domain.com/saml/acs\n   ```\n\n3. **Verify SP Entity ID** matches IdP configuration:\n   ```bash\n   grep SAML_SP_ENTITY_ID .env\n   ```\n\n4. **Check time synchronization** (CRITICAL for SAML):\n   ```bash\n   # SAML assertions have tight time bounds (typically 5 minutes)\n   date -u\n   timedatectl status\n\n   # If time is off, SAML will fail\n   sudo timedatectl set-ntp true\n   ```\n\n5. **Inspect SAML response** for details:\n   ```bash\n   # Enable SAML debug logging\n   echo \"SAML_DEBUG=true\" >> .env\n   docker-compose restart\n\n   # Check logs for full SAML response\n   docker-compose logs | grep -A 50 \"SAML Response\"\n   ```\n\n**Example**:\n```bash\n# Certificate expired\nERROR: SAMLValidationError: IdP certificate expired\n\n# Fix: Update IdP metadata with new certificate\ncurl https://idp.example.com/metadata > idp-metadata.xml\n# Or update SAML_IDP_CERT in .env\n```\n\n**Related Errors**: ACGS-2211, ACGS-2214\n\n---\n\n### ACGS-2213: SAMLProviderError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `SAMLProviderError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/saml_handler.py`\n\n**Description**: Error communicating with SAML identity provider.\n\n**Common Causes**:\n- IdP metadata URL unreachable\n- Network connectivity to IdP failed\n- IdP is down or unavailable\n- DNS resolution failure\n- SSL/TLS certificate issues\n\n**Symptoms**:\n```\nSAMLProviderError: Cannot connect to IdP\nFailed to fetch metadata from https://idp.example.com/metadata\nConnection timeout to SAML provider\nSSL certificate verification failed\n```\n\n**Resolution**:\n1. **Test connectivity to IdP**:\n   ```bash\n   # Test IdP metadata endpoint\n   curl -v https://idp.example.com/metadata\n\n   # Test IdP SSO endpoint\n   curl -v https://idp.example.com/sso\n   ```\n\n2. **Verify DNS resolution**:\n   ```bash\n   nslookup idp.example.com\n   dig idp.example.com\n   ```\n\n3. **Check SSL certificate**:\n   ```bash\n   openssl s_client -connect idp.example.com:443 -servername idp.example.com\n   ```\n\n4. **Test from Docker network**:\n   ```bash\n   docker-compose exec enhanced-agent-bus curl -v https://idp.example.com/metadata\n   ```\n\n5. **Check IdP status** page or contact IdP administrator\n\n**Example**:\n```bash\n# IdP metadata unreachable\nERROR: SAMLProviderError: Cannot fetch metadata\n\n# Diagnosis:\ncurl -v https://idp.example.com/metadata\n\n# Workaround: Use local metadata file\n# Download metadata when IdP is available:\ncurl https://idp.example.com/metadata > idp-metadata.xml\n# Configure to use local file:\necho \"SAML_IDP_METADATA_FILE=/app/config/idp-metadata.xml\" >> .env\n```\n\n**Related Errors**: ACGS-2211, ACGS-2203\n\n---\n\n### ACGS-2214: SAMLReplayError\n\n**Severity**: CRITICAL\n**Impact**: Security-Violation\n**Exception**: `SAMLReplayError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/saml_handler.py`\n\n**Description**: SAML replay attack detected - same assertion used multiple times.\n\n**Common Causes**:\n- Replay attack attempt (malicious)\n- Browser back button during SSO flow\n- Assertion ID already used\n- Cached SAML response resubmitted\n\n**Symptoms**:\n```\nCRITICAL: SAMLReplayError: Assertion ID already used\nReplay attack detected: assertion ID <id> seen before\nDuplicate InResponseTo ID\n```\n\n**Security Impact**: This is a CRITICAL security error. SAML assertions must be single-use to prevent session hijacking.\n\n**Resolution**:\n1. **User action**: Initiate fresh SSO login\n   ```bash\n   # Don't use browser back button during SSO\n   # Start new login flow from application\n   ```\n\n2. **Check assertion ID cache**:\n   ```bash\n   # ACGS tracks used assertion IDs in Redis\n   docker-compose exec redis redis-cli KEYS \"saml_assertion:*\"\n   ```\n\n3. **Verify assertion ID uniqueness** in IdP configuration\n\n4. **If legitimate duplicate** (rare), may need to clear cache:\n   ```bash\n   # ONLY if confirmed not an attack\n   docker-compose exec redis redis-cli DEL \"saml_assertion:<assertion-id>\"\n   ```\n\n5. **Investigate potential attack**:\n   ```bash\n   # Check for repeated replay attempts from same IP\n   docker-compose logs | grep SAMLReplayError | \\\n     grep -o \"IP: [0-9.]*\" | sort | uniq -c\n\n   # If attack confirmed, block IP at firewall level\n   ```\n\n**Example**:\n```bash\n# Replay detected\nCRITICAL: SAMLReplayError: Assertion ID already used\n\n# Legitimate cause: User clicked back button\n# Resolution: Start fresh login from application\n\n# If attack suspected:\n# 1. Alert security team\n# 2. Review logs for pattern\n# 3. Block source IP if confirmed attack\n```\n\n**Related Errors**: ACGS-2211, ACGS-2212\n\n---\n\n### ACGS-2215: SAMLError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `SAMLError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/saml_handler.py`\n\n**Description**: Base exception for SAML-related errors. Generic SAML failure.\n\n**Resolution**: See related errors for specific SAML issues:\n- ACGS-2211: SAMLAuthenticationError\n- ACGS-2212: SAMLValidationError\n- ACGS-2213: SAMLProviderError\n- ACGS-2214: SAMLReplayError\n\nCheck logs for specific error:\n```bash\ndocker-compose logs | grep -i saml | tail -50\n```\n\n**Related Errors**: ACGS-2211, ACGS-2212, ACGS-2213, ACGS-2214\n\n---\n\n### ACGS-2221: AzureADAuthError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `AzureADAuthError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/azure_ad_connector.py`\n\n**Description**: Azure Active Directory authentication error.\n\n**Common Causes**:\n- Invalid Azure AD credentials\n- Application not registered in Azure AD\n- Missing required permissions/scopes\n- Tenant ID incorrect\n- Multi-factor authentication required\n\n**Symptoms**:\n```\nAzureADAuthError: Authentication failed\nAADSTS70011: Invalid scope\nAADSTS50126: Invalid username or password\nAADSTS50076: MFA required\n```\n\n**Resolution**:\n1. **Verify Azure AD configuration**:\n   ```bash\n   grep -E \"AZURE_AD_\" .env\n\n   # Required:\n   # AZURE_AD_TENANT_ID=<tenant-id>\n   # AZURE_AD_CLIENT_ID=<application-id>\n   # AZURE_AD_CLIENT_SECRET=<client-secret>\n   ```\n\n2. **Check application registration**:\n   - Azure Portal \u2192 App Registrations \u2192 Your App\n   - Verify client ID matches\n   - Ensure client secret hasn't expired\n\n3. **Verify required API permissions**:\n   - Microsoft Graph API permissions\n   - User.Read (minimum)\n   - Additional permissions as needed\n\n4. **Test Azure AD connectivity**:\n   ```bash\n   # Test token endpoint\n   curl -X POST https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \\\n     -d \"client_id=<client-id>\" \\\n     -d \"client_secret=<client-secret>\" \\\n     -d \"scope=https://graph.microsoft.com/.default\" \\\n     -d \"grant_type=client_credentials\"\n   ```\n\n**Example**:\n```bash\n# Invalid tenant ID\nERROR: AzureADAuthError: AADSTS90002: Tenant not found\n\n# Fix: Verify tenant ID in Azure Portal\necho \"AZURE_AD_TENANT_ID=<correct-tenant-id>\" >> .env\ndocker-compose restart identity-service\n```\n\n**Related Errors**: ACGS-2222, ACGS-2223, ACGS-2224\n\n---\n\n### ACGS-2222: AzureADConfigError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: `AzureADConfigError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/azure_ad_connector.py`\n\n**Description**: Azure AD configuration error prevents service startup.\n\n**Common Causes**:\n- Missing required Azure AD environment variables\n- Invalid tenant ID format\n- Client ID/secret not set\n- Configuration validation failed\n\n**Symptoms**:\n```\nAzureADConfigError: Missing required configuration\nAZURE_AD_TENANT_ID not set\nInvalid tenant ID format\nService failed to start\n```\n\n**Resolution**:\n1. **Set required environment variables**:\n   ```bash\n   # Get from Azure Portal \u2192 App Registrations\n   echo \"AZURE_AD_TENANT_ID=<tenant-id-or-domain.onmicrosoft.com>\" >> .env\n   echo \"AZURE_AD_CLIENT_ID=<application-id>\" >> .env\n   echo \"AZURE_AD_CLIENT_SECRET=<client-secret>\" >> .env\n   ```\n\n2. **Verify tenant ID format**:\n   ```bash\n   # Can be GUID or domain name:\n   # GUID: 12345678-1234-1234-1234-123456789012\n   # Domain: contoso.onmicrosoft.com\n   ```\n\n3. **Restart service**:\n   ```bash\n   docker-compose restart identity-service\n   ```\n\n**Example**:\n```bash\n# Missing configuration\nERROR: AzureADConfigError: AZURE_AD_CLIENT_ID not set\n\n# Fix:\necho \"AZURE_AD_CLIENT_ID=abc123...\" >> .env\ndocker-compose restart identity-service\n```\n\n**Related Errors**: ACGS-1101, ACGS-2221\n\n---\n\n### ACGS-2223: AzureADGraphError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `AzureADGraphError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/azure_ad_connector.py`\n\n**Description**: Microsoft Graph API error when querying Azure AD.\n\n**Common Causes**:\n- Insufficient Graph API permissions\n- User not found in Azure AD\n- Group query failed\n- API rate limit exceeded\n- Network error calling Graph API\n\n**Symptoms**:\n```\nAzureADGraphError: Graph API request failed\nInsufficient privileges to complete the operation\nUser not found\nRate limit exceeded: retry after 60 seconds\n```\n\n**Resolution**:\n1. **Verify Graph API permissions**:\n   - Azure Portal \u2192 App Registrations \u2192 API Permissions\n   - Required: User.Read.All or Directory.Read.All\n   - Ensure admin consent granted\n\n2. **Check API rate limits**:\n   ```bash\n   # Graph API has rate limits\n   # Implement exponential backoff for retries\n   ```\n\n3. **Test Graph API manually**:\n   ```bash\n   # Get access token\n   TOKEN=$(curl -X POST \\\n     https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \\\n     -d \"client_id=<client-id>\" \\\n     -d \"client_secret=<client-secret>\" \\\n     -d \"scope=https://graph.microsoft.com/.default\" \\\n     -d \"grant_type=client_credentials\" | jq -r .access_token)\n\n   # Query user\n   curl -H \"Authorization: Bearer $TOKEN\" \\\n     https://graph.microsoft.com/v1.0/users/user@example.com\n   ```\n\n**Example**:\n```bash\n# Insufficient permissions\nERROR: AzureADGraphError: Insufficient privileges\n\n# Fix: Grant required permissions in Azure Portal\n# Then grant admin consent\n```\n\n**Related Errors**: ACGS-2221, ACGS-2224\n\n---\n\n### ACGS-2224: AzureADError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `AzureADError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/azure_ad_connector.py`\n\n**Description**: Base exception for Azure AD errors.\n\n**Resolution**: See related errors:\n- ACGS-2221: AzureADAuthError\n- ACGS-2222: AzureADConfigError\n- ACGS-2223: AzureADGraphError\n\n---\n\n### ACGS-2231: OktaAuthError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `OktaAuthError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/okta_models.py`\n\n**Description**: Okta authentication error.\n\n**Common Causes**:\n- Invalid Okta API token\n- Okta domain incorrect\n- Application not configured in Okta\n- User authentication failed\n\n**Symptoms**:\n```\nOktaAuthError: Authentication failed\nInvalid Okta API token\nOkta domain not found\nE0000011: Invalid token provided\n```\n\n**Resolution**:\n1. **Verify Okta configuration**:\n   ```bash\n   grep -E \"OKTA_\" .env\n\n   # Required:\n   # OKTA_DOMAIN=your-domain.okta.com\n   # OKTA_API_TOKEN=<api-token>\n   # OKTA_CLIENT_ID=<client-id>\n   ```\n\n2. **Generate new API token** if needed:\n   - Okta Admin Console \u2192 Security \u2192 API \u2192 Tokens\n   - Create Token \u2192 Copy and save\n\n3. **Verify Okta domain**:\n   ```bash\n   # Format: your-domain.okta.com (no https://)\n   # Or: your-domain.oktapreview.com (preview)\n   ```\n\n4. **Test Okta API**:\n   ```bash\n   curl -H \"Authorization: SSWS <api-token>\" \\\n     https://your-domain.okta.com/api/v1/users/me\n   ```\n\n**Example**:\n```bash\n# Invalid API token\nERROR: OktaAuthError: Invalid token\n\n# Fix: Generate new token in Okta admin console\necho \"OKTA_API_TOKEN=<new-token>\" >> .env\ndocker-compose restart identity-service\n```\n\n**Related Errors**: ACGS-2232, ACGS-2233, ACGS-2234\n\n---\n\n### ACGS-2232: OktaConfigError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: `OktaConfigError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/okta_models.py`\n\n**Description**: Okta configuration error.\n\n**Common Causes**:\n- Missing Okta environment variables\n- Invalid domain format\n- API token not set\n\n**Resolution**:\n```bash\n# Set required configuration\necho \"OKTA_DOMAIN=your-domain.okta.com\" >> .env\necho \"OKTA_API_TOKEN=<api-token>\" >> .env\necho \"OKTA_CLIENT_ID=<client-id>\" >> .env\ndocker-compose restart identity-service\n```\n\n**Related Errors**: ACGS-1101, ACGS-2231\n\n---\n\n### ACGS-2233: OktaProvisioningError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `OktaProvisioningError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/okta_models.py`\n\n**Description**: Okta user provisioning failed.\n\n**Common Causes**:\n- User already exists in Okta\n- Required user attributes missing\n- Email domain not allowed\n- Provisioning disabled\n\n**Resolution**: Check Okta user provisioning configuration and ensure required attributes are provided.\n\n**Related Errors**: ACGS-2231, ACGS-2234, ACGS-2311\n\n---\n\n### ACGS-2234: OktaGroupError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `OktaGroupError` (identity-service)\n**Location**: `acgs2-core/services/identity/connectors/okta_models.py`\n\n**Description**: Okta group operation failed.\n\n**Common Causes**:\n- Group not found in Okta\n- Insufficient permissions to manage groups\n- User already member of group\n- Group provisioning disabled\n\n**Resolution**: Verify Okta group exists and API token has group management permissions.\n\n**Related Errors**: ACGS-2231, ACGS-2233\n\n---\n\n### ACGS-2301: RoleVerificationError\n\n**Severity**: HIGH\n**Impact**: Security-Gap\n**Location**: `acgs2-core/services/hitl_approvals/app/services/approval_chain_engine.py:148`\n\n**Description**: Role verification failed or not implemented. **TODO**: Implement role verification via OPA (see TODO_CATALOG.md HIGH priority item).\n\n**Current Behavior**: Role verification is not currently performed via OPA, creating a security gap.\n\n**Common Causes**:\n- Role verification not implemented (TODO pending)\n- User role not found\n- OPA policy for role verification not loaded\n- Role assignment missing\n\n**Symptoms**:\n```\nRoleVerificationError: Cannot verify user role\nTODO: Implement role verification via OPA\nUser role not validated\n```\n\n**Resolution**:\n\n**Temporary Workaround**:\n1. **Verify user roles in database**:\n   ```bash\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT id, email, roles FROM users WHERE email='user@example.com';\"\n   ```\n\n2. **Check approval chain configuration**:\n   ```bash\n   # Ensure approval chains have correct role requirements\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT id, name, required_roles FROM approval_chains;\"\n   ```\n\n**Permanent Fix** (TODO - HIGH priority):\n1. Implement OPA policy for role verification\n2. Add role verification call to approval chain engine\n3. Add role verification tests\n4. Document role verification process\n\n**TODO Reference**: See `TODO_CATALOG.md` - Item #2 (HIGH priority)\n\n**Example**:\n```bash\n# Role not verified via OPA\nWARN: RoleVerificationError: Role verification not implemented\n\n# Current workaround: Manual role assignment in database\n# Pending: OPA integration for role verification\n```\n\n**Related Errors**: ACGS-2302, ACGS-2401, ACGS-5101\n\n---\n\n### ACGS-2302: InsufficientPermissionsError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n\n**Description**: User lacks required permissions for the requested operation.\n\n**Common Causes**:\n- User role doesn't grant required permission\n- Permission not assigned to role\n- User not member of required group\n- Resource-specific permission denied\n\n**Symptoms**:\n```\n403 Forbidden: Insufficient permissions\nUser lacks permission: approve_high_risk\nRequired role: approver, actual: viewer\n```\n\n**Resolution**:\n1. **Check required permissions** for operation:\n   ```bash\n   # Query OPA for required permissions\n   curl -X POST http://localhost:8181/v1/data/acgs2/rbac/required_permissions \\\n     -d '{\"input\": {\"action\": \"approve\", \"resource\": \"/api/approvals/123\"}}'\n   ```\n\n2. **Verify user's current permissions**:\n   ```bash\n   # Query OPA for user permissions\n   curl -X POST http://localhost:8181/v1/data/acgs2/rbac/user_permissions \\\n     -d '{\"input\": {\"user\": \"user@example.com\"}}'\n   ```\n\n3. **Grant required permission or role**:\n   ```bash\n   # Via admin API or database\n   curl -X POST http://localhost:8080/api/users/<user-id>/roles \\\n     -H \"Authorization: Bearer <admin-token>\" \\\n     -d '{\"role\": \"approver\"}'\n   ```\n\n**Example**:\n```bash\n# User can't approve high-risk items\nERROR: InsufficientPermissionsError: Permission denied: approve_high_risk\n\n# Fix: Grant approver role with high-risk permission\n```\n\n**Related Errors**: ACGS-2002, ACGS-2301, ACGS-2401\n\n---\n\n### ACGS-2303: RoleMappingError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `RoleMappingError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/role_mapper.py`\n\n**Description**: Failed to map identity provider groups/roles to ACGS roles.\n\n**Common Causes**:\n- Role mapping configuration missing\n- IdP group not mapped to ACGS role\n- Attribute name mismatch (groups vs roles)\n- Mapping rules syntax error\n\n**Symptoms**:\n```\nRoleMappingError: Cannot map IdP groups to ACGS roles\nNo mapping found for group 'Engineering'\nAttribute 'groups' not found in SAML assertion\n```\n\n**Resolution**:\n1. **Configure role mappings**:\n   ```bash\n   # In .env or config file\n   cat > role_mappings.json <<EOF\n   {\n     \"Engineering\": [\"developer\", \"viewer\"],\n     \"Approvers\": [\"approver\"],\n     \"Admins\": [\"admin\"]\n   }\n   EOF\n   ```\n\n2. **Verify IdP provides group information**:\n   - SAML: Check 'groups' or 'roles' attribute in assertion\n   - OIDC: Check 'groups' claim in ID token\n   - Azure AD: Ensure group claims enabled\n\n3. **Check attribute name** configuration:\n   ```bash\n   # Configure which attribute contains groups\n   echo \"ROLE_MAPPING_ATTRIBUTE=groups\" >> .env\n   # Or for Azure AD:\n   echo \"ROLE_MAPPING_ATTRIBUTE=roles\" >> .env\n   ```\n\n**Example**:\n```bash\n# Group not mapped\nERROR: RoleMappingError: No mapping for group 'Engineering'\n\n# Fix: Add mapping\necho \"ROLE_MAPPING_Engineering=developer,viewer\" >> .env\ndocker-compose restart\n```\n\n**Related Errors**: ACGS-2304, ACGS-2311\n\n---\n\n### ACGS-2304: ProviderNotFoundError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `ProviderNotFoundError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/role_mapper.py`\n\n**Description**: Identity provider not found for role mapping.\n\n**Common Causes**:\n- Provider ID not configured\n- Provider not registered in system\n- Provider name typo\n- Multi-tenant provider issue\n\n**Resolution**:\n1. **List configured providers**:\n   ```bash\n   docker-compose exec postgres psql -U acgs2 -c \\\n     \"SELECT id, name, type FROM identity_providers;\"\n   ```\n\n2. **Register provider** if missing:\n   ```bash\n   # Via admin API\n   curl -X POST http://localhost:8080/api/identity-providers \\\n     -H \"Authorization: Bearer <admin-token>\" \\\n     -d '{\"name\": \"okta\", \"type\": \"oidc\", \"config\": {...}}'\n   ```\n\n**Example**:\n```bash\n# Provider not found\nERROR: ProviderNotFoundError: Provider 'okta' not found\n\n# Fix: Register provider in system\n```\n\n**Related Errors**: ACGS-2303, ACGS-2201, ACGS-2211\n\n---\n\n### ACGS-2311: ProvisioningError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `ProvisioningError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/provisioning.py`\n\n**Description**: Base exception for user provisioning errors.\n\n**Resolution**: See related errors:\n- ACGS-2312: DomainNotAllowedError\n- ACGS-2313: ProvisioningDisabledError\n\n---\n\n### ACGS-2312: DomainNotAllowedError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `DomainNotAllowedError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/provisioning.py`\n\n**Description**: User's email domain is not in the allowed list for auto-provisioning.\n\n**Common Causes**:\n- Domain allowlist configured but user domain not included\n- Typo in allowed domains configuration\n- Corporate domain not whitelisted\n\n**Symptoms**:\n```\nDomainNotAllowedError: Domain 'contractor.com' not in allowed list\nAuto-provisioning denied for user@contractor.com\nAllowed domains: example.com, corp.example.com\n```\n\n**Resolution**:\n1. **Check allowed domains configuration**:\n   ```bash\n   grep ALLOWED_EMAIL_DOMAINS .env\n   ```\n\n2. **Add domain to allowlist**:\n   ```bash\n   # Comma-separated list\n   echo \"ALLOWED_EMAIL_DOMAINS=example.com,contractor.com,corp.example.com\" >> .env\n   docker-compose restart\n   ```\n\n3. **Or disable domain restriction** (not recommended for production):\n   ```bash\n   # Allow all domains (use with caution)\n   echo \"ALLOWED_EMAIL_DOMAINS=*\" >> .env\n   ```\n\n**Example**:\n```bash\n# Domain not allowed\nERROR: DomainNotAllowedError: contractor.com not in allowed list\n\n# Fix: Add domain\necho \"ALLOWED_EMAIL_DOMAINS=example.com,contractor.com\" >> .env\ndocker-compose restart\n```\n\n**Related Errors**: ACGS-2311, ACGS-2313\n\n---\n\n### ACGS-2313: ProvisioningDisabledError\n\n**Severity**: LOW\n**Impact**: Informational\n**Exception**: `ProvisioningDisabledError` (shared-auth)\n**Location**: `acgs2-core/shared/auth/provisioning.py`\n\n**Description**: Auto-provisioning is disabled, manual user creation required.\n\n**Resolution**:\n1. **Enable auto-provisioning** if desired:\n   ```bash\n   echo \"AUTO_PROVISION_USERS=true\" >> .env\n   docker-compose restart\n   ```\n\n2. **Or manually create user**:\n   ```bash\n   curl -X POST http://localhost:8080/api/users \\\n     -H \"Authorization: Bearer <admin-token>\" \\\n     -d '{\"email\": \"user@example.com\", \"roles\": [\"viewer\"]}'\n   ```\n\n**Related Errors**: ACGS-2311, ACGS-2312\n\n---\n\n### ACGS-2401: PolicyEvaluationError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `PolicyEvaluationError` (enhanced-agent-bus, hitl-approvals)\n**Location**: `enhanced_agent_bus/exceptions.py`, `hitl_approvals/app/core/opa_client.py`\n\n**Description**: OPA policy evaluation failed during execution.\n\n**Common Causes**:\n- Policy execution error (runtime error in Rego)\n- Invalid input data format\n- Policy returns error result\n- Missing required input fields\n- Type mismatch in policy evaluation\n\n**Symptoms**:\n```\nPolicyEvaluationError: Policy evaluation failed\nOPA returned error: undefined variable 'user'\nType error in policy: expected string, got number\nPolicy result: {\"error\": \"division by zero\"}\n```\n\n**Resolution**:\n1. **Check OPA policy syntax**:\n   ```bash\n   # Validate policy\n   curl -X PUT http://localhost:8181/v1/policies/test \\\n     --data-binary @policy.rego\n\n   # If syntax error, fix and reload\n   ```\n\n2. **Verify input data format**:\n   ```bash\n   # Test policy with sample input\n   curl -X POST http://localhost:8181/v1/data/acgs2/policy/allow \\\n     -d '{\n       \"input\": {\n         \"user\": \"user@example.com\",\n         \"action\": \"read\",\n         \"resource\": \"/api/approvals\"\n       }\n     }'\n   ```\n\n3. **Check OPA logs** for detailed error:\n   ```bash\n   docker-compose logs opa | tail -50\n   ```\n\n4. **Review policy logic** for runtime errors:\n   - Division by zero\n   - Null pointer access\n   - Array index out of bounds\n   - Undefined variables\n\n**Example**:\n```bash\n# Policy evaluation error\nERROR: PolicyEvaluationError: undefined variable 'user.role'\n\n# Fix: Update policy or ensure input includes user.role\ncurl -X POST http://localhost:8181/v1/data/acgs2/rbac/allow \\\n  -d '{\"input\": {\"user\": {\"email\": \"test@example.com\", \"role\": \"admin\"}}}'\n```\n\n**Related Errors**: ACGS-2402, ACGS-2403, ACGS-2411\n\n---\n\n### ACGS-2402: PolicyNotFoundError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `PolicyNotFoundError` (enhanced-agent-bus)\n**Location**: `enhanced_agent_bus/exceptions.py`\n\n**Description**: Required OPA policy not found - query returns undefined.\n\n**Common Causes**:\n- Policy not loaded into OPA\n- Wrong policy path in query\n- Policy compilation failed\n- Policy bundle not deployed\n- Typo in policy package name\n\n**Symptoms**:\n```\nPolicyNotFoundError: Policy not found: acgs2.rbac.allow\nOPA query returned undefined\nNo policy at path: data.acgs2.policy.constitutional\nPolicy bundle failed to load\n```\n\n**Resolution**:\n1. **List loaded policies**:\n   ```bash\n   curl http://localhost:8181/v1/policies\n   # Should show all loaded .rego files\n   ```\n\n2. **Check policy bundle**:\n   ```bash\n   # If using bundle:\n   curl http://localhost:8181/v1/data\n   # Should show policy data\n   ```\n\n3. **Load missing policy**:\n   ```bash\n   # Upload policy\n   curl -X PUT http://localhost:8181/v1/policies/acgs2 \\\n     --data-binary @acgs2-policies.rego\n   ```\n\n4. **Verify policy path**:\n   ```bash\n   # Query structure: data.<package>.<rule>\n   # If policy package is: package acgs2.rbac\n   # Query should be: data.acgs2.rbac.allow\n   ```\n\n5. **Check OPA startup logs**:\n   ```bash\n   docker-compose logs opa | grep -i \"error\\|fail\"\n   # Look for bundle loading errors\n   ```\n\n**Example**:\n```bash\n# Policy not found\nERROR: PolicyNotFoundError: acgs2.constitutional.validate undefined\n\n# Diagnosis:\ncurl http://localhost:8181/v1/policies | jq .\n# Policy not loaded\n\n# Fix: Upload policy\ndocker-compose restart opa\n# Or manually upload:\ncurl -X PUT http://localhost:8181/v1/policies/constitutional \\\n  --data-binary @constitutional-policy.rego\n```\n\n**Related Errors**: ACGS-2401, ACGS-2403, ACGS-2404\n\n---\n\n### ACGS-2404: OPANotInitializedError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: `OPANotInitializedError` (enhanced-agent-bus, hitl-approvals)\n**Location**: `enhanced_agent_bus/exceptions.py`, `hitl_approvals/app/core/opa_client.py`\n\n**Description**: OPA client not properly initialized before use.\n\n**Common Causes**:\n- Service startup race condition\n- OPA initialization failed silently\n- Configuration error during OPA client creation\n- OPA_URL not set\n\n**Symptoms**:\n```\nOPANotInitializedError: OPA client not initialized\nCannot use OPA client before initialization\nService startup failed: OPA client null\n```\n\n**Resolution**:\n1. **Check service initialization order**:\n   ```bash\n   # Ensure OPA starts before dependent services\n   docker-compose logs --tail=100 opa enhanced-agent-bus\n   ```\n\n2. **Verify OPA_URL is set**:\n   ```bash\n   grep OPA_URL .env\n   # Should be: OPA_URL=http://opa:8181\n   ```\n\n3. **Restart services in correct order**:\n   ```bash\n   docker-compose up -d opa\n   # Wait for OPA to be ready\n   sleep 5\n   docker-compose up -d enhanced-agent-bus hitl-approvals\n   ```\n\n4. **Check OPA health before service starts**:\n   ```bash\n   # Service should wait for OPA health check\n   curl http://localhost:8181/health\n   # Should return: {\"status\": \"ok\"}\n   ```\n\n**Example**:\n```bash\n# OPA not initialized\nERROR: OPANotInitializedError: OPA client not initialized\n\n# Fix: Restart services in order\ndocker-compose restart opa\nsleep 5\ndocker-compose restart enhanced-agent-bus hitl-approvals\n```\n\n**Related Errors**: ACGS-2403, ACGS-1101\n\n---\n\n### ACGS-2411: PolicyError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `PolicyError` (enhanced-agent-bus)\n**Location**: `enhanced_agent_bus/exceptions.py`\n\n**Description**: Base exception for policy-related errors.\n\n**Resolution**: See related errors:\n- ACGS-2401: PolicyEvaluationError\n- ACGS-2402: PolicyNotFoundError\n\n---\n\n### ACGS-2412: OPAClientError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `OPAClientError` (hitl-approvals)\n**Location**: `hitl_approvals/app/core/opa_client.py`\n\n**Description**: Base exception for OPA client errors in HITL approvals service.\n\n**Resolution**: See related errors:\n- ACGS-2401: PolicyEvaluationError\n- ACGS-2403: OPAConnectionError\n- ACGS-2404: OPANotInitializedError\n\n---\n\n### ACGS-2413: OPAServiceError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `OPAServiceError` (cli)\n**Location**: `cli/opa_service.py`\n\n**Description**: Base exception for OPA service CLI errors.\n\n**Resolution**: Check CLI logs and OPA connectivity:\n```bash\ndocker-compose logs cli | grep -i opa\n```\n\n**Related Errors**: ACGS-2403, ACGS-2411, ACGS-2412\n\n---\n\n### ACGS-2501: TokenRefreshError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n\n**Description**: OAuth token refresh operation failed.\n\n**Common Causes**:\n- Refresh token expired\n- Refresh token revoked\n- Invalid client credentials\n- Token endpoint unreachable\n\n**Symptoms**:\n```\nTokenRefreshError: Failed to refresh access token\nRefresh token expired\ninvalid_grant: Refresh token has been revoked\n```\n\n**Resolution**:\n1. **Check refresh token expiration**:\n   ```bash\n   # Refresh tokens typically last 30-90 days\n   # Check token expiration in database or token store\n   ```\n\n2. **Verify client credentials**:\n   ```bash\n   # Ensure client_id and client_secret correct\n   grep -E \"CLIENT_ID|CLIENT_SECRET\" .env\n   ```\n\n3. **Re-authenticate** if refresh token expired:\n   ```bash\n   # User must re-authenticate via SSO/OAuth flow\n   # Cannot refresh with expired refresh token\n   ```\n\n4. **Test token refresh**:\n   ```bash\n   curl -X POST https://auth.example.com/oauth/token \\\n     -d \"grant_type=refresh_token\" \\\n     -d \"refresh_token=<refresh-token>\" \\\n     -d \"client_id=<client-id>\" \\\n     -d \"client_secret=<client-secret>\"\n   ```\n\n**Example**:\n```bash\n# Refresh token expired\nERROR: TokenRefreshError: Refresh token expired\n\n# Fix: User must re-authenticate\n# Redirect to SSO login page\n```\n\n**Related Errors**: ACGS-2104, ACGS-2502\n\n---\n\n### ACGS-2502: TokenRevocationError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n\n**Description**: Token revocation operation failed during logout or security event.\n\n**Common Causes**:\n- Revocation endpoint unreachable\n- Token already revoked\n- Invalid token format\n- Insufficient client permissions\n\n**Symptoms**:\n```\nTokenRevocationError: Failed to revoke token\nConnection failed to revocation endpoint\nToken not found or already revoked\n```\n\n**Resolution**:\n1. **Check revocation endpoint**:\n   ```bash\n   # Test connectivity\n   curl https://auth.example.com/.well-known/openid-configuration | \\\n     jq .revocation_endpoint\n   ```\n\n2. **Verify token is valid format**:\n   ```bash\n   # Ensure token isn't already expired/revoked\n   ```\n\n3. **Fallback**: Delete token locally even if revocation fails:\n   ```bash\n   # Remove from local token store\n   docker-compose exec redis redis-cli DEL \"token:<token-id>\"\n   ```\n\n**Example**:\n```bash\n# Revocation failed\nWARN: TokenRevocationError: Failed to revoke token at IdP\n\n# System should still delete token locally for security\n# User session terminated locally even if IdP revocation fails\n```\n\n**Related Errors**: ACGS-2501, ACGS-2104\n\n---\n\n### ACGS-2403: OPAConnectionError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable (Fail-Closed)\n**Exception**: `OPAConnectionError` (enhanced-agent-bus, hitl-approvals, cli)\n\n**Description**: Cannot connect to OPA policy server. **System fails closed** - all policy evaluations denied.\n\n**Common Causes**:\n- OPA container not running\n- Wrong OPA_URL (using `localhost` instead of Docker service name)\n- Port 8181 not accessible\n- Network connectivity issues\n- OPA startup not complete\n\n**Symptoms**:\n```\nCRITICAL: Cannot connect to OPA at http://opa:8181\nAll requests denied (fail-closed)\nConnection refused on port 8181\nOPA health check failed\n```\n\n**Impact Note**: When OPA is unavailable, the system **fails closed** - all authorization requests are denied to ensure security.\n\n**Resolution**:\n1. **Check if OPA is running**:\n   ```bash\n   docker-compose ps opa\n   # Should show \"Up\" status\n   ```\n\n2. **Start OPA if stopped**:\n   ```bash\n   docker-compose up -d opa\n   ```\n\n3. **Verify OPA is healthy**:\n   ```bash\n   curl http://localhost:8181/health\n   # Should return: {\"status\": \"ok\"}\n   ```\n\n4. **Check OPA_URL configuration**:\n   ```bash\n   # In Docker Compose context:\n   # Wrong: OPA_URL=http://localhost:8181\n   # Right: OPA_URL=http://opa:8181\n\n   grep OPA_URL .env\n   ```\n\n5. **Check OPA logs for startup errors**:\n   ```bash\n   docker-compose logs opa\n   # Look for policy bundle loading errors\n   ```\n\n6. **Verify network connectivity**:\n   ```bash\n   # From inside another container:\n   docker-compose exec enhanced-agent-bus ping opa\n   docker-compose exec enhanced-agent-bus curl http://opa:8181/health\n   ```\n\n7. **Check policy bundle loaded**:\n   ```bash\n   curl http://localhost:8181/v1/policies\n   # Should list loaded policies\n   ```\n\n**Example**:\n```bash\n# Typical failure:\nERROR: OPAConnectionError: Cannot connect to http://localhost:8181\n\n# Diagnosis:\ndocker-compose ps opa  # Check if running\ndocker-compose logs opa | tail -20  # Check for errors\n\n# Fix:\n# 1. Start OPA\ndocker-compose up -d opa\n\n# 2. Fix URL if needed\nsed -i 's|OPA_URL=http://localhost:8181|OPA_URL=http://opa:8181|' .env\n\n# 3. Restart dependent services\ndocker-compose restart enhanced-agent-bus hitl-approvals\n\n# 4. Verify\ncurl http://localhost:8181/health\n```\n\n**Frequency**: Very common during initial setup\n\n**Related Errors**: ACGS-2401, ACGS-2402, ACGS-2404, ACGS-4401\n\n---\n\n## ACGS-3xxx: Deployment/Infrastructure Errors\n\n**Category Description**: Errors related to deployment, infrastructure, containers, networking, and platform issues.\n\n**Common Severity**: CRITICAL (deployment-blocking) to MEDIUM\n\n**Related Components**: Docker, Kubernetes, network, ports\n\n---\n\n### ACGS-3101: DockerDaemonNotRunningError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n\n**Description**: Cannot connect to Docker daemon. This is the most common development environment error.\n\n**Common Causes**:\n- Docker Desktop not started (macOS/Windows)\n- Docker systemd service stopped (Linux)\n- Docker socket permissions issue\n- WSL2 integration disabled (Windows)\n\n**Symptoms**:\n```\nCannot connect to the Docker daemon at unix:///var/run/docker.sock\nIs the Docker daemon running?\ndocker-compose up fails immediately\n```\n\n**Resolution**:\n\n**For macOS/Windows**:\n1. **Start Docker Desktop**:\n   - Open Docker Desktop application\n   - Wait for \"Docker Desktop is running\" status\n   - Check whale icon in system tray is active\n\n2. **Verify Docker is running**:\n   ```bash\n   docker info\n   # Should show Docker version and system info\n   ```\n\n**For Linux**:\n1. **Check Docker service status**:\n   ```bash\n   systemctl status docker\n   ```\n\n2. **Start Docker service**:\n   ```bash\n   sudo systemctl start docker\n   ```\n\n3. **Enable Docker to start on boot**:\n   ```bash\n   sudo systemctl enable docker\n   ```\n\n4. **Add user to docker group** (if permission denied):\n   ```bash\n   sudo usermod -aG docker $USER\n   # Log out and log back in for changes to take effect\n   ```\n\n**For WSL2 (Windows)**:\n1. **Enable WSL integration** in Docker Desktop:\n   - Settings \u2192 Resources \u2192 WSL Integration\n   - Enable integration for your WSL2 distro\n\n2. **Restart WSL2**:\n   ```powershell\n   wsl --shutdown\n   # Then restart WSL\n   ```\n\n**Verification**:\n```bash\n# Test Docker is working\ndocker run hello-world\ndocker-compose version\n```\n\n**Frequency**: Very common in development (first-time setup)\n\n**Related Errors**: ACGS-3102, ACGS-3103, ACGS-8301\n\n---\n\n### ACGS-3301: PortAlreadyInUseError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n\n**Description**: Port conflict detected - another process is using the required port.\n\n**Common Ports**:\n- 8181: OPA\n- 8000: Agent Bus (**common macOS conflict with Airplay**)\n- 8080: API Gateway\n- 6379: Redis\n- 19092: Kafka (external)\n- 9092: Kafka (internal)\n- 5432: PostgreSQL\n- 8088: Temporal Web UI\n- 7233: Temporal gRPC\n\n**Symptoms**:\n```\nError: Port 8000 is already in use\nCannot start container: port already allocated\nAddress already in use: 0.0.0.0:8181\n```\n\n**Resolution**:\n\n1. **Identify process using port**:\n   ```bash\n   # Linux/macOS:\n   lsof -i :8000\n\n   # Windows:\n   netstat -ano | findstr :8000\n   ```\n\n2. **Kill the process**:\n   ```bash\n   # Linux/macOS:\n   lsof -ti:8000 | xargs kill -9\n\n   # Windows:\n   taskkill /PID <PID> /F\n   ```\n\n3. **For macOS port 8000 (Airplay conflict)**:\n   ```bash\n   # Disable Airplay Receiver:\n   # System Preferences \u2192 Sharing \u2192 Uncheck \"AirPlay Receiver\"\n\n   # OR change Agent Bus port in docker-compose.yml:\n   services:\n     enhanced-agent-bus:\n       ports:\n         - \"8001:8000\"  # Changed from 8000:8000\n   ```\n\n4. **Check for leftover Docker containers**:\n   ```bash\n   docker ps -a\n   docker rm -f $(docker ps -aq)  # Remove all containers\n   ```\n\n5. **Restart Docker Compose**:\n   ```bash\n   docker-compose down\n   docker-compose up -d\n   ```\n\n**Example - macOS Airplay Conflict**:\n```bash\n# Port 8000 in use by Airplay\nlsof -i :8000\n# COMMAND   PID   USER\n# ControlCe 1234  user\n\n# Solution 1: Disable Airplay (System Preferences \u2192 Sharing)\n\n# Solution 2: Change port\nsed -i '' 's/8000:8000/8001:8000/' docker-compose.yml\ndocker-compose up -d\n\n# Access Agent Bus on port 8001\ncurl http://localhost:8001/health\n```\n\n**Frequency**: Very common in development (especially macOS)\n\n**Related Errors**: ACGS-3302, ACGS-3303, ACGS-8201\n\n---\n\n### ACGS-3102: ContainerStartupError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: N/A (Docker-level error)\n\n**Description**: Container failed to start. The container was created but the application inside could not start successfully.\n\n**Common Causes**:\n- Missing or invalid environment variables\n- Port conflicts within the container\n- Volume mount errors preventing access to required files\n- Application code errors on startup\n- Insufficient resources (memory, CPU)\n- Database connection failures during initialization\n- Missing dependencies or misconfigured paths\n\n**Symptoms**:\n```\nContainer exited with code 1\nContainer exits immediately after starting\ndocker-compose up shows repeated restarts\nContainer status: Exited (1)\nLogs show application startup errors\n```\n\n**Resolution**:\n\n1. **Check container logs**:\n   ```bash\n   # View logs for the failed container\n   docker-compose logs <service-name>\n\n   # Follow logs in real-time\n   docker-compose logs -f <service-name>\n\n   # Get last 50 lines\n   docker logs --tail 50 <container-id>\n   ```\n\n2. **Verify environment variables**:\n   ```bash\n   # Check environment variables in container\n   docker-compose config\n\n   # Verify .env file exists and is loaded\n   cat .env\n\n   # Check for missing required variables\n   grep -E \"REQUIRED_|DATABASE_|REDIS_\" .env\n   ```\n\n3. **Check resource constraints**:\n   ```bash\n   # Inspect container configuration\n   docker inspect <container-id>\n\n   # Check resource limits\n   docker stats --no-stream\n\n   # View OOM events\n   dmesg | grep -i oom\n   ```\n\n4. **Test container interactively**:\n   ```bash\n   # Start container with shell override\n   docker-compose run --rm <service-name> /bin/bash\n\n   # Manually run startup command to see errors\n   python main.py  # or whatever the startup command is\n   ```\n\n5. **Verify port conflicts**:\n   ```bash\n   # Check if ports are already in use inside container\n   docker-compose ps\n   netstat -tulpn | grep <port>\n   ```\n\n6. **Check volume mounts**:\n   ```bash\n   # Verify volume mounts are accessible\n   docker-compose run --rm <service-name> ls -la /path/to/mount\n\n   # Check permissions\n   docker-compose run --rm <service-name> ls -la /app\n   ```\n\n7. **Rebuild and restart**:\n   ```bash\n   # Rebuild container from scratch\n   docker-compose build --no-cache <service-name>\n   docker-compose up -d <service-name>\n   ```\n\n**Example - Missing Environment Variable**:\n```bash\n# Container exits with code 1\ndocker-compose logs enhanced-agent-bus\n# Error: KeyError: 'CONSTITUTIONAL_HASH'\n\n# Add missing variable to .env\necho \"CONSTITUTIONAL_HASH=cdd01ef066bc6cf2\" >> .env\n\n# Recreate container\ndocker-compose up -d enhanced-agent-bus\n\n# Verify startup\ndocker-compose logs -f enhanced-agent-bus\n```\n\n**Frequency**: Common\n\n**Related Errors**: ACGS-1101, ACGS-3301, ACGS-3104, ACGS-3105\n\n---\n\n### ACGS-3103: ImagePullError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Docker/Kubernetes infrastructure error)\n\n**Description**: Failed to pull container image from registry. This prevents deployment from starting.\n\n**Common Causes**:\n- Network connectivity issues to container registry\n- Registry authentication failures\n- Image tag or version doesn't exist\n- Wrong registry URL or credentials\n- Corporate proxy blocking registry access\n- Rate limiting from public registries (Docker Hub)\n- Registry temporarily unavailable\n\n**Symptoms**:\n```\nError response from daemon: pull access denied\nmanifest unknown: manifest unknown\nno basic auth credentials\nTLS handshake timeout\ndial tcp: lookup registry-1.docker.io: no such host\ntoomanyrequests: You have reached your pull rate limit\n```\n\n**Resolution**:\n\n1. **Verify network connectivity**:\n   ```bash\n   # Test connectivity to Docker Hub\n   curl -I https://registry-1.docker.io/v2/\n\n   # Test connectivity to custom registry\n   curl -I https://your-registry.example.com/v2/\n\n   # Check DNS resolution\n   nslookup registry-1.docker.io\n   ```\n\n2. **Authenticate to registry**:\n   ```bash\n   # Login to Docker Hub\n   docker login\n   # Enter username and password\n\n   # Login to custom registry\n   docker login your-registry.example.com\n\n   # Login with token (CI/CD)\n   echo \"$REGISTRY_TOKEN\" | docker login -u username --password-stdin\n   ```\n\n3. **Verify image exists**:\n   ```bash\n   # Check if image and tag exist\n   docker pull <image>:<tag>\n\n   # List available tags (if you have access)\n   curl -s https://registry.hub.docker.com/v2/repositories/<image>/tags/ | jq\n   ```\n\n4. **Check proxy configuration**:\n   ```bash\n   # Configure Docker to use proxy\n   # Edit ~/.docker/config.json or /etc/systemd/system/docker.service.d/http-proxy.conf\n\n   # For systemd (Linux):\n   sudo mkdir -p /etc/systemd/system/docker.service.d\n   sudo cat > /etc/systemd/system/docker.service.d/http-proxy.conf <<EOF\n   [Service]\n   Environment=\"HTTP_PROXY=http://proxy.example.com:8080\"\n   Environment=\"HTTPS_PROXY=http://proxy.example.com:8080\"\n   Environment=\"NO_PROXY=localhost,127.0.0.1\"\n   EOF\n\n   sudo systemctl daemon-reload\n   sudo systemctl restart docker\n   ```\n\n5. **Handle rate limiting (Docker Hub)**:\n   ```bash\n   # Authenticate to increase rate limit\n   docker login\n\n   # Use mirror or cache\n   # Edit /etc/docker/daemon.json:\n   {\n     \"registry-mirrors\": [\"https://your-mirror.example.com\"]\n   }\n\n   sudo systemctl restart docker\n   ```\n\n6. **Retry with different registry**:\n   ```bash\n   # Pull from alternative registry\n   docker pull ghcr.io/<org>/<image>:<tag>\n\n   # Or build locally instead\n   docker-compose build\n   ```\n\n**Example - Authentication Failure**:\n```bash\n# Image pull fails\ndocker pull your-registry.example.com/acgs2/agent-bus:latest\n# Error: pull access denied\n\n# Login to registry\ndocker login your-registry.example.com\nUsername: youruser\nPassword: ********\n\n# Retry pull\ndocker pull your-registry.example.com/acgs2/agent-bus:latest\n# Success\n\n# Restart deployment\ndocker-compose up -d\n```\n\n**Frequency**: Occasional\n\n**Related Errors**: ACGS-3201, ACGS-3203, ACGS-3402\n\n---\n\n### ACGS-3104: VolumeMountError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Docker/Kubernetes infrastructure error)\n\n**Description**: Failed to mount volume to container. This prevents container from accessing required files or persistent data.\n\n**Common Causes**:\n- Mount path doesn't exist on host\n- Insufficient permissions on host path\n- SELinux blocking volume mount (Linux)\n- Path syntax incorrect (Windows paths)\n- Volume driver not available\n- Concurrent access conflicts\n- Read-only filesystem\n\n**Symptoms**:\n```\nError response from daemon: error while creating mount source path\nPermission denied\ninvalid mount config for type \"bind\": bind source path does not exist\nError: failed to create shim: OCI runtime create failed: runc create failed\nSELinux is preventing docker from read access on the directory\n```\n\n**Resolution**:\n\n1. **Verify path exists**:\n   ```bash\n   # Check if mount path exists\n   ls -la /path/to/mount\n\n   # Create if missing\n   mkdir -p /path/to/mount\n\n   # Check docker-compose.yml for correct paths\n   cat docker-compose.yml | grep -A 5 volumes:\n   ```\n\n2. **Fix permissions**:\n   ```bash\n   # Set correct permissions on host\n   sudo chmod 755 /path/to/mount\n\n   # Change ownership if needed\n   sudo chown -R $USER:$USER /path/to/mount\n\n   # For specific Docker user (e.g., UID 1000)\n   sudo chown -R 1000:1000 /path/to/mount\n   ```\n\n3. **Handle SELinux (Linux)**:\n   ```bash\n   # Check SELinux status\n   getenforce\n\n   # Add SELinux label to directory\n   sudo chcon -Rt svirt_sandbox_file_t /path/to/mount\n\n   # Or use :z or :Z suffix in docker-compose.yml\n   volumes:\n     - /path/to/mount:/container/path:z\n\n   # Temporarily disable for testing (not recommended for production)\n   sudo setenforce 0\n   ```\n\n4. **Fix Windows path issues**:\n   ```bash\n   # Use forward slashes in docker-compose.yml\n   volumes:\n     - C:/Users/username/data:/app/data\n\n   # Or use environment variable\n   volumes:\n     - ${PWD}/data:/app/data\n   ```\n\n5. **Check volume driver**:\n   ```bash\n   # List volume drivers\n   docker volume ls\n\n   # Inspect volume\n   docker volume inspect <volume-name>\n\n   # Create volume explicitly\n   docker volume create --name <volume-name>\n   ```\n\n6. **Test mount interactively**:\n   ```bash\n   # Test mount with simple container\n   docker run --rm -v /path/to/mount:/test:ro alpine ls -la /test\n\n   # Check if files are accessible\n   docker-compose run --rm <service> ls -la /mounted/path\n   ```\n\n**Example - SELinux Blocking Mount**:\n```bash\n# Container fails to start\ndocker-compose up -d hitl-approvals\n# Error: Permission denied\n\n# Check SELinux\ngetenforce\n# Enforcing\n\n# Add SELinux label\nsudo chcon -Rt svirt_sandbox_file_t ./data/hitl-approvals\n\n# Or update docker-compose.yml\nvolumes:\n  - ./data/hitl-approvals:/app/data:z\n\n# Restart\ndocker-compose up -d hitl-approvals\n```\n\n**Frequency**: Common (especially Linux/SELinux)\n\n**Related Errors**: ACGS-3102, ACGS-8301, ACGS-8101\n\n---\n\n### ACGS-3105: ContainerOOMError\n\n**Severity**: HIGH\n**Impact**: Service-Crash\n**Exception**: N/A (Kernel OOM killer)\n\n**Description**: Container killed due to out-of-memory condition (exit code 137). The Linux kernel OOM killer terminated the container when it exceeded memory limits.\n\n**Common Causes**:\n- Memory limit set too low for workload\n- Memory leak in application\n- Excessive concurrent requests\n- Large dataset loaded into memory\n- Insufficient swap space\n- Gradual memory growth over time\n- Spike in traffic causing memory spike\n\n**Symptoms**:\n```\nContainer exited with code 137\nOOMKilled: true\nReason: OOMKilled\ndmesg: Out of memory: Kill process\nContainer keeps restarting with 137 exit code\nMemory usage at 100% before crash\n```\n\n**Resolution**:\n\n1. **Confirm OOM kill**:\n   ```bash\n   # Check exit code\n   docker inspect <container-id> | jq '.[0].State'\n   # ExitCode: 137 indicates OOM kill\n\n   # Check kernel logs\n   dmesg | grep -i \"oom\\|killed\"\n   dmesg | grep -i \"<container-name>\"\n\n   # Check Docker events\n   docker events --filter 'event=oom' --since '1h'\n   ```\n\n2. **Analyze memory usage**:\n   ```bash\n   # Monitor memory usage\n   docker stats <container-name>\n\n   # Check memory limit\n   docker inspect <container-id> | jq '.[0].HostConfig.Memory'\n\n   # View memory trends\n   docker logs <container-id> | grep -i memory\n   ```\n\n3. **Increase memory limit**:\n   ```bash\n   # Update docker-compose.yml\n   services:\n     enhanced-agent-bus:\n       deploy:\n         resources:\n           limits:\n             memory: 2G  # Increased from 1G\n           reservations:\n             memory: 1G\n\n   # Restart with new limits\n   docker-compose up -d\n   ```\n\n4. **Optimize application**:\n   ```bash\n   # Profile memory usage\n   # For Python: use memory_profiler\n   pip install memory-profiler\n   python -m memory_profiler your_script.py\n\n   # Check for memory leaks\n   # Monitor over time and look for steady growth\n   docker stats --no-stream <container-name>\n   ```\n\n5. **Add swap (if appropriate)**:\n   ```bash\n   # Allow container to use swap (Docker)\n   services:\n     your-service:\n       deploy:\n         resources:\n           limits:\n             memory: 1G\n       mem_swappiness: 60  # Allow swap usage\n   ```\n\n6. **Implement graceful degradation**:\n   ```bash\n   # Add memory limits with buffer\n   # Set limit higher than reservation\n   deploy:\n     resources:\n       limits:\n         memory: 2G      # Hard limit\n       reservations:\n         memory: 1G      # Soft limit\n\n   # Implement memory monitoring in application\n   # to shed load before OOM\n   ```\n\n7. **Scale horizontally**:\n   ```bash\n   # Instead of increasing memory, add more instances\n   docker-compose up -d --scale enhanced-agent-bus=3\n\n   # Or in Kubernetes\n   kubectl scale deployment enhanced-agent-bus --replicas=3\n   ```\n\n**Example - OPA Memory Exhaustion**:\n```bash\n# OPA container keeps crashing\ndocker-compose logs opa\n# Last logs before crash...\n\n# Check exit code\ndocker inspect $(docker-compose ps -q opa) | jq '.[0].State.ExitCode'\n# 137\n\n# Check OOM in dmesg\ndmesg | grep opa\n# Out of memory: Killed process 1234 (opa)\n\n# Increase memory limit in docker-compose.yml\nservices:\n  opa:\n    deploy:\n      resources:\n        limits:\n          memory: 512M  # Increased from 256M\n        reservations:\n          memory: 256M\n\n# Restart\ndocker-compose up -d opa\n\n# Monitor\ndocker stats opa\n```\n\n**Frequency**: Common (production, high load)\n\n**Related Errors**: ACGS-3502, ACGS-7301, ACGS-3501\n\n---\n\n### ACGS-3201: NetworkConnectivityError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: N/A (Network-level error)\n\n**Description**: Network connectivity lost between services or to external resources. This is a general network failure.\n\n**Common Causes**:\n- Network interface down\n- Docker network misconfiguration\n- Firewall blocking traffic\n- Network partition in distributed system\n- DNS resolution failure\n- Routing table issues\n- Network driver failure\n\n**Symptoms**:\n```\nConnection refused\nConnection timed out\nNo route to host\nNetwork is unreachable\nCannot resolve hostname\ncurl: (7) Failed to connect\n```\n\n**Resolution**:\n\n1. **Check Docker network**:\n   ```bash\n   # List Docker networks\n   docker network ls\n\n   # Inspect network\n   docker network inspect <network-name>\n\n   # Check if containers are on same network\n   docker inspect <container-id> | jq '.[0].NetworkSettings.Networks'\n   ```\n\n2. **Test connectivity**:\n   ```bash\n   # Test from host\n   curl http://localhost:8000/health\n\n   # Test from container to container\n   docker exec <container-1> curl http://<container-2>:8000/health\n\n   # Test DNS resolution\n   docker exec <container> nslookup <service-name>\n   docker exec <container> ping <service-name>\n   ```\n\n3. **Recreate Docker network**:\n   ```bash\n   # Stop all containers\n   docker-compose down\n\n   # Remove network\n   docker network rm <network-name>\n\n   # Recreate\n   docker-compose up -d\n   ```\n\n4. **Check firewall**:\n   ```bash\n   # Check iptables (Linux)\n   sudo iptables -L -n\n\n   # Check firewalld\n   sudo firewall-cmd --list-all\n\n   # Temporarily disable for testing (not recommended)\n   sudo systemctl stop firewalld\n   ```\n\n5. **Verify network configuration**:\n   ```bash\n   # Check docker-compose network config\n   cat docker-compose.yml | grep -A 5 networks:\n\n   # Ensure all services use same network\n   networks:\n     acgs2-network:\n       driver: bridge\n   ```\n\n**Example - Service Cannot Reach OPA**:\n```bash\n# Agent bus cannot reach OPA\ndocker-compose logs enhanced-agent-bus\n# ConnectionError: Cannot connect to OPA at http://opa:8181\n\n# Test connectivity\ndocker exec enhanced-agent-bus curl http://opa:8181/health\n# curl: (6) Could not resolve host: opa\n\n# Check network\ndocker network inspect acgs2_default\n\n# Recreate network\ndocker-compose down\ndocker-compose up -d\n\n# Verify\ndocker exec enhanced-agent-bus curl http://opa:8181/health\n# OK\n```\n\n**Frequency**: Occasional\n\n**Related Errors**: ACGS-3202, ACGS-3204, ACGS-4101, ACGS-4201\n\n---\n\n### ACGS-3202: DNSResolutionError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: N/A (DNS infrastructure error)\n\n**Description**: DNS resolution failed - unable to resolve hostname to IP address.\n\n**Common Causes**:\n- DNS server unavailable\n- Docker DNS misconfiguration\n- /etc/resolv.conf issues\n- Network connectivity to DNS server lost\n- Hostname doesn't exist\n- DNS cache poisoning\n- Corporate DNS restrictions\n\n**Symptoms**:\n```\ncould not resolve host: <hostname>\ndial tcp: lookup <hostname>: no such host\ngetaddrinfo: Name or service not known\ntemporary failure in name resolution\n```\n\n**Resolution**:\n\n1. **Test DNS resolution**:\n   ```bash\n   # From host\n   nslookup google.com\n   dig google.com\n\n   # From container\n   docker exec <container> nslookup google.com\n   docker exec <container> cat /etc/resolv.conf\n   ```\n\n2. **Check Docker DNS configuration**:\n   ```bash\n   # Inspect container DNS settings\n   docker inspect <container> | jq '.[0].HostConfig.Dns'\n   docker inspect <container> | jq '.[0].HostConfig.DnsSearch'\n\n   # Check Docker daemon DNS\n   cat /etc/docker/daemon.json\n   ```\n\n3. **Configure custom DNS**:\n   ```bash\n   # Update /etc/docker/daemon.json\n   {\n     \"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n   }\n\n   # Restart Docker\n   sudo systemctl restart docker\n\n   # Or in docker-compose.yml\n   services:\n     your-service:\n       dns:\n         - 8.8.8.8\n         - 8.8.4.4\n   ```\n\n4. **Use IP addresses instead** (temporary workaround):\n   ```bash\n   # Get service IP\n   docker inspect <container> | jq '.[0].NetworkSettings.Networks[].IPAddress'\n\n   # Update connection string to use IP\n   # Instead of: http://opa:8181\n   # Use: http://172.18.0.5:8181\n   ```\n\n5. **Check /etc/hosts**:\n   ```bash\n   # View container hosts file\n   docker exec <container> cat /etc/hosts\n\n   # Add custom entry in docker-compose.yml\n   services:\n     your-service:\n       extra_hosts:\n         - \"opa:172.18.0.5\"\n   ```\n\n**Example - Cannot Resolve Service Name**:\n```bash\n# Service cannot resolve Redis\ndocker exec hitl-approvals nslookup redis\n# Server: 127.0.0.11\n# ** server can't find redis: NXDOMAIN\n\n# Check if Redis is running\ndocker-compose ps redis\n# redis   Up\n\n# Verify they're on same network\ndocker network inspect acgs2_default | jq '.[0].Containers'\n\n# Restart both services\ndocker-compose restart redis hitl-approvals\n\n# Verify DNS now works\ndocker exec hitl-approvals nslookup redis\n# Name: redis\n# Address: 172.18.0.3\n```\n\n**Frequency**: Occasional\n\n**Related Errors**: ACGS-3201, ACGS-3203\n\n---\n\n### ACGS-3203: ProxyConfigurationError\n\n**Severity**: MEDIUM\n**Impact**: Deployment-Blocking (if external registry/APIs required)\n**Exception**: N/A (Network configuration error)\n\n**Description**: Proxy misconfigured, preventing access to external resources through corporate proxy.\n\n**Common Causes**:\n- Missing proxy environment variables\n- Incorrect proxy URL or credentials\n- Proxy not configured for Docker daemon\n- No-proxy settings incomplete\n- Proxy authentication failure\n- SSL/TLS proxy inspection issues\n\n**Symptoms**:\n```\nProxyError: Cannot connect to proxy\n407 Proxy Authentication Required\nSSL certificate verification failed\ndial tcp: lookup proxy.example.com: no such host\nConnection to proxy refused\n```\n\n**Resolution**:\n\n1. **Set proxy environment variables**:\n   ```bash\n   # Add to .env file\n   HTTP_PROXY=http://proxy.example.com:8080\n   HTTPS_PROXY=http://proxy.example.com:8080\n   NO_PROXY=localhost,127.0.0.1,.example.com\n\n   # With authentication\n   HTTP_PROXY=http://username:password@proxy.example.com:8080\n   ```\n\n2. **Configure Docker daemon proxy**:\n   ```bash\n   # Create systemd override\n   sudo mkdir -p /etc/systemd/system/docker.service.d\n\n   # Create proxy config\n   sudo cat > /etc/systemd/system/docker.service.d/http-proxy.conf <<EOF\n   [Service]\n   Environment=\"HTTP_PROXY=http://proxy.example.com:8080\"\n   Environment=\"HTTPS_PROXY=http://proxy.example.com:8080\"\n   Environment=\"NO_PROXY=localhost,127.0.0.1,docker-registry.example.com\"\n   EOF\n\n   # Reload and restart\n   sudo systemctl daemon-reload\n   sudo systemctl restart docker\n   ```\n\n3. **Configure Docker client proxy** (for builds):\n   ```bash\n   # Edit ~/.docker/config.json\n   {\n     \"proxies\": {\n       \"default\": {\n         \"httpProxy\": \"http://proxy.example.com:8080\",\n         \"httpsProxy\": \"http://proxy.example.com:8080\",\n         \"noProxy\": \"localhost,127.0.0.1\"\n       }\n     }\n   }\n   ```\n\n4. **Pass proxy to containers**:\n   ```bash\n   # In docker-compose.yml\n   services:\n     your-service:\n       environment:\n         - HTTP_PROXY=http://proxy.example.com:8080\n         - HTTPS_PROXY=http://proxy.example.com:8080\n         - NO_PROXY=localhost,127.0.0.1,opa,redis,kafka\n   ```\n\n5. **Handle SSL inspection**:\n   ```bash\n   # Add corporate CA certificate\n   # Copy certificate to container\n   COPY corporate-ca.crt /usr/local/share/ca-certificates/\n   RUN update-ca-certificates\n\n   # Or disable SSL verification (not recommended)\n   export CURL_CA_BUNDLE=/path/to/ca-bundle.crt\n   ```\n\n**Example - Cannot Pull Images Through Proxy**:\n```bash\n# Image pull fails\ndocker pull redis:7-alpine\n# Error: dial tcp: lookup registry-1.docker.io\n\n# Configure Docker daemon proxy\nsudo mkdir -p /etc/systemd/system/docker.service.d\nsudo nano /etc/systemd/system/docker.service.d/http-proxy.conf\n# [Service]\n# Environment=\"HTTP_PROXY=http://proxy.corp.example.com:8080\"\n# Environment=\"HTTPS_PROXY=http://proxy.corp.example.com:8080\"\n# Environment=\"NO_PROXY=localhost,127.0.0.1\"\n\n# Restart Docker\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n# Retry pull\ndocker pull redis:7-alpine\n# Success\n```\n\n**Frequency**: Occasional (corporate environments)\n\n**Related Errors**: ACGS-3103, ACGS-3201\n\n---\n\n### ACGS-3204: NetworkPartitionError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: N/A (Network infrastructure failure)\n\n**Description**: Network partition detected - part of the distributed system cannot communicate with another part.\n\n**Common Causes**:\n- Network switch failure\n- Firewall rule changes\n- Network segmentation issues\n- Cloud provider network issues\n- Split-brain scenario in distributed system\n- Chaos engineering test (intentional)\n\n**Symptoms**:\n```\nCluster split detected\nCannot reach quorum\nPartitioned from coordinator\nMajority of nodes unreachable\nNetwork partition detected in logs\nHealth checks failing for subset of nodes\n```\n\n**Resolution**:\n\n1. **Detect partition**:\n   ```bash\n   # Check node connectivity\n   kubectl get nodes\n\n   # Check pod connectivity\n   kubectl get pods -o wide\n\n   # Test connectivity between nodes\n   ping <node-ip>\n\n   # Check Kafka cluster status\n   docker exec kafka kafka-topics.sh --bootstrap-server localhost:9092 --describe\n   ```\n\n2. **Verify network configuration**:\n   ```bash\n   # Check network routes\n   ip route show\n\n   # Check iptables rules\n   sudo iptables -L -n\n\n   # Check network interfaces\n   ip addr show\n   ```\n\n3. **Restore connectivity**:\n   ```bash\n   # Restart networking\n   sudo systemctl restart networking\n\n   # Restart network manager\n   sudo systemctl restart NetworkManager\n\n   # Flush iptables (if safe)\n   sudo iptables -F\n   ```\n\n4. **Handle split-brain** (if applicable):\n   ```bash\n   # Force rejoin to cluster\n   # (specific to your distributed system)\n\n   # For Kafka\n   docker exec kafka kafka-broker-api-versions.sh --bootstrap-server localhost:9092\n\n   # For PostgreSQL replication\n   # Promote standby or re-sync from primary\n   ```\n\n5. **Monitor and alert**:\n   ```bash\n   # Set up network monitoring\n   # Alert on packet loss >1%\n   # Alert on latency >100ms\n   # Alert on network interface status changes\n   ```\n\n**Example - Chaos Engineering Network Partition**:\n```bash\n# During chaos test, network partition injected\n# Redis and Kafka temporarily unreachable\n\n# Check connectivity\ndocker exec enhanced-agent-bus curl http://redis:6379\n# Connection refused\n\n# Partition heals automatically after test\n# Verify services recover\ndocker exec enhanced-agent-bus curl http://redis:6379\n# OK\n\n# Check logs for automatic recovery\ndocker-compose logs enhanced-agent-bus | grep -i \"redis.*recovered\"\n```\n\n**Frequency**: Rare (testing or emergencies)\n\n**Related Errors**: ACGS-3201, ACGS-4201, ACGS-4301\n\n---\n\n### ACGS-3302: PortBindingError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Docker/OS-level error)\n\n**Description**: Failed to bind to port due to insufficient permissions or system restrictions.\n\n**Common Causes**:\n- Attempting to bind to privileged port (<1024) without root\n- Port reserved by operating system\n- Container running as non-root user\n- SELinux/AppArmor restrictions\n- Port binding disabled in Docker configuration\n\n**Symptoms**:\n```\nError: permission denied while trying to connect to port\nbind: permission denied\ncannot bind to port 80: Permission denied\nError starting userland proxy: listen tcp4 0.0.0.0:80: bind: permission denied\n```\n\n**Resolution**:\n\n1. **Use non-privileged ports**:\n   ```bash\n   # Change port to >1024 in docker-compose.yml\n   services:\n     api-gateway:\n       ports:\n         - \"8080:8080\"  # Instead of 80:8080\n\n   # Or use host port >1024, map to container port 80\n   ports:\n     - \"8080:80\"\n   ```\n\n2. **Grant CAP_NET_BIND_SERVICE capability**:\n   ```bash\n   # Allow non-root user to bind privileged ports\n   # In docker-compose.yml\n   services:\n     api-gateway:\n       cap_add:\n         - NET_BIND_SERVICE\n   ```\n\n3. **Run container as root** (not recommended):\n   ```bash\n   # In docker-compose.yml\n   services:\n     your-service:\n       user: root\n   ```\n\n4. **Use sysctl to allow port binding**:\n   ```bash\n   # Allow non-root to bind ports <1024 (Linux)\n   sudo sysctl net.ipv4.ip_unprivileged_port_start=80\n\n   # Make permanent\n   echo \"net.ipv4.ip_unprivileged_port_start=80\" | sudo tee -a /etc/sysctl.conf\n   sudo sysctl -p\n   ```\n\n5. **Check SELinux/AppArmor**:\n   ```bash\n   # Check SELinux\n   getenforce\n   sudo ausearch -m avc -ts recent\n\n   # Add policy or use permissive mode for testing\n   sudo setenforce 0\n\n   # Check AppArmor\n   sudo aa-status\n   ```\n\n**Example - Cannot Bind to Port 80**:\n```bash\n# Service fails to bind port 80\ndocker-compose up api-gateway\n# Error: bind: permission denied\n\n# Change to unprivileged port\n# Edit docker-compose.yml\nservices:\n  api-gateway:\n    ports:\n      - \"8080:8080\"  # Changed from 80:8080\n\n# Restart\ndocker-compose up -d api-gateway\n\n# Access on new port\ncurl http://localhost:8080/health\n```\n\n**Frequency**: Occasional (production deployments)\n\n**Related Errors**: ACGS-3301, ACGS-8301\n\n---\n\n### ACGS-3303: PortAccessError\n\n**Severity**: MEDIUM\n**Impact**: Development-Issue (not blocking for production)\n**Exception**: N/A (Network configuration issue)\n\n**Description**: Cannot access service on port from host machine, despite service running.\n\n**Common Causes**:\n- Port not exposed in docker-compose.yml\n- Firewall blocking port on host\n- Service listening on 127.0.0.1 instead of 0.0.0.0\n- Wrong URL scheme (http vs https)\n- Docker/host network confusion\n- Port forwarding not configured (remote access)\n\n**Symptoms**:\n```\ncurl: (7) Failed to connect to localhost port 8000\nConnection refused from host\nService accessible from container but not from host\nBrowser cannot load http://localhost:8000\n```\n\n**Resolution**:\n\n1. **Verify port is exposed**:\n   ```bash\n   # Check docker-compose.yml\n   cat docker-compose.yml | grep -A 3 \"ports:\"\n\n   # Should have port mapping like:\n   ports:\n     - \"8000:8000\"  # host:container\n\n   # Check running containers\n   docker-compose ps\n   docker ps --format \"table {{.Names}}\\t{{.Ports}}\"\n   ```\n\n2. **Verify service is listening on 0.0.0.0**:\n   ```bash\n   # Check what service is listening on\n   docker exec <container> netstat -tlnp\n\n   # Should show 0.0.0.0:8000, not 127.0.0.1:8000\n\n   # Update application to listen on all interfaces\n   # For Python: app.run(host='0.0.0.0', port=8000)\n   # For Node: app.listen(8000, '0.0.0.0')\n   ```\n\n3. **Check firewall**:\n   ```bash\n   # Linux\n   sudo ufw status\n   sudo ufw allow 8000\n\n   # macOS\n   sudo /usr/libexec/ApplicationFirewall/socketfilterfw --getglobalstate\n\n   # Windows\n   netsh advfirewall firewall add rule name=\"Port 8000\" dir=in action=allow protocol=TCP localport=8000\n   ```\n\n4. **Test connectivity**:\n   ```bash\n   # Test from host\n   curl http://localhost:8000/health\n\n   # Test from another container\n   docker run --rm --network acgs2_default curlimages/curl curl http://enhanced-agent-bus:8000/health\n\n   # Check if service is accessible inside container\n   docker exec enhanced-agent-bus curl http://localhost:8000/health\n   ```\n\n5. **Fix port mapping**:\n   ```bash\n   # Add port mapping to docker-compose.yml\n   services:\n     enhanced-agent-bus:\n       ports:\n         - \"8000:8000\"\n\n   # Recreate container\n   docker-compose up -d enhanced-agent-bus\n\n   # Verify\n   curl http://localhost:8000/health\n   ```\n\n**Example - Port Not Exposed**:\n```bash\n# Cannot access agent bus from host\ncurl http://localhost:8000/health\n# curl: (7) Failed to connect\n\n# Check port mapping\ndocker ps | grep agent-bus\n# No 8000 in PORTS column\n\n# Add port mapping to docker-compose.yml\nservices:\n  enhanced-agent-bus:\n    ports:\n      - \"8000:8000\"\n\n# Recreate\ndocker-compose up -d enhanced-agent-bus\n\n# Verify\ncurl http://localhost:8000/health\n# {\"status\": \"healthy\"}\n```\n\n**Frequency**: Common (development)\n\n**Related Errors**: ACGS-3301, ACGS-3302\n\n---\n\n### ACGS-3401: PodCrashLoopBackOffError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: N/A (Kubernetes infrastructure state)\n\n**Description**: Kubernetes pod is in a crash loop - it starts, crashes, restarts repeatedly with exponential backoff.\n\n**Common Causes**:\n- Application startup failure\n- Missing ConfigMap or Secret\n- Liveness probe failing too quickly\n- Image entrypoint/command error\n- Resource constraints (memory/CPU)\n- Database not ready during startup\n- Missing dependencies\n\n**Symptoms**:\n```\nkubectl get pods\nNAME                        READY   STATUS             RESTARTS\nenhanced-agent-bus-xxx      0/1     CrashLoopBackOff   5\n\nEvents:\n  Back-off restarting failed container\n  Error: ImagePullBackOff\n  Liveness probe failed\n```\n\n**Resolution**:\n\n1. **Check pod status and logs**:\n   ```bash\n   # Get pod status\n   kubectl get pods\n   kubectl describe pod <pod-name>\n\n   # View logs from current container\n   kubectl logs <pod-name>\n\n   # View logs from previous container (after crash)\n   kubectl logs <pod-name> --previous\n\n   # Follow logs\n   kubectl logs -f <pod-name>\n   ```\n\n2. **Check recent events**:\n   ```bash\n   # View pod events\n   kubectl describe pod <pod-name> | grep -A 10 Events:\n\n   # View all events in namespace\n   kubectl get events --sort-by='.lastTimestamp'\n   ```\n\n3. **Verify ConfigMaps and Secrets**:\n   ```bash\n   # List ConfigMaps\n   kubectl get configmaps\n\n   # List Secrets\n   kubectl get secrets\n\n   # Check if referenced ConfigMap exists\n   kubectl describe pod <pod-name> | grep configmap\n   kubectl get configmap <configmap-name>\n\n   # Check Secret\n   kubectl get secret <secret-name>\n   ```\n\n4. **Check resource limits**:\n   ```bash\n   # View pod resource requests/limits\n   kubectl describe pod <pod-name> | grep -A 5 \"Limits:\\|Requests:\"\n\n   # Check node resources\n   kubectl top nodes\n   kubectl top pods\n\n   # Describe node\n   kubectl describe node <node-name>\n   ```\n\n5. **Adjust liveness probe**:\n   ```bash\n   # Edit deployment to increase probe delay\n   kubectl edit deployment <deployment-name>\n\n   # Increase initialDelaySeconds\n   livenessProbe:\n     httpGet:\n       path: /health\n       port: 8000\n     initialDelaySeconds: 60  # Increased from 10\n     periodSeconds: 10\n     timeoutSeconds: 5\n     failureThreshold: 3\n   ```\n\n6. **Test pod interactively**:\n   ```bash\n   # Run pod with shell override\n   kubectl run debug-pod --rm -it --image=<your-image> -- /bin/bash\n\n   # Or create debug container in pod\n   kubectl debug <pod-name> -it --image=<your-image>\n   ```\n\n7. **Fix and redeploy**:\n   ```bash\n   # Update deployment\n   kubectl apply -f deployment.yaml\n\n   # Or rollback to previous version\n   kubectl rollout undo deployment/<deployment-name>\n\n   # Watch rollout status\n   kubectl rollout status deployment/<deployment-name>\n   ```\n\n**Example - Missing ConfigMap**:\n```bash\n# Pod in CrashLoopBackOff\nkubectl get pods\n# enhanced-agent-bus-xxx   0/1   CrashLoopBackOff\n\n# Check logs\nkubectl logs enhanced-agent-bus-xxx --previous\n# Error: Config file /config/app-config.yaml not found\n\n# Check ConfigMap\nkubectl get configmap agent-bus-config\n# Error: configmap \"agent-bus-config\" not found\n\n# Create missing ConfigMap\nkubectl create configmap agent-bus-config \\\n  --from-file=app-config.yaml=./config/app-config.yaml\n\n# Wait for pod to restart\nkubectl get pods -w\n# enhanced-agent-bus-xxx   1/1   Running\n```\n\n**Frequency**: Very common (Kubernetes deployments)\n\n**Related Errors**: ACGS-3402, ACGS-3403, ACGS-3102, ACGS-1101\n\n---\n\n### ACGS-3402: ImagePullBackOffError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Kubernetes infrastructure error)\n\n**Description**: Kubernetes cannot pull container image - pod stuck in ImagePullBackOff state with exponential backoff.\n\n**Common Causes**:\n- Image doesn't exist in registry\n- Registry authentication failure\n- Wrong image tag or name\n- ImagePullSecret missing or invalid\n- Registry unreachable\n- Rate limiting from public registry\n\n**Symptoms**:\n```\nkubectl get pods\nNAME                        READY   STATUS              RESTARTS\napp-xxx                     0/1     ImagePullBackOff    0\n\nEvents:\n  Failed to pull image \"registry.example.com/app:v1.2.3\": rpc error\n  Back-off pulling image \"registry.example.com/app:v1.2.3\"\n  Error: ErrImagePull\n```\n\n**Resolution**:\n\n1. **Check image pull status**:\n   ```bash\n   # Get pod details\n   kubectl describe pod <pod-name>\n\n   # Look for Events section\n   kubectl describe pod <pod-name> | grep -A 10 \"Events:\"\n\n   # Check image pull errors\n   kubectl get events | grep -i \"pull\\|image\"\n   ```\n\n2. **Verify image exists**:\n   ```bash\n   # Try pulling image manually\n   docker pull <image>:<tag>\n\n   # Check image name and tag\n   kubectl get deployment <deployment-name> -o yaml | grep image:\n\n   # Verify image in registry (if accessible)\n   curl -u username:password https://registry.example.com/v2/<image>/tags/list\n   ```\n\n3. **Check ImagePullSecret**:\n   ```bash\n   # List secrets\n   kubectl get secrets\n\n   # Check if ImagePullSecret exists\n   kubectl get secret <imagepullsecret-name>\n\n   # Verify secret is referenced in deployment\n   kubectl get deployment <deployment-name> -o yaml | grep imagePullSecrets -A 2\n   ```\n\n4. **Create ImagePullSecret**:\n   ```bash\n   # Create secret for private registry\n   kubectl create secret docker-registry regcred \\\n     --docker-server=registry.example.com \\\n     --docker-username=<username> \\\n     --docker-password=<password> \\\n     --docker-email=<email>\n\n   # Or from Docker config\n   kubectl create secret generic regcred \\\n     --from-file=.dockerconfigjson=$HOME/.docker/config.json \\\n     --type=kubernetes.io/dockerconfigjson\n   ```\n\n5. **Update deployment to use secret**:\n   ```bash\n   # Edit deployment\n   kubectl edit deployment <deployment-name>\n\n   # Add imagePullSecrets\n   spec:\n     template:\n       spec:\n         imagePullSecrets:\n         - name: regcred\n         containers:\n         - name: app\n           image: registry.example.com/app:v1.2.3\n   ```\n\n6. **Verify image pull works**:\n   ```bash\n   # Delete pod to force new pull\n   kubectl delete pod <pod-name>\n\n   # Watch pod status\n   kubectl get pods -w\n\n   # Should change from ImagePullBackOff to Running\n   ```\n\n**Example - Missing ImagePullSecret**:\n```bash\n# Pod stuck in ImagePullBackOff\nkubectl describe pod enhanced-agent-bus-xxx\n# Failed to pull image: authentication required\n\n# Create ImagePullSecret\nkubectl create secret docker-registry regcred \\\n  --docker-server=ghcr.io \\\n  --docker-username=$GITHUB_USERNAME \\\n  --docker-password=$GITHUB_TOKEN \\\n  --docker-email=$GITHUB_EMAIL\n\n# Update deployment\nkubectl patch deployment enhanced-agent-bus -p '\n{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"imagePullSecrets\": [{\"name\": \"regcred\"}]\n      }\n    }\n  }\n}'\n\n# Verify pod starts\nkubectl get pods -w\n# enhanced-agent-bus-xxx   1/1   Running\n```\n\n**Frequency**: Common (Kubernetes with private registries)\n\n**Related Errors**: ACGS-3103, ACGS-3401\n\n---\n\n### ACGS-3403: PersistentVolumeError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Kubernetes infrastructure error)\n\n**Description**: PersistentVolume or PersistentVolumeClaim issues preventing pod from mounting storage.\n\n**Common Causes**:\n- PersistentVolumeClaim not bound to PersistentVolume\n- StorageClass misconfigured or missing\n- Insufficient storage available\n- Access mode mismatch\n- Volume still attached to another node\n- Dynamic provisioning failure\n\n**Symptoms**:\n```\nkubectl get pods\nNAME        READY   STATUS              RESTARTS\napp-xxx     0/1     ContainerCreating   0\n\nkubectl describe pod:\n  FailedMount: Unable to attach or mount volumes\n  PersistentVolumeClaim is not bound: \"data-pvc\"\n  waiting for a volume to be created\n```\n\n**Resolution**:\n\n1. **Check PVC status**:\n   ```bash\n   # List PVCs\n   kubectl get pvc\n\n   # Check PVC details\n   kubectl describe pvc <pvc-name>\n\n   # Should show:\n   # Status: Bound\n   # Volume: <pv-name>\n   ```\n\n2. **Check PV status**:\n   ```bash\n   # List PVs\n   kubectl get pv\n\n   # Check PV details\n   kubectl describe pv <pv-name>\n\n   # Status should be: Bound\n   # Claim should match your PVC\n   ```\n\n3. **Check StorageClass**:\n   ```bash\n   # List StorageClasses\n   kubectl get storageclass\n\n   # Check default StorageClass\n   kubectl get storageclass -o yaml | grep \"is-default-class: \\\"true\\\"\"\n\n   # Verify PVC uses correct StorageClass\n   kubectl get pvc <pvc-name> -o yaml | grep storageClassName\n   ```\n\n4. **Fix PVC/PV binding**:\n   ```bash\n   # If PVC is Pending, check events\n   kubectl describe pvc <pvc-name>\n\n   # Create PV manually if needed\n   cat <<EOF | kubectl apply -f -\n   apiVersion: v1\n   kind: PersistentVolume\n   metadata:\n     name: manual-pv\n   spec:\n     capacity:\n       storage: 10Gi\n     accessModes:\n       - ReadWriteOnce\n     persistentVolumeReclaimPolicy: Retain\n     storageClassName: manual\n     hostPath:\n       path: /mnt/data\n   EOF\n   ```\n\n5. **Check node attachment**:\n   ```bash\n   # Check which node has volume attached\n   kubectl get pods -o wide\n\n   # Describe node\n   kubectl describe node <node-name>\n\n   # Detach volume if stuck (cloud provider specific)\n   # AWS: aws ec2 detach-volume --volume-id vol-xxx\n   # GCP: gcloud compute instances detach-disk\n   ```\n\n6. **Recreate PVC** (if safe):\n   ```bash\n   # Delete pod\n   kubectl delete pod <pod-name>\n\n   # Delete PVC (WARNING: data loss if not backed up)\n   kubectl delete pvc <pvc-name>\n\n   # Recreate PVC\n   kubectl apply -f pvc.yaml\n\n   # Verify binding\n   kubectl get pvc\n   ```\n\n**Example - PVC Not Bound**:\n```bash\n# Pod stuck in ContainerCreating\nkubectl get pods\n# hitl-approvals-xxx   0/1   ContainerCreating\n\n# Check PVC\nkubectl get pvc\n# NAME              STATUS    VOLUME   CAPACITY\n# postgres-pvc      Pending\n\n# Describe PVC\nkubectl describe pvc postgres-pvc\n# Events: waiting for first consumer to be created before binding\n\n# Check StorageClass\nkubectl get storageclass\n# No default StorageClass found\n\n# Create or set default StorageClass\nkubectl patch storageclass standard -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\n# Delete pod to retry\nkubectl delete pod hitl-approvals-xxx\n\n# Watch PVC bind\nkubectl get pvc -w\n# postgres-pvc   Bound   pvc-xxx   10Gi\n```\n\n**Frequency**: Common (Kubernetes with persistent storage)\n\n**Related Errors**: ACGS-3401, ACGS-3104\n\n---\n\n### ACGS-3404: ServiceUnavailableError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n**Exception**: `ServiceUnavailableError` (sdk)\n**Location**: `acgs2-core/sdk/python/acgs2_sdk/exceptions.py`\n\n**Description**: Service is unavailable - typically HTTP 503 status code. The service cannot handle requests temporarily.\n\n**Common Causes**:\n- Service overloaded or at capacity\n- Upstream dependency failure\n- Health check failing\n- Circuit breaker open\n- Rate limiting active\n- Deployment in progress\n- Database connection pool exhausted\n\n**Symptoms**:\n```\nHTTP 503 Service Unavailable\nServiceUnavailableError raised in SDK\nRetry-After header in response\nService health check returning unhealthy\nCircuit breaker: OPEN\n```\n\n**Resolution**:\n\n1. **Check service health**:\n   ```bash\n   # Check health endpoint\n   curl http://localhost:8000/health\n\n   # Check Kubernetes pod status\n   kubectl get pods\n   kubectl describe pod <pod-name>\n\n   # Check Docker container\n   docker-compose ps\n   docker-compose logs <service-name>\n   ```\n\n2. **Check upstream dependencies**:\n   ```bash\n   # Test database\n   docker exec postgres psql -U postgres -c \"SELECT 1\"\n\n   # Test Redis\n   docker exec redis redis-cli PING\n\n   # Test Kafka\n   docker exec kafka kafka-broker-api-versions.sh --bootstrap-server localhost:9092\n\n   # Test OPA\n   curl http://localhost:8181/health\n   ```\n\n3. **Check resource utilization**:\n   ```bash\n   # Docker stats\n   docker stats --no-stream\n\n   # Kubernetes resources\n   kubectl top pods\n   kubectl top nodes\n\n   # Check connection pool\n   docker exec <service> ps aux | wc -l\n   ```\n\n4. **Check circuit breaker status**:\n   ```bash\n   # View application metrics/status\n   curl http://localhost:8000/metrics | grep circuit_breaker\n\n   # Check logs for circuit breaker events\n   docker-compose logs <service> | grep -i \"circuit.*open\"\n   ```\n\n5. **Scale service if overloaded**:\n   ```bash\n   # Docker Compose\n   docker-compose up -d --scale <service>=3\n\n   # Kubernetes\n   kubectl scale deployment <deployment> --replicas=3\n   ```\n\n6. **Restart service**:\n   ```bash\n   # Docker Compose\n   docker-compose restart <service>\n\n   # Kubernetes\n   kubectl rollout restart deployment/<deployment>\n   ```\n\n**Example - Service Overloaded**:\n```bash\n# Service returning 503\ncurl http://localhost:8000/api/approvals\n# HTTP/1.1 503 Service Unavailable\n\n# Check logs\ndocker-compose logs enhanced-agent-bus\n# Connection pool exhausted, max 100 connections\n\n# Check stats\ndocker stats enhanced-agent-bus\n# CPU: 95%, Memory: 1.8GB/2GB\n\n# Scale horizontally\ndocker-compose up -d --scale enhanced-agent-bus=3\n\n# Verify load distributed\ncurl http://localhost:8000/health\n# {\"status\": \"healthy\", \"connections\": 45}\n```\n\n**Frequency**: Occasional (high load, deployments)\n\n**Related Errors**: ACGS-7401, ACGS-3504, ACGS-4301\n\n---\n\n### ACGS-3501: CPUExhaustionError\n\n**Severity**: HIGH\n**Impact**: Performance-Degradation\n**Exception**: N/A (Resource limit)\n\n**Description**: CPU limit reached - container or pod is CPU-throttled.\n\n**Common Causes**:\n- CPU limit set too low\n- Excessive computation or processing\n- Inefficient algorithms or code\n- Sudden traffic spike\n- Background tasks consuming CPU\n\n**Symptoms**:\n```\nHigh CPU usage (95-100%)\nSlow response times\nCPU throttling in metrics\nContainer CPU usage at limit\nApplication timeouts\n```\n\n**Resolution**:\n\n1. **Monitor CPU usage**:\n   ```bash\n   # Docker\n   docker stats <container-name>\n\n   # Kubernetes\n   kubectl top pods\n   kubectl top nodes\n\n   # Detailed metrics\n   docker inspect <container> | jq '.[0].HostConfig.CpuQuota'\n   ```\n\n2. **Increase CPU limit**:\n   ```bash\n   # docker-compose.yml\n   services:\n     your-service:\n       deploy:\n         resources:\n           limits:\n             cpus: '2.0'  # Increased from 1.0\n           reservations:\n             cpus: '1.0'\n\n   # Restart\n   docker-compose up -d\n   ```\n\n3. **Profile application**:\n   ```bash\n   # Python profiling\n   python -m cProfile -o output.prof your_script.py\n\n   # Analyze profile\n   python -m pstats output.prof\n\n   # Or use py-spy for live profiling\n   py-spy top --pid <process-id>\n   ```\n\n4. **Optimize code**:\n   - Identify CPU-intensive operations\n   - Add caching for repeated computations\n   - Use more efficient algorithms\n   - Offload heavy processing to background tasks\n\n5. **Scale horizontally**:\n   ```bash\n   # Add more instances instead of increasing CPU\n   docker-compose up -d --scale <service>=3\n\n   # Kubernetes\n   kubectl scale deployment <deployment> --replicas=3\n   ```\n\n**Example**:\n```bash\n# Check CPU usage\ndocker stats enhanced-agent-bus\n# CPU: 98%\n\n# Increase CPU limit\n# Edit docker-compose.yml\nservices:\n  enhanced-agent-bus:\n    deploy:\n      resources:\n        limits:\n          cpus: '2.0'\n\n# Restart\ndocker-compose up -d enhanced-agent-bus\n\n# Verify\ndocker stats enhanced-agent-bus\n# CPU: 45%\n```\n\n**Frequency**: Occasional (high load)\n\n**Related Errors**: ACGS-3502, ACGS-7101, ACGS-7201\n\n---\n\n### ACGS-3502: MemoryExhaustionError\n\n**Severity**: HIGH\n**Impact**: Service-Crash\n**Exception**: N/A (Resource limit)\n\n**Description**: Memory limit reached or approaching limit. May lead to OOM kill (ACGS-3105).\n\n**Common Causes**:\n- Memory limit set too low\n- Memory leak in application\n- Large datasets in memory\n- Insufficient garbage collection\n- Caching too much data\n\n**Symptoms**:\n```\nMemory usage at or near limit\nSlow performance\nSwapping activity\nOOMKilled status\nContainer restarts with exit code 137\n```\n\n**Resolution**:\n\n1. **Monitor memory**:\n   ```bash\n   # Docker\n   docker stats <container-name>\n\n   # Kubernetes\n   kubectl top pods\n\n   # Check memory limit\n   docker inspect <container> | jq '.[0].HostConfig.Memory'\n   ```\n\n2. **Increase memory limit**:\n   ```bash\n   # docker-compose.yml\n   services:\n     your-service:\n       deploy:\n         resources:\n           limits:\n             memory: 2G  # Increased from 1G\n           reservations:\n             memory: 1G\n\n   docker-compose up -d\n   ```\n\n3. **Profile memory usage**:\n   ```bash\n   # Python memory profiler\n   pip install memory-profiler\n   python -m memory_profiler your_script.py\n\n   # Check for memory leaks\n   docker logs <container> | grep -i \"memory\\|oom\"\n   ```\n\n4. **Optimize application**:\n   - Fix memory leaks\n   - Implement streaming for large datasets\n   - Add cache eviction policies\n   - Use memory-efficient data structures\n\n**Frequency**: Common (production, high load)\n\n**Related Errors**: ACGS-3105, ACGS-3501, ACGS-7301\n\n---\n\n### ACGS-3503: DiskFullError\n\n**Severity**: CRITICAL\n**Impact**: Service-Crash\n**Exception**: N/A (Filesystem error)\n\n**Description**: Disk space exhausted - no space left on device.\n\n**Common Causes**:\n- Excessive logging\n- Large database growth\n- Docker image/container accumulation\n- Volume filling up\n- Temp files not cleaned\n\n**Symptoms**:\n```\nNo space left on device\nwrite /var/lib/docker: no space left on device\nENOSPC: no space left on device\nContainer exits due to disk full\n```\n\n**Resolution**:\n\n1. **Check disk usage**:\n   ```bash\n   # Overall disk usage\n   df -h\n\n   # Docker disk usage\n   docker system df\n\n   # Find large directories\n   du -sh /* | sort -hr | head -10\n   ```\n\n2. **Clean Docker resources**:\n   ```bash\n   # Remove unused containers, images, networks\n   docker system prune -a\n\n   # Remove unused volumes (WARNING: data loss)\n   docker volume prune\n\n   # Clean build cache\n   docker builder prune\n   ```\n\n3. **Rotate logs**:\n   ```bash\n   # Configure log rotation in docker-compose.yml\n   services:\n     your-service:\n       logging:\n         driver: \"json-file\"\n         options:\n           max-size: \"10m\"\n           max-file: \"3\"\n   ```\n\n4. **Expand disk** (if possible):\n   - Add more storage to VM\n   - Resize volume in cloud provider\n   - Move data to larger volume\n\n**Frequency**: Occasional (production, long-running)\n\n**Related Errors**: ACGS-7301, ACGS-3502\n\n---\n\n### ACGS-3504: ConnectionPoolExhaustedError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: N/A (Connection pool limit)\n\n**Description**: Connection pool is full - no available connections for new requests.\n\n**Common Causes**:\n- Pool size too small for load\n- Connections not being released\n- Connection leaks in code\n- Slow queries holding connections\n- Sudden traffic spike\n\n**Symptoms**:\n```\nConnection pool timeout\nAll connections in use\nQueuePool limit exceeded\nWaiting for available connection\nTimeoutError: could not obtain connection\n```\n\n**Resolution**:\n\n1. **Increase pool size**:\n   ```bash\n   # Environment variable\n   DATABASE_POOL_SIZE=50  # Increased from 20\n   DATABASE_MAX_OVERFLOW=10\n\n   # Or in application config\n   engine = create_engine(\n       database_url,\n       pool_size=50,\n       max_overflow=10\n   )\n   ```\n\n2. **Check for connection leaks**:\n   ```bash\n   # Monitor active connections\n   docker exec postgres psql -U postgres -c \"\n     SELECT count(*) FROM pg_stat_activity;\n   \"\n\n   # Find long-running queries\n   docker exec postgres psql -U postgres -c \"\n     SELECT pid, query, state, state_change\n     FROM pg_stat_activity\n     WHERE state != 'idle'\n     ORDER BY state_change;\n   \"\n   ```\n\n3. **Optimize connection usage**:\n   - Use connection pooling properly\n   - Ensure connections are released in `finally` blocks\n   - Add timeouts to queries\n   - Use connection recycling\n\n**Frequency**: Occasional (high concurrency)\n\n**Related Errors**: ACGS-4301, ACGS-3404, ACGS-7201\n\n---\n\n### ACGS-3601: AWSConfigurationError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Cloud provider configuration)\n\n**Description**: AWS-specific configuration error preventing deployment.\n\n**Common Causes**:\n- Invalid AWS credentials\n- Incorrect IAM permissions\n- Wrong region configuration\n- VPC/subnet misconfiguration\n- Security group rules blocking access\n\n**Symptoms**:\n```\nInvalidClientTokenId\nAccessDenied\nUnauthorizedOperation\nVPC not found\nSubnet not available in AZ\n```\n\n**Resolution**:\n\n1. **Verify AWS credentials**:\n   ```bash\n   # Check credentials\n   aws sts get-caller-identity\n\n   # Configure credentials\n   aws configure\n   ```\n\n2. **Check IAM permissions**:\n   ```bash\n   # Test specific permission\n   aws iam simulate-principal-policy \\\n     --policy-source-arn <user-arn> \\\n     --action-names ec2:RunInstances\n   ```\n\n3. **Verify region and resources**:\n   ```bash\n   # Check region\n   aws configure get region\n\n   # List VPCs\n   aws ec2 describe-vpcs\n\n   # Check subnet availability\n   aws ec2 describe-subnets --filters \"Name=vpc-id,Values=<vpc-id>\"\n   ```\n\n**Frequency**: Occasional (AWS deployments)\n\n**Related Errors**: ACGS-3602, ACGS-3603\n\n---\n\n### ACGS-3602: GCPConfigurationError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Cloud provider configuration)\n\n**Description**: GCP-specific configuration error preventing deployment.\n\n**Common Causes**:\n- Invalid GCP credentials\n- Project ID incorrect\n- API not enabled\n- Network/firewall misconfiguration\n- Quota exceeded\n\n**Symptoms**:\n```\nPermission denied\nProject not found\nAPI [service.googleapis.com] not enabled\nQuota exceeded\nInvalid network configuration\n```\n\n**Resolution**:\n\n1. **Verify credentials**:\n   ```bash\n   # Check authentication\n   gcloud auth list\n\n   # Login\n   gcloud auth login\n\n   # Set project\n   gcloud config set project <project-id>\n   ```\n\n2. **Enable required APIs**:\n   ```bash\n   # List enabled APIs\n   gcloud services list\n\n   # Enable API\n   gcloud services enable compute.googleapis.com\n   gcloud services enable container.googleapis.com\n   ```\n\n3. **Check quotas**:\n   ```bash\n   # View quotas\n   gcloud compute project-info describe --project=<project-id>\n\n   # Request quota increase if needed\n   ```\n\n**Frequency**: Occasional (GCP deployments)\n\n**Related Errors**: ACGS-3601, ACGS-3603\n\n---\n\n### ACGS-3603: AzureConfigurationError\n\n**Severity**: HIGH\n**Impact**: Deployment-Blocking\n**Exception**: N/A (Cloud provider configuration)\n\n**Description**: Azure-specific configuration error preventing deployment.\n\n**Common Causes**:\n- Invalid Azure credentials\n- Subscription not found\n- Resource group missing\n- Network configuration issues\n- Permission errors\n\n**Symptoms**:\n```\nAuthenticationFailed\nSubscriptionNotFound\nResourceGroupNotFound\nInvalidResourceReference\nAuthorization failed\n```\n\n**Resolution**:\n\n1. **Verify credentials**:\n   ```bash\n   # Check login\n   az account show\n\n   # Login\n   az login\n\n   # Set subscription\n   az account set --subscription <subscription-id>\n   ```\n\n2. **Verify resources**:\n   ```bash\n   # List resource groups\n   az group list\n\n   # Check resource group\n   az group show --name <resource-group>\n   ```\n\n3. **Check permissions**:\n   ```bash\n   # List role assignments\n   az role assignment list --assignee <user-email>\n   ```\n\n**Frequency**: Occasional (Azure deployments)\n\n**Related Errors**: ACGS-3601, ACGS-3602\n\n---\n\n### ACGS-3701: ApplicationFailoverError\n\n**Severity**: HIGH\n**Impact**: Service-Interruption\n**Exception**: N/A (Multi-region failover scenario)\n\n**Description**: Application-level failover failed during multi-region failover procedure.\n\n**RTO Target**: < 60 seconds\n\n**Common Causes**:\n- VirtualService weights not updating in Istio\n- Envoy proxy not reflecting configuration changes\n- Service mesh connectivity issues\n- DNS propagation delays\n- Health checks failing in target region\n\n**Symptoms**:\n```\nTraffic not redirecting to backup region\nIstio VirtualService weights unchanged\nEnvoy endpoints stale\n503 errors during failover window\nHealth checks timeout in secondary region\n```\n\n**Resolution**:\n\n1. **Verify Istio VirtualService**:\n   ```bash\n   # Check VirtualService configuration\n   kubectl get virtualservice -n acgs2-prod\n\n   # Verify weights\n   kubectl get virtualservice enhanced-agent-bus -o yaml | grep weight\n\n   # Should show shifted weights:\n   # - destination: primary-cluster\n   #   weight: 0\n   # - destination: secondary-cluster\n   #   weight: 100\n   ```\n\n2. **Force Envoy config reload**:\n   ```bash\n   # Restart Envoy sidecars\n   kubectl rollout restart deployment -n acgs2-prod\n\n   # Or kill individual Envoy containers\n   kubectl exec <pod> -c istio-proxy -- pkill envoy\n   ```\n\n3. **Check service mesh connectivity**:\n   ```bash\n   # Test connectivity from primary to secondary\n   kubectl exec -it <pod> -c app -- curl http://enhanced-agent-bus.secondary.svc.cluster.local/health\n\n   # Check Istio pilot logs\n   kubectl logs -n istio-system <istiod-pod>\n   ```\n\n4. **Manual traffic shift**:\n   ```bash\n   # Update VirtualService manually\n   kubectl patch virtualservice enhanced-agent-bus -p '\n   {\n     \"spec\": {\n       \"http\": [{\n         \"route\": [{\n           \"destination\": {\"host\": \"enhanced-agent-bus.secondary\"},\n           \"weight\": 100\n         }]\n       }]\n     }\n   }'\n   ```\n\n**Example - Failover During Regional Outage**:\n```bash\n# Primary region unhealthy, initiate failover\n./scripts/failover-to-secondary.sh\n\n# Check VirtualService weights\nkubectl get virtualservice enhanced-agent-bus -o yaml\n# weight: 0 (primary), weight: 100 (secondary)\n\n# But traffic still going to primary\ncurl http://enhanced-agent-bus.example.com/health\n# Error: Connection timeout (primary region down)\n\n# Force Envoy refresh\nkubectl rollout restart deployment enhanced-agent-bus -n acgs2-prod\n\n# Verify failover\ncurl http://enhanced-agent-bus.example.com/health\n# {\"status\": \"healthy\", \"region\": \"us-west-2\"}\n# Success - now serving from secondary\n\n# Monitor RTO\n# Total failover time: 45 seconds (within 60s target)\n```\n\n**Frequency**: Rare (multi-region emergency)\n\n**Related Errors**: ACGS-3702, ACGS-4311, ACGS-4211\n\n---\n\n### ACGS-3702: RegionalSyncError\n\n**Severity**: HIGH\n**Impact**: Data-Consistency-Risk\n**Exception**: N/A (Multi-region data sync)\n\n**Description**: Regional synchronization error - data not in sync between regions.\n\n**Common Causes**:\n- Database replication lag\n- Kafka MirrorMaker failure\n- Network partition between regions\n- Replication bandwidth insufficient\n- Cross-region latency high\n\n**Symptoms**:\n```\nReplication lag > threshold\nMirrorMaker consumer lag high\nData inconsistency between regions\nStale reads from secondary region\nSync status: DEGRADED\n```\n\n**Resolution**:\n\n1. **Check replication lag**:\n   ```bash\n   # PostgreSQL replication lag\n   docker exec postgres psql -U postgres -c \"\n     SELECT\n       application_name,\n       state,\n       sync_state,\n       pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS lag_bytes\n     FROM pg_stat_replication;\n   \"\n\n   # Should be < 1MB for RPO < 1 minute\n   ```\n\n2. **Check Kafka MirrorMaker**:\n   ```bash\n   # Check consumer lag\n   kafka-consumer-groups.sh \\\n     --bootstrap-server localhost:9092 \\\n     --group mirror-maker \\\n     --describe\n\n   # Lag should be < 1000 messages\n   ```\n\n3. **Monitor cross-region network**:\n   ```bash\n   # Test latency between regions\n   ping <secondary-region-endpoint>\n\n   # Check bandwidth\n   iperf3 -c <secondary-region-endpoint>\n   ```\n\n4. **Resolve sync issues**:\n   ```bash\n   # Restart replication if stalled\n   # PostgreSQL\n   docker exec postgres psql -U postgres -c \"\n     SELECT pg_reload_conf();\n   \"\n\n   # Kafka MirrorMaker\n   kubectl rollout restart deployment kafka-mirror-maker\n   ```\n\n**Frequency**: Occasional (multi-region)\n\n**Related Errors**: ACGS-3701, ACGS-4311, ACGS-4211\n\n---\n\n## ACGS-4xxx: Service Integration Errors\n\n**Category Description**: Errors related to external service integrations (Redis, Kafka, PostgreSQL, OPA).\n\n**Common Severity**: CRITICAL to HIGH\n\n**Related Services**: Redis, Kafka, PostgreSQL, OPA, external APIs\n\n---\n\n### ACGS-4101: RedisConnectionError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded (Cache unavailable, database fallback)\n**Exception**: `RedisConnectionError` (hitl-approvals/escalation), `RedisNotAvailableError` (hitl-approvals/audit)\n\n**Description**: Cannot connect to Redis. Service remains operational with database fallback (slower performance).\n\n**Common Causes**:\n- Redis container not running\n- Wrong REDIS_URL configuration\n- Port 6379 conflict\n- Network connectivity issues\n- Redis authentication failure\n\n**Symptoms**:\n```\nWarning: Redis connection failed, using database fallback\nConnection refused: redis:6379\nCache unavailable, queries slower\n```\n\n**Impact**: Service continues working but:\n- Cache misses go directly to database (slower)\n- Increased database load\n- Higher response latency\n- Session data may not persist\n\n**Resolution**:\n\n1. **Check if Redis is running**:\n   ```bash\n   docker-compose ps redis\n   # Should show \"Up\" status\n   ```\n\n2. **Start Redis if stopped**:\n   ```bash\n   docker-compose up -d redis\n   ```\n\n3. **Verify Redis is responding**:\n   ```bash\n   docker-compose exec redis redis-cli ping\n   # Should return: PONG\n   ```\n\n4. **Check REDIS_URL configuration**:\n   ```bash\n   grep REDIS_URL .env\n   # Should be: redis://redis:6379 (in Docker)\n   # Or: redis://localhost:6379 (host)\n   ```\n\n5. **Check authentication**:\n   ```bash\n   # If Redis requires password:\n   # REDIS_URL=redis://:password@redis:6379\n\n   # Test connection:\n   docker-compose exec redis redis-cli -a password ping\n   ```\n\n6. **Check Redis logs**:\n   ```bash\n   docker-compose logs redis | tail -20\n   ```\n\n7. **Restart dependent services**:\n   ```bash\n   docker-compose restart hitl-approvals enhanced-agent-bus\n   ```\n\n**Example**:\n```bash\n# Redis not running\ndocker-compose ps redis\n# State: Exit 1\n\n# Start Redis\ndocker-compose up -d redis\n\n# Verify connection\ndocker-compose exec redis redis-cli ping\n# PONG\n\n# Check service logs\ndocker-compose logs hitl-approvals | grep -i redis\n# Should show: \"Redis connection established\"\n```\n\n**Performance Impact**:\n- With Redis: P99 latency ~5ms\n- Without Redis: P99 latency ~50-100ms (database queries)\n\n**Related Errors**: ACGS-4102, ACGS-4103, ACGS-4104\n\n---\n\n### ACGS-4201: KafkaConnectionError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded (Async processing degraded)\n**Exception**: `KafkaConnectionError` (hitl-approvals)\n\n**Description**: Cannot connect to Kafka broker. Async message processing unavailable.\n\n**Common Causes**:\n- Kafka container not running\n- Zookeeper not running (Kafka dependency)\n- Wrong KAFKA_BOOTSTRAP_SERVERS configuration\n- Kafka still starting up (takes 30-60 seconds)\n- Network connectivity issues\n\n**Symptoms**:\n```\nError: Failed to connect to Kafka broker\nConnection error: kafka:19092\nAsync events not being processed\n```\n\n**Resolution**:\n\n1. **Check if Kafka and Zookeeper are running**:\n   ```bash\n   docker-compose ps kafka zookeeper\n   # Both should show \"Up\" status\n   ```\n\n2. **Start Kafka stack if stopped**:\n   ```bash\n   docker-compose up -d zookeeper\n   # Wait 10 seconds for Zookeeper\n   sleep 10\n   docker-compose up -d kafka\n   # Wait 30-60 seconds for Kafka to initialize\n   ```\n\n3. **Check Kafka logs for startup**:\n   ```bash\n   docker-compose logs kafka | grep -i \"started\"\n   # Look for: \"Kafka Server started\"\n   ```\n\n4. **Verify Kafka is accepting connections**:\n   ```bash\n   # List topics (should not error)\n   docker-compose exec kafka kafka-topics --list \\\n     --bootstrap-server localhost:9092\n   ```\n\n5. **Check KAFKA_BOOTSTRAP_SERVERS configuration**:\n   ```bash\n   grep KAFKA_BOOTSTRAP_SERVERS .env\n   # Should be: kafka:19092 (from inside Docker)\n   # Or: localhost:9092 (from host)\n   ```\n\n6. **Test Kafka connection**:\n   ```bash\n   # From inside container:\n   docker-compose exec hitl-approvals nc -zv kafka 19092\n   # Should show: Connection successful\n   ```\n\n7. **Restart services that depend on Kafka**:\n   ```bash\n   docker-compose restart hitl-approvals enhanced-agent-bus\n   ```\n\n**Startup Wait Time**: Kafka typically takes 30-60 seconds to fully start. Wait for log message:\n```\nINFO Kafka Server started (kafka.server.KafkaServer)\n```\n\n**Example**:\n```bash\n# Check Kafka status\ndocker-compose ps kafka zookeeper\n\n# If not running, start in correct order\ndocker-compose up -d zookeeper && sleep 10\ndocker-compose up -d kafka && sleep 30\n\n# Verify Kafka ready\ndocker-compose exec kafka kafka-broker-api-versions \\\n  --bootstrap-server localhost:9092\n\n# Check topic creation\ndocker-compose exec kafka kafka-topics --list \\\n  --bootstrap-server localhost:9092\n```\n\n**Frequency**: Common during initial setup and after system restarts\n\n**Related Errors**: ACGS-4202, ACGS-4203, ACGS-4204, ACGS-4205\n\n---\n\n### ACGS-4301: DatabaseConnectionError\n\n**Severity**: CRITICAL\n**Impact**: Service-Unavailable\n\n**Description**: Cannot connect to PostgreSQL database. All database-dependent services unavailable.\n\n**Common Causes**:\n- PostgreSQL container not running\n- Wrong DATABASE_URL connection string\n- Invalid credentials\n- Database not initialized\n- Network connectivity issues\n- Connection pool exhausted\n\n**Symptoms**:\n```\nCRITICAL: Database connection failed\nCould not connect to server: Connection refused\nFATAL: password authentication failed for user \"acgs2\"\nService unavailable: database required\n```\n\n**Resolution**:\n\n1. **Check if PostgreSQL is running**:\n   ```bash\n   docker-compose ps postgres\n   # Should show \"Up\" status\n   ```\n\n2. **Start PostgreSQL if stopped**:\n   ```bash\n   docker-compose up -d postgres\n   # Wait for database to initialize (~10 seconds)\n   ```\n\n3. **Check DATABASE_URL format**:\n   ```bash\n   grep DATABASE_URL .env\n   # Format: postgresql://user:password@host:port/database\n   # Example: postgresql://acgs2:password@postgres:5432/acgs2_db\n   ```\n\n4. **Verify database is responding**:\n   ```bash\n   docker-compose exec postgres pg_isready\n   # Should return: postgres:5432 - accepting connections\n   ```\n\n5. **Test connection with psql**:\n   ```bash\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \"SELECT 1;\"\n   # Should return: 1\n   ```\n\n6. **Check database logs**:\n   ```bash\n   docker-compose logs postgres | tail -30\n   # Look for initialization completion\n   ```\n\n7. **Verify credentials**:\n   ```bash\n   # Username and password in DATABASE_URL must match\n   # postgres container environment variables:\n   # POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB\n   ```\n\n8. **Check for migration issues**:\n   ```bash\n   # Run migrations if needed\n   docker-compose exec hitl-approvals alembic upgrade head\n   ```\n\n**Common Connection String Issues**:\n```bash\n# Wrong (using localhost in Docker context):\nDATABASE_URL=postgresql://acgs2:password@localhost:5432/acgs2_db\n\n# Right (using Docker service name):\nDATABASE_URL=postgresql://acgs2:password@postgres:5432/acgs2_db\n\n# With special characters in password (URL-encode):\n# Password: p@ssw0rd! \u2192 p%40ssw0rd%21\nDATABASE_URL=postgresql://acgs2:p%40ssw0rd%21@postgres:5432/acgs2_db\n```\n\n**Example**:\n```bash\n# Database not responding\npsql -h localhost -U acgs2 -d acgs2_db\n# Connection refused\n\n# Diagnosis:\ndocker-compose ps postgres\n# State: Exit 1\n\n# Fix: Start PostgreSQL\ndocker-compose up -d postgres\nsleep 10\n\n# Verify:\ndocker-compose exec postgres pg_isready\n# postgres:5432 - accepting connections\n\n# Test from service:\ndocker-compose exec hitl-approvals python -c \\\n  \"import psycopg2; psycopg2.connect('$DATABASE_URL'); print('OK')\"\n```\n\n**Related Errors**: ACGS-4302, ACGS-4303, ACGS-4304, ACGS-4305\n\n---\n\n### ACGS-4102: RedisAuthenticationError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Scenario**: Redis authentication failed (DEPLOYMENT_FAILURE_SCENARIOS.md #5.2)\n\n**Description**: Redis requires authentication but credentials are missing or invalid.\n\n**Common Causes**:\n- Missing password in REDIS_URL\n- Incorrect Redis password\n- Redis ACL configuration mismatch\n- Redis requirepass not configured properly\n- Password contains special characters not URL-encoded\n\n**Symptoms**:\n```\nNOAUTH Authentication required\nERR invalid password\nRedis authentication failed\nService degraded: cache unavailable\n```\n\n**Resolution**:\n\n1. **Check Redis configuration**:\n   ```bash\n   # Verify Redis requires auth\n   docker-compose exec redis redis-cli CONFIG GET requirepass\n   # Returns: requirepass <password>\n   ```\n\n2. **Update REDIS_URL with password**:\n   ```bash\n   # In .env file:\n   # Format: redis://:<password>@host:port\n   REDIS_URL=redis://:mypassword@redis:6379\n\n   # Or with username (Redis 6+):\n   REDIS_URL=redis://username:password@redis:6379\n   ```\n\n3. **URL-encode special characters in password**:\n   ```bash\n   # Password: p@ss!word#123\n   # Encoded: p%40ss%21word%23123\n   REDIS_URL=redis://:p%40ss%21word%23123@redis:6379\n   ```\n\n4. **Test authentication**:\n   ```bash\n   # With password\n   docker-compose exec redis redis-cli -a mypassword ping\n   # Should return: PONG\n\n   # Or using AUTH command\n   docker-compose exec redis redis-cli\n   > AUTH mypassword\n   > PING\n   # Should return: PONG\n   ```\n\n5. **Verify service can authenticate**:\n   ```bash\n   # Check service logs\n   docker-compose logs hitl-approvals | grep -i redis\n   # Should show: \"Redis connection established\"\n   ```\n\n6. **Restart dependent services**:\n   ```bash\n   docker-compose restart hitl-approvals enhanced-agent-bus\n   ```\n\n**Common Password Encoding Issues**:\n```bash\n# Password with @: pass@word \u2192 pass%40word\n# Password with !: pass!word \u2192 pass%21word\n# Password with #: pass#word \u2192 pass%23word\n# Password with %: pass%word \u2192 pass%25word\n```\n\n**Example**:\n```bash\n# Authentication failure\ndocker-compose logs redis\n# ERR: Client sent AUTH, but no password is set\n\n# Fix: Set password in docker-compose.yml\n# redis:\n#   command: redis-server --requirepass mypassword\n\n# Update .env\nREDIS_URL=redis://:mypassword@redis:6379\n\n# Restart Redis and services\ndocker-compose restart redis hitl-approvals\n\n# Verify\ndocker-compose exec redis redis-cli -a mypassword ping\n# PONG\n```\n\n**Security Note**: Store Redis password in secrets management system (e.g., Vault, AWS Secrets Manager) for production deployments.\n\n**Related Errors**: ACGS-4101, ACGS-4103, ACGS-1504\n\n---\n\n### ACGS-4103: RedisTimeoutError\n\n**Severity**: MEDIUM\n**Impact**: Performance-Degradation\n\n**Description**: Redis operation timed out. Network latency or Redis overloaded.\n\n**Common Causes**:\n- Network latency between service and Redis\n- Redis under heavy load (high CPU)\n- Slow command (e.g., KEYS *, large SCAN)\n- Redis memory eviction in progress\n- Network packet loss\n- Connection pool exhausted\n\n**Symptoms**:\n```\nRedisTimeoutError: Operation timed out after 5 seconds\nCache operation slow: 5000ms\nWarning: Redis timeout, using database fallback\nP99 latency spike: 5000ms\n```\n\n**Resolution**:\n\n1. **Check Redis performance**:\n   ```bash\n   # Monitor Redis stats\n   docker-compose exec redis redis-cli INFO stats\n   # Look at: total_connections_received, evicted_keys\n\n   # Check slow log\n   docker-compose exec redis redis-cli SLOWLOG GET 10\n   ```\n\n2. **Monitor Redis CPU and memory**:\n   ```bash\n   docker-compose stats redis\n   # CPU should be < 80%, Memory usage reasonable\n   ```\n\n3. **Check for slow commands**:\n   ```bash\n   # View slow log (commands > 10ms)\n   docker-compose exec redis redis-cli CONFIG GET slowlog-log-slower-than\n   docker-compose exec redis redis-cli SLOWLOG GET 20\n   ```\n\n4. **Check network connectivity**:\n   ```bash\n   # Test latency from service to Redis\n   docker-compose exec hitl-approvals ping -c 5 redis\n   # Should be < 1ms\n   ```\n\n5. **Increase timeout if necessary**:\n   ```bash\n   # In .env or service config:\n   REDIS_TIMEOUT=10  # Default is usually 5 seconds\n   REDIS_SOCKET_CONNECT_TIMEOUT=5\n   REDIS_SOCKET_KEEPALIVE=true\n   ```\n\n6. **Scale Redis if under load**:\n   ```bash\n   # Check Redis memory usage\n   docker-compose exec redis redis-cli INFO memory\n\n   # Consider Redis Cluster or read replicas\n   # for high-throughput scenarios\n   ```\n\n**Performance Tuning**:\n```bash\n# Optimize Redis configuration\ndocker-compose exec redis redis-cli CONFIG SET maxmemory-policy allkeys-lru\ndocker-compose exec redis redis-cli CONFIG SET lazyfree-lazy-eviction yes\n\n# Monitor improvements\ndocker-compose exec redis redis-cli INFO commandstats\n```\n\n**Example**:\n```bash\n# Identify slow operations\ndocker-compose exec redis redis-cli SLOWLOG GET 10\n# 1) 1) (integer) 123\n#    2) (integer) 1640000000\n#    3) (integer) 5000000  # 5 seconds!\n#    4) 1) \"KEYS\"\n#       2) \"*\"            # NEVER use KEYS in production!\n\n# Fix: Replace KEYS with SCAN\n# Before (slow): KEYS pattern*\n# After (fast): SCAN 0 MATCH pattern* COUNT 100\n\n# Verify performance improved\ndocker-compose logs hitl-approvals | grep -i \"cache.*ms\"\n```\n\n**Best Practices**:\n- Use SCAN instead of KEYS for pattern matching\n- Set appropriate TTL on cache keys\n- Use connection pooling (default in most clients)\n- Monitor Redis slow log regularly\n\n**Related Errors**: ACGS-4101, ACGS-4102, ACGS-7101\n\n---\n\n### ACGS-4104: RedisKeyNotFoundError\n\n**Severity**: LOW\n**Impact**: Informational\n\n**Description**: Cache key not found (cache miss). Normal behavior when data not in cache.\n\n**Common Causes**:\n- Key never set (first access)\n- Key expired (TTL reached)\n- Key evicted (memory pressure)\n- Key deleted explicitly\n- Cache cleared/flushed\n\n**Symptoms**:\n```\nCache miss: key not found\nFetching from database (cache miss)\nRedis key expired\n```\n\n**Behavior**: This is expected and normal. Service falls back to database on cache miss.\n\n**Resolution**: No action needed. Cache misses are expected behavior.\n\n**Monitoring**:\n```bash\n# Check cache hit rate\ndocker-compose exec redis redis-cli INFO stats | grep keyspace\n# keyspace_hits: 1000\n# keyspace_misses: 100\n# Hit rate = 1000 / (1000 + 100) = 90.9%\n\n# Target hit rate: > 80% for good cache performance\n```\n\n**Optimization** (if hit rate < 80%):\n```bash\n# 1. Increase cache TTL (if data doesn't change often)\nCACHE_TTL=3600  # 1 hour\n\n# 2. Increase Redis memory\n# In docker-compose.yml:\n# redis:\n#   command: redis-server --maxmemory 512mb\n\n# 3. Use better eviction policy\ndocker-compose exec redis redis-cli CONFIG SET maxmemory-policy allkeys-lru\n```\n\n**Example**:\n```bash\n# Cache miss on first access\ncurl http://localhost:8080/api/v1/approvals/123\n# Logs: Cache miss for approval:123, fetching from database\n\n# Second access (cache hit)\ncurl http://localhost:8080/api/v1/approvals/123\n# Logs: Cache hit for approval:123 (5ms vs 50ms)\n\n# Check cache stats\ndocker-compose exec redis redis-cli INFO stats | grep keyspace\n```\n\n**Related Errors**: ACGS-4101, ACGS-4103\n\n---\n\n### ACGS-4105: EscalationTimerError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `EscalationTimerError` (hitl-approvals)\n\n**Description**: Base exception for escalation timer errors in approval workflows.\n\n**Common Causes**:\n- Redis connection failed (timers stored in Redis)\n- Timer configuration invalid\n- Timer expired while processing\n- Redis key corruption\n\n**Symptoms**:\n```\nEscalation timer error\nFailed to schedule escalation timer\nTimer processing failed\nApproval escalation delayed\n```\n\n**Resolution**:\n\n1. **Verify Redis is running** (timers depend on Redis):\n   ```bash\n   docker-compose ps redis\n   # Should show \"Up\" status\n   ```\n\n2. **Check Redis connectivity**:\n   ```bash\n   docker-compose exec redis redis-cli ping\n   # PONG\n   ```\n\n3. **View pending timers**:\n   ```bash\n   # List escalation timers in Redis\n   docker-compose exec redis redis-cli KEYS \"timer:*\"\n   ```\n\n4. **Check HITL Approvals logs**:\n   ```bash\n   docker-compose logs hitl-approvals | grep -i \"escalation\\|timer\"\n   ```\n\n5. **Verify escalation configuration**:\n   ```bash\n   # Check service config\n   grep ESCALATION .env\n   # Example:\n   # ESCALATION_TIMEOUT=3600  # 1 hour\n   # ESCALATION_ENABLED=true\n   ```\n\n6. **Restart HITL Approvals service**:\n   ```bash\n   docker-compose restart hitl-approvals\n   ```\n\n**Timer Behavior**:\n- Timers schedule automatic escalation if approval not completed within SLA\n- Uses Redis for distributed timer storage\n- Escalates to next approver or admin after timeout\n\n**Example**:\n```bash\n# Check approval with escalation timer\ncurl http://localhost:8080/api/v1/approvals/123\n# {\n#   \"id\": 123,\n#   \"status\": \"pending\",\n#   \"escalation_timeout\": \"2024-01-03T15:00:00Z\"\n# }\n\n# Monitor escalation\ndocker-compose logs -f hitl-approvals | grep \"approval:123\"\n# Escalation timer set: approval:123 (3600s)\n# Escalation triggered: approval:123 \u2192 admin@example.com\n```\n\n**Related Errors**: ACGS-4101, ACGS-4106\n\n---\n\n### ACGS-4106: TimerNotFoundError\n\n**Severity**: LOW\n**Impact**: Informational\n**Exception**: `TimerNotFoundError` (hitl-approvals)\n\n**Description**: Escalation timer not found. Occurs when trying to cancel timer for already-completed approval.\n\n**Common Causes**:\n- Approval already completed (timer cleaned up)\n- Timer already fired (escalation occurred)\n- Timer never created\n- Redis key manually deleted\n\n**Symptoms**:\n```\nWarning: Timer not found for approval 123\nUnable to cancel timer: not found\nTimer already processed\n```\n\n**Behavior**: Normal when approval completes before escalation timeout.\n\n**Resolution**: No action needed. This is expected behavior when approvals complete quickly.\n\n**Example**:\n```bash\n# Fast approval (completed before escalation)\n# 1. Submit approval request (timer set for 1 hour)\ncurl -X POST http://localhost:8080/api/v1/approvals \\\n  -d '{\"request_id\": \"req-123\", \"approvers\": [\"user1@example.com\"]}'\n# Timer set: 3600s\n\n# 2. Approve within 5 minutes\ncurl -X POST http://localhost:8080/api/v1/approvals/123/approve\n# Status: approved\n# Log: Timer not found for approval 123 (already completed)\n\n# This is normal - approval was fast, timer not needed\n```\n\n**Related Errors**: ACGS-4105, ACGS-4101\n\n---\n\n### ACGS-4202: KafkaNotAvailableError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `KafkaNotAvailableError` (hitl-approvals)\n\n**Description**: aiokafka library not installed. Python dependency missing.\n\n**Common Causes**:\n- Missing aiokafka in requirements.txt\n- Virtual environment not activated\n- Dependency installation failed\n- Wrong Python environment\n\n**Symptoms**:\n```\nImportError: No module named 'aiokafka'\nKafkaNotAvailableError: aiokafka not installed\nAsync message processing unavailable\n```\n\n**Resolution**:\n\n1. **Install aiokafka**:\n   ```bash\n   # In service directory (e.g., hitl-approvals)\n   pip install aiokafka\n\n   # Or from requirements.txt\n   pip install -r requirements.txt\n   ```\n\n2. **Verify installation**:\n   ```bash\n   python -c \"import aiokafka; print(aiokafka.__version__)\"\n   # Should print version (e.g., 0.8.1)\n   ```\n\n3. **Rebuild Docker container**:\n   ```bash\n   # If using Docker, rebuild to include dependency\n   docker-compose build hitl-approvals\n   docker-compose up -d hitl-approvals\n   ```\n\n4. **Check requirements.txt**:\n   ```bash\n   # Ensure aiokafka is listed\n   grep aiokafka acgs2-core/services/hitl_approvals/requirements.txt\n   # Should show: aiokafka>=0.8.0\n   ```\n\n**For Docker Deployments**:\n```dockerfile\n# Ensure Dockerfile includes:\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Verify aiokafka in requirements.txt:\n# aiokafka>=0.8.0\n# kafka-python>=2.0.2  # Alternative client\n```\n\n**Example**:\n```bash\n# Error in logs\ndocker-compose logs hitl-approvals\n# ImportError: No module named 'aiokafka'\n\n# Fix: Rebuild container\ndocker-compose build hitl-approvals\ndocker-compose up -d hitl-approvals\n\n# Verify\ndocker-compose exec hitl-approvals python -c \"import aiokafka; print('OK')\"\n# OK\n```\n\n**Alternative**: If aiokafka unavailable, service may fall back to synchronous processing (slower, no async benefits).\n\n**Related Errors**: ACGS-4201, ACGS-4203\n\n---\n\n### ACGS-4203: KafkaPublishError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `KafkaPublishError` (hitl-approvals)\n\n**Description**: Failed to publish message to Kafka topic.\n\n**Common Causes**:\n- Kafka broker unavailable\n- Topic doesn't exist (and auto-create disabled)\n- Insufficient permissions\n- Message too large (exceeds max.message.bytes)\n- Network issues\n- Producer timeout\n\n**Symptoms**:\n```\nKafkaPublishError: Failed to publish message\nKafka producer timeout\nTopic not found: approval-events\nMessage size exceeds limit\n```\n\n**Resolution**:\n\n1. **Verify Kafka is running**:\n   ```bash\n   docker-compose ps kafka\n   # Should show \"Up\" status\n   ```\n\n2. **Check topic exists**:\n   ```bash\n   docker-compose exec kafka kafka-topics --list \\\n     --bootstrap-server localhost:9092\n   # Should list: approval-events, audit-events, etc.\n   ```\n\n3. **Create topic if missing**:\n   ```bash\n   docker-compose exec kafka kafka-topics --create \\\n     --topic approval-events \\\n     --partitions 3 \\\n     --replication-factor 1 \\\n     --bootstrap-server localhost:9092\n   ```\n\n4. **Check message size limits**:\n   ```bash\n   # Get max message size\n   docker-compose exec kafka kafka-configs --describe \\\n     --entity-type topics --entity-name approval-events \\\n     --bootstrap-server localhost:9092 | grep max.message.bytes\n\n   # Default is usually 1MB. Increase if needed:\n   docker-compose exec kafka kafka-configs --alter \\\n     --entity-type topics --entity-name approval-events \\\n     --add-config max.message.bytes=5242880 \\\n     --bootstrap-server localhost:9092\n   # 5MB limit\n   ```\n\n5. **Test publishing**:\n   ```bash\n   # Test with console producer\n   echo \"test message\" | docker-compose exec -T kafka \\\n     kafka-console-producer \\\n     --topic approval-events \\\n     --bootstrap-server localhost:9092\n   ```\n\n6. **Check producer configuration**:\n   ```bash\n   # In service config:\n   KAFKA_PRODUCER_TIMEOUT=30000  # 30 seconds\n   KAFKA_REQUEST_TIMEOUT=30000\n   KAFKA_MAX_REQUEST_SIZE=1048576  # 1MB\n   ```\n\n**Common Topic Naming**:\n- `approval-events` - Approval workflow events\n- `audit-events` - Audit trail events\n- `policy-updates` - OPA policy change events\n\n**Example**:\n```bash\n# Publishing fails\ndocker-compose logs hitl-approvals\n# KafkaPublishError: Topic 'approval-events' not found\n\n# Create topic\ndocker-compose exec kafka kafka-topics --create \\\n  --topic approval-events \\\n  --partitions 3 \\\n  --replication-factor 1 \\\n  --bootstrap-server localhost:9092\n# Created topic approval-events\n\n# Verify\ndocker-compose exec kafka kafka-topics --describe \\\n  --topic approval-events \\\n  --bootstrap-server localhost:9092\n\n# Retry operation\ncurl -X POST http://localhost:8080/api/v1/approvals/123/approve\n# Success: Event published to Kafka\n```\n\n**Related Errors**: ACGS-4201, ACGS-4205, ACGS-4206\n\n---\n\n### ACGS-4204: KafkaConsumerError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n\n**Description**: Kafka consumer error. Unable to consume messages from topic.\n\n**Common Causes**:\n- Consumer group blocked\n- Offset out of range\n- Topic partition reassignment\n- Consumer timeout\n- Deserialization error\n\n**Symptoms**:\n```\nKafka consumer error\nOffset out of range\nConsumer group rebalancing\nFailed to deserialize message\n```\n\n**Resolution**:\n\n1. **Check consumer group status**:\n   ```bash\n   docker-compose exec kafka kafka-consumer-groups --describe \\\n     --group approval-service \\\n     --bootstrap-server localhost:9092\n   ```\n\n2. **Reset consumer offset if out of range**:\n   ```bash\n   # Reset to earliest\n   docker-compose exec kafka kafka-consumer-groups --reset-offsets \\\n     --group approval-service \\\n     --topic approval-events \\\n     --to-earliest \\\n     --bootstrap-server localhost:9092 \\\n     --execute\n\n   # Or reset to latest (skip old messages)\n   --to-latest\n   ```\n\n3. **Monitor consumer lag**:\n   ```bash\n   docker-compose exec kafka kafka-consumer-groups --describe \\\n     --group approval-service \\\n     --bootstrap-server localhost:9092 | grep LAG\n   # LAG should be low (< 100 messages)\n   ```\n\n4. **Check service logs for deserialization errors**:\n   ```bash\n   docker-compose logs hitl-approvals | grep -i \"deserialize\\|kafka\"\n   ```\n\n5. **Restart consumer**:\n   ```bash\n   docker-compose restart hitl-approvals\n   ```\n\n**Example**:\n```bash\n# Check consumer lag\ndocker-compose exec kafka kafka-consumer-groups --describe \\\n  --group approval-service \\\n  --bootstrap-server localhost:9092\n# GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG\n# approval-service approval-events 0          100             1000            900\n# High lag (900 messages behind)\n\n# Verify consumer is processing\ndocker-compose logs -f hitl-approvals | grep \"Consumed message\"\n\n# If stuck, restart\ndocker-compose restart hitl-approvals\n```\n\n**Related Errors**: ACGS-4201, ACGS-4203, ACGS-4205\n\n---\n\n### ACGS-4205: KafkaTopicNotFoundError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n\n**Description**: Kafka topic doesn't exist and auto-creation is disabled.\n\n**Common Causes**:\n- Topic not created during setup\n- Wrong topic name in configuration\n- Auto-create disabled in Kafka broker\n- Typo in topic name\n\n**Symptoms**:\n```\nTopic 'approval-events' not found\nUnknown topic or partition\nBroker: Unknown topic or partition\n```\n\n**Resolution**:\n\n1. **List existing topics**:\n   ```bash\n   docker-compose exec kafka kafka-topics --list \\\n     --bootstrap-server localhost:9092\n   ```\n\n2. **Create missing topic**:\n   ```bash\n   docker-compose exec kafka kafka-topics --create \\\n     --topic approval-events \\\n     --partitions 3 \\\n     --replication-factor 1 \\\n     --bootstrap-server localhost:9092 \\\n     --config retention.ms=86400000  # 24 hours\n   ```\n\n3. **Verify topic configuration**:\n   ```bash\n   # In service .env or config:\n   KAFKA_TOPIC_APPROVALS=approval-events\n   KAFKA_TOPIC_AUDIT=audit-events\n   KAFKA_TOPIC_POLICIES=policy-updates\n   ```\n\n4. **Enable auto-create (not recommended for production)**:\n   ```bash\n   # In docker-compose.yml kafka environment:\n   KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n\n   # Restart Kafka\n   docker-compose restart kafka\n   ```\n\n**Required Topics for ACGS2**:\n```bash\n# Create all required topics\nfor topic in approval-events audit-events policy-updates agent-messages; do\n  docker-compose exec kafka kafka-topics --create \\\n    --topic $topic \\\n    --partitions 3 \\\n    --replication-factor 1 \\\n    --bootstrap-server localhost:9092 \\\n    --if-not-exists\ndone\n```\n\n**Example**:\n```bash\n# Service fails to start\ndocker-compose logs hitl-approvals\n# KafkaTopicNotFoundError: Topic 'approval-events' not found\n\n# Create topic\ndocker-compose exec kafka kafka-topics --create \\\n  --topic approval-events \\\n  --partitions 3 \\\n  --replication-factor 1 \\\n  --bootstrap-server localhost:9092\n\n# Verify creation\ndocker-compose exec kafka kafka-topics --describe \\\n  --topic approval-events \\\n  --bootstrap-server localhost:9092\n\n# Restart service\ndocker-compose restart hitl-approvals\n```\n\n**Related Errors**: ACGS-4201, ACGS-4203, ACGS-4204\n\n---\n\n### ACGS-4206: KafkaClientError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Exception**: `KafkaClientError` (hitl-approvals)\n\n**Description**: Base exception for Kafka client errors. Generic Kafka operation failure.\n\n**Common Causes**:\n- Kafka client library error\n- Invalid Kafka configuration\n- Protocol version mismatch\n- Network error\n- Authentication failure\n\n**Symptoms**:\n```\nKafkaClientError: Kafka operation failed\nClient exception occurred\nKafka protocol error\n```\n\n**Resolution**:\n\n1. **Check Kafka client logs**:\n   ```bash\n   docker-compose logs hitl-approvals | grep -i kafka\n   ```\n\n2. **Verify Kafka configuration**:\n   ```bash\n   grep KAFKA .env\n   # KAFKA_BOOTSTRAP_SERVERS=kafka:19092\n   # KAFKA_CLIENT_ID=hitl-approvals\n   # KAFKA_API_VERSION=auto\n   ```\n\n3. **Test Kafka connectivity**:\n   ```bash\n   docker-compose exec hitl-approvals nc -zv kafka 19092\n   # Connection successful\n   ```\n\n4. **Check Kafka broker version**:\n   ```bash\n   docker-compose exec kafka kafka-broker-api-versions \\\n     --bootstrap-server localhost:9092 | head -5\n   ```\n\n5. **Restart Kafka and dependent services**:\n   ```bash\n   docker-compose restart kafka\n   sleep 30  # Wait for Kafka to start\n   docker-compose restart hitl-approvals enhanced-agent-bus\n   ```\n\n**Example**:\n```bash\n# Generic Kafka error\ndocker-compose logs hitl-approvals\n# KafkaClientError: Operation failed\n\n# Check Kafka status\ndocker-compose ps kafka\n# Ensure \"Up\" and healthy\n\n# Check for specific error in Kafka logs\ndocker-compose logs kafka | tail -50\n\n# Restart services\ndocker-compose restart kafka hitl-approvals\n```\n\n**Related Errors**: ACGS-4201, ACGS-4203, ACGS-4204, ACGS-4205\n\n---\n\n### ACGS-4211: KafkaMirrorMakerError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded (Multi-Region)\n**Scenario**: Kafka MirrorMaker 2 failures (DEPLOYMENT_FAILURE_SCENARIOS.md #11.3)\n\n**Description**: Kafka MirrorMaker 2 replication error in multi-region deployments.\n\n**Common Causes**:\n- MirrorMaker container not running\n- Network connectivity between regions\n- Source or target cluster unavailable\n- Topic whitelist misconfiguration\n- Replication lag excessive\n\n**Symptoms**:\n```\nMirrorMaker 2 replication failed\nCross-region Kafka sync degraded\nReplication lag > 1000 messages\nConsumer group offset sync failed\n```\n\n**Resolution**:\n\n1. **Check MirrorMaker status**:\n   ```bash\n   docker-compose ps kafka-mirror-maker\n   # Should show \"Up\" status\n   ```\n\n2. **Monitor replication lag**:\n   ```bash\n   # Check MirrorMaker metrics\n   curl http://localhost:9090/metrics | grep mirror_lag\n   ```\n\n3. **Verify connectivity to both clusters**:\n   ```bash\n   # Test source cluster\n   docker-compose exec kafka-mirror-maker nc -zv source-kafka:9092\n\n   # Test target cluster\n   docker-compose exec kafka-mirror-maker nc -zv target-kafka:9092\n   ```\n\n4. **Check MirrorMaker configuration**:\n   ```bash\n   # Verify mm2.properties\n   docker-compose exec kafka-mirror-maker cat /etc/mm2/mm2.properties\n   # Check: source/target bootstrap servers, topic whitelist\n   ```\n\n5. **Review MirrorMaker logs**:\n   ```bash\n   docker-compose logs kafka-mirror-maker | tail -100\n   ```\n\n6. **Restart MirrorMaker**:\n   ```bash\n   docker-compose restart kafka-mirror-maker\n   ```\n\n**MirrorMaker 2 Configuration**:\n```properties\n# mm2.properties\nclusters = source, target\nsource.bootstrap.servers = source-kafka:9092\ntarget.bootstrap.servers = target-kafka:9092\nsource->target.enabled = true\nsource->target.topics = approval-.*, audit-.*, policy-.*\nreplication.factor = 3\nsync.topic.acls.enabled = false\n```\n\n**Example**:\n```bash\n# Check replication status\ndocker-compose exec kafka-mirror-maker \\\n  curl http://localhost:8083/connectors/mm2-checkpoint-connector/status\n# Should show: \"state\": \"RUNNING\"\n\n# Monitor lag\ndocker-compose exec target-kafka kafka-consumer-groups --describe \\\n  --group mm2-group \\\n  --bootstrap-server localhost:9092 | grep LAG\n\n# If high lag, check MirrorMaker logs\ndocker-compose logs kafka-mirror-maker | grep ERROR\n```\n\n**Note**: Only applicable for multi-region deployments with cross-region Kafka replication.\n\n**Related Errors**: ACGS-4201, ACGS-3702\n\n---\n\n### ACGS-4302: DatabaseQueryError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n\n**Description**: Database query execution failed.\n\n**Common Causes**:\n- SQL syntax error\n- Invalid table or column name\n- Constraint violation\n- Deadlock detected\n- Query timeout\n- Permission denied\n\n**Symptoms**:\n```\nDatabaseQueryError: Query execution failed\nSQL error: relation \"approvals\" does not exist\nDeadlock detected\nPermission denied for table approvals\n```\n\n**Resolution**:\n\n1. **Check query syntax**:\n   ```bash\n   # View service logs for SQL error\n   docker-compose logs hitl-approvals | grep -i \"SQL\\|query\"\n   ```\n\n2. **Verify database schema**:\n   ```bash\n   # List tables\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \"\\dt\"\n\n   # Describe table structure\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db \\\n     -c \"\\d approvals\"\n   ```\n\n3. **Run database migrations**:\n   ```bash\n   # Apply pending migrations\n   docker-compose exec hitl-approvals alembic upgrade head\n\n   # Check migration status\n   docker-compose exec hitl-approvals alembic current\n   ```\n\n4. **Check database permissions**:\n   ```bash\n   # Grant permissions if needed\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO acgs2;\"\n   ```\n\n5. **Investigate deadlocks**:\n   ```bash\n   # View deadlock details\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT * FROM pg_stat_activity WHERE wait_event_type = 'Lock';\"\n   ```\n\n**Example**:\n```bash\n# Query error: table doesn't exist\ndocker-compose logs hitl-approvals\n# relation \"approvals\" does not exist\n\n# Run migrations\ndocker-compose exec hitl-approvals alembic upgrade head\n# INFO  [alembic.runtime.migration] Running upgrade -> abc123, create approvals table\n\n# Verify table created\ndocker-compose exec postgres psql -U acgs2 -d acgs2_db -c \"\\dt\"\n# List of relations\n# Schema | Name      | Type  | Owner\n# public | approvals | table | acgs2\n\n# Retry operation\ncurl http://localhost:8080/api/v1/approvals\n# Success\n```\n\n**Related Errors**: ACGS-4301, ACGS-4303, ACGS-4304\n\n---\n\n### ACGS-4303: DatabaseTimeoutError\n\n**Severity**: HIGH\n**Impact**: Performance-Degradation\n\n**Description**: Database query timeout exceeded.\n\n**Common Causes**:\n- Slow query (missing index)\n- Database under heavy load\n- Lock contention\n- Large result set\n- Network latency\n- Connection pool exhausted\n\n**Symptoms**:\n```\nDatabaseTimeoutError: Query timeout after 30s\nStatement timeout\nQuery cancelled on user request\nConnection timeout\n```\n\n**Resolution**:\n\n1. **Identify slow queries**:\n   ```bash\n   # Enable slow query logging\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"ALTER DATABASE acgs2_db SET log_min_duration_statement = 1000;\"\n   # Logs queries > 1 second\n\n   # Check logs\n   docker-compose logs postgres | grep \"duration:\"\n   ```\n\n2. **Analyze query execution plan**:\n   ```bash\n   # Get EXPLAIN ANALYZE for slow query\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"EXPLAIN ANALYZE SELECT * FROM approvals WHERE status = 'pending';\"\n   ```\n\n3. **Add missing indexes**:\n   ```bash\n   # Create index on frequently queried columns\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"CREATE INDEX idx_approvals_status ON approvals(status);\"\n   ```\n\n4. **Check connection pool**:\n   ```bash\n   # View active connections\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT count(*) FROM pg_stat_activity WHERE datname = 'acgs2_db';\"\n\n   # Increase pool size if needed (in service config):\n   DATABASE_POOL_SIZE=20\n   DATABASE_MAX_OVERFLOW=10\n   ```\n\n5. **Increase query timeout** (if appropriate):\n   ```bash\n   # In service config:\n   DATABASE_QUERY_TIMEOUT=60  # 60 seconds\n\n   # Or per-session in PostgreSQL:\n   SET statement_timeout = 60000;  # milliseconds\n   ```\n\n6. **Optimize query**:\n   ```bash\n   # Add LIMIT to large queries\n   # Use pagination instead of fetching all rows\n   # SELECT * FROM approvals LIMIT 100 OFFSET 0\n   ```\n\n**Performance Tuning**:\n```sql\n-- Check table statistics\nANALYZE approvals;\n\n-- Vacuum to reclaim space\nVACUUM ANALYZE approvals;\n\n-- View index usage\nSELECT schemaname, tablename, indexname, idx_scan\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY idx_scan ASC;\n```\n\n**Example**:\n```bash\n# Slow query detected\ndocker-compose logs postgres\n# duration: 30000.123 ms  statement: SELECT * FROM approvals\n\n# Analyze query\ndocker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n  \"EXPLAIN ANALYZE SELECT * FROM approvals WHERE status = 'pending';\"\n# Seq Scan on approvals (cost=0.00..1000.00 rows=1000)\n# Missing index!\n\n# Add index\ndocker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n  \"CREATE INDEX idx_approvals_status ON approvals(status);\"\n\n# Verify improvement\ndocker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n  \"EXPLAIN ANALYZE SELECT * FROM approvals WHERE status = 'pending';\"\n# Index Scan using idx_approvals_status (cost=0.00..10.00 rows=100)\n# 100x faster!\n```\n\n**Related Errors**: ACGS-4301, ACGS-4302, ACGS-7101\n\n---\n\n### ACGS-4304: DatabaseConstraintError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n\n**Description**: Database constraint violation (unique, foreign key, check constraint).\n\n**Common Causes**:\n- Duplicate key violation (UNIQUE constraint)\n- Foreign key violation (referenced row doesn't exist)\n- Check constraint violation\n- NOT NULL constraint violation\n- Application logic error\n\n**Symptoms**:\n```\nIntegrityError: duplicate key value violates unique constraint\nForeignKeyViolation: Key is still referenced from table\nCheckViolation: new row violates check constraint\nNotNullViolation: null value in column \"status\" violates not-null constraint\n```\n\n**Resolution**:\n\n1. **Identify constraint violation**:\n   ```bash\n   # Check service logs for constraint name\n   docker-compose logs hitl-approvals | grep -i constraint\n   # Key (email)=(user@example.com) already exists\n   ```\n\n2. **View table constraints**:\n   ```bash\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT conname, contype FROM pg_constraint WHERE conrelid = 'approvals'::regclass;\"\n   ```\n\n3. **For duplicate key violations**:\n   ```bash\n   # Check existing records\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT * FROM approvals WHERE email = 'user@example.com';\"\n\n   # Application should handle: UPDATE instead of INSERT, or ignore duplicate\n   ```\n\n4. **For foreign key violations**:\n   ```bash\n   # Verify referenced record exists\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT * FROM users WHERE id = 123;\"\n\n   # Create referenced record first, or fix reference\n   ```\n\n5. **For check constraint violations**:\n   ```bash\n   # View constraint definition\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT conname, pg_get_constraintdef(oid) FROM pg_constraint WHERE conname = 'check_status';\"\n\n   # Ensure data meets constraint requirements\n   ```\n\n**Common Constraints in ACGS2**:\n```sql\n-- Unique constraints\nUNIQUE (email)\nUNIQUE (approval_id, user_id)\n\n-- Foreign keys\nFOREIGN KEY (user_id) REFERENCES users(id)\nFOREIGN KEY (policy_id) REFERENCES policies(id)\n\n-- Check constraints\nCHECK (status IN ('pending', 'approved', 'rejected'))\nCHECK (priority >= 0 AND priority <= 10)\n```\n\n**Example**:\n```bash\n# Duplicate key error\ncurl -X POST http://localhost:8080/api/v1/approvals \\\n  -d '{\"email\": \"user@example.com\", ...}'\n# Error: Key (email)=(user@example.com) already exists\n\n# Check existing record\ndocker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n  \"SELECT id, email, status FROM approvals WHERE email = 'user@example.com';\"\n# id |        email        | status\n# 1  | user@example.com    | pending\n\n# Fix: Update existing record instead\ncurl -X PUT http://localhost:8080/api/v1/approvals/1 \\\n  -d '{\"status\": \"approved\"}'\n# Success\n```\n\n**Application Fix**: Handle constraint violations gracefully with try/except and appropriate HTTP status codes (409 Conflict).\n\n**Related Errors**: ACGS-4301, ACGS-4302\n\n---\n\n### ACGS-4305: DatabaseReplicationError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded (Multi-Region)\n**Scenario**: Database replication lag (DEPLOYMENT_FAILURE_SCENARIOS.md #4.2)\n\n**Description**: Database replication lag or failure in multi-region setup.\n\n**Common Causes**:\n- Network latency between regions\n- Replication slot full\n- WAL archiving delayed\n- Replica fell too far behind\n- Disk I/O bottleneck on replica\n\n**Symptoms**:\n```\nReplication lag > 10 seconds\nReplica is behind by 1000 WAL segments\nReplication slot inactive\nRead replica unavailable\nData inconsistency between regions\n```\n\n**Resolution**:\n\n1. **Check replication status**:\n   ```bash\n   # On primary\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT client_addr, state, sync_state, replay_lag FROM pg_stat_replication;\"\n   ```\n\n2. **Monitor replication lag**:\n   ```bash\n   # On replica\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT now() - pg_last_xact_replay_timestamp() AS replication_lag;\"\n   # Should be < 1 second\n   ```\n\n3. **Check replication slots**:\n   ```bash\n   # On primary\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT slot_name, active, restart_lsn FROM pg_replication_slots;\"\n   ```\n\n4. **Investigate lag causes**:\n   ```bash\n   # Check WAL sender/receiver\n   docker-compose logs postgres | grep -i \"wal\\|replication\"\n\n   # Monitor network latency\n   docker-compose exec postgres ping -c 10 replica-postgres\n   ```\n\n5. **Force replication catchup**:\n   ```bash\n   # On replica, pause reads and let it catch up\n   # Monitor until lag < 1s\n   watch -n 1 'docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT now() - pg_last_xact_replay_timestamp() AS lag;\"'\n   ```\n\n6. **For severe lag, rebuild replica**:\n   ```bash\n   # Stop replica\n   docker-compose stop postgres-replica\n\n   # Create new basebackup from primary\n   docker-compose exec postgres pg_basebackup -h postgres -D /var/lib/postgresql/replica -U replicator -Fp -Xs -P -R\n\n   # Start replica\n   docker-compose start postgres-replica\n   ```\n\n**Monitoring Thresholds**:\n- **Normal**: < 1 second lag\n- **Warning**: 1-10 seconds lag\n- **Critical**: > 10 seconds lag\n- **Alerting**: > 30 seconds lag (data loss risk)\n\n**Example**:\n```bash\n# Check replication status\ndocker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n  \"SELECT client_addr, state, replay_lag FROM pg_stat_replication;\"\n#  client_addr   | state     | replay_lag\n#  10.0.2.5      | streaming | 00:00:15    # 15 seconds behind!\n\n# Investigate\ndocker-compose logs postgres | tail -50\n# LOG: could not send data to WAL stream: Connection reset\n\n# Check network\ndocker-compose exec postgres ping replica-postgres\n# High latency: time=500ms\n\n# Monitor until recovered\nwatch -n 1 'docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n  \"SELECT replay_lag FROM pg_stat_replication;\"'\n```\n\n**Note**: Only applicable for deployments with PostgreSQL replication (read replicas or multi-region).\n\n**Related Errors**: ACGS-4301, ACGS-4311, ACGS-3702\n\n---\n\n### ACGS-4311: DatabaseFailoverError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n**Scenario**: Database failover issues (DEPLOYMENT_FAILURE_SCENARIOS.md #4.3, #11.2)\n\n**Description**: Database failover failed or incomplete during primary failure.\n\n**Common Causes**:\n- Automatic failover not configured\n- Replica promotion failed\n- Connection string not updated\n- Health check failed to detect primary failure\n- Split-brain scenario (multiple primaries)\n\n**Symptoms**:\n```\nCRITICAL: Database primary unavailable\nFailover to replica failed\nCannot promote replica to primary\nSplit-brain detected: multiple primaries\nConnection refused: primary database\n```\n\n**Resolution**:\n\n1. **Verify primary status**:\n   ```bash\n   # Check if primary is truly down\n   docker-compose exec postgres pg_isready -h postgres\n   # postgres:5432 - rejecting connections (or timeout)\n   ```\n\n2. **Check replica status**:\n   ```bash\n   # Verify replica is healthy\n   docker-compose exec postgres-replica pg_isready\n   # postgres-replica:5432 - accepting connections\n   ```\n\n3. **Manual failover (if automatic failed)**:\n   ```bash\n   # Promote replica to primary\n   docker-compose exec postgres-replica pg_ctl promote -D /var/lib/postgresql/data\n\n   # Verify promotion\n   docker-compose exec postgres-replica psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT pg_is_in_recovery();\"\n   # f (false = now primary)\n   ```\n\n4. **Update application connection strings**:\n   ```bash\n   # Point all services to new primary\n   # Update DATABASE_URL in .env or service configs\n   DATABASE_URL=postgresql://acgs2:password@postgres-replica:5432/acgs2_db\n\n   # Restart services\n   docker-compose restart hitl-approvals enhanced-agent-bus\n   ```\n\n5. **Rebuild old primary as new replica** (after fixing):\n   ```bash\n   # Once old primary fixed, make it new replica\n   docker-compose exec postgres pg_basebackup -h postgres-replica -D /var/lib/postgresql/data -U replicator -Fp -Xs -P -R\n\n   # Start as replica\n   docker-compose start postgres\n   ```\n\n6. **Prevent split-brain**:\n   ```bash\n   # Verify only ONE primary exists\n   docker-compose exec postgres psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT pg_is_in_recovery();\"\n   docker-compose exec postgres-replica psql -U acgs2 -d acgs2_db -c \\\n     \"SELECT pg_is_in_recovery();\"\n   # One should be 'f' (primary), other 't' (replica)\n   ```\n\n**Automatic Failover with Patroni** (recommended for production):\n```yaml\n# docker-compose.yml\npatroni:\n  image: patroni/patroni:latest\n  environment:\n    PATRONI_SCOPE: acgs2-cluster\n    PATRONI_POSTGRESQL_DATA_DIR: /data/postgres\n    # Automatic failover enabled\n```\n\n**Recovery Time Objectives (RTO)**:\n- **Detection**: < 30 seconds (health checks)\n- **Promotion**: < 30 seconds (pg_ctl promote)\n- **Application switchover**: < 60 seconds (restart services)\n- **Total RTO**: < 2 minutes\n\n**Example**:\n```bash\n# Primary failure detected\ndocker-compose ps postgres\n# State: Exit 1\n\n# Check replica\ndocker-compose ps postgres-replica\n# State: Up\n\n# Promote replica\ndocker-compose exec postgres-replica pg_ctl promote -D /var/lib/postgresql/data\n# server promoting\n\n# Update connection strings\nsed -i 's/postgres:5432/postgres-replica:5432/g' .env\n\n# Restart services\ndocker-compose restart hitl-approvals enhanced-agent-bus audit-service\n\n# Verify services healthy\ncurl http://localhost:8080/health\n# {\"status\": \"healthy\", \"database\": \"connected\"}\n```\n\n**Related Errors**: ACGS-4301, ACGS-4305, ACGS-3701\n\n---\n\n### ACGS-4401: OPAIntegrationError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n\n**Description**: OPA integration failed (distinct from ACGS-24xx authentication/authorization errors). This is for general OPA integration issues, not policy evaluation failures.\n\n**Common Causes**:\n- OPA container not running\n- OPA API endpoint misconfigured\n- OPA version mismatch\n- Invalid OPA configuration\n- Network connectivity issues\n\n**Symptoms**:\n```\nOPA integration error\nFailed to connect to OPA service\nOPA health check failed\nOPA integration degraded\n```\n\n**Resolution**:\n\n1. **Check OPA status**:\n   ```bash\n   docker-compose ps opa\n   # Should show \"Up\" status\n   ```\n\n2. **Verify OPA health**:\n   ```bash\n   curl http://localhost:8181/health\n   # Should return: {\"status\": \"ok\"}\n   ```\n\n3. **Check OPA configuration**:\n   ```bash\n   grep OPA .env\n   # OPA_URL=http://opa:8181\n   # OPA_POLICY_PATH=/v1/data/acgs2/allow\n   ```\n\n4. **Test OPA connectivity from service**:\n   ```bash\n   docker-compose exec hitl-approvals curl -v http://opa:8181/health\n   # Should connect successfully\n   ```\n\n5. **Check OPA logs**:\n   ```bash\n   docker-compose logs opa | tail -50\n   ```\n\n6. **Restart OPA and dependent services**:\n   ```bash\n   docker-compose restart opa\n   sleep 5\n   docker-compose restart hitl-approvals enhanced-agent-bus\n   ```\n\n**Example**:\n```bash\n# OPA integration error\ndocker-compose logs hitl-approvals\n# Error: Failed to connect to OPA: connection refused\n\n# Check OPA\ndocker-compose ps opa\n# State: Exit 1\n\n# Start OPA\ndocker-compose up -d opa\n\n# Verify health\ncurl http://localhost:8181/health\n# {\"status\": \"ok\"}\n\n# Restart services\ndocker-compose restart hitl-approvals\n```\n\n**Related Errors**: ACGS-2403, ACGS-4402, ACGS-4403, ACGS-4404\n\n---\n\n### ACGS-4402: OPAQueryError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n\n**Description**: OPA query execution failed. Policy loaded but query execution error.\n\n**Common Causes**:\n- Invalid query input format\n- Policy runtime error\n- Undefined policy rule\n- Query timeout\n- OPA internal error\n\n**Symptoms**:\n```\nOPA query error\nPolicy evaluation failed\nUndefined rule: allow\nInvalid query input\nOPA returned error: evaluation error\n```\n\n**Resolution**:\n\n1. **Check query format**:\n   ```bash\n   # Test OPA query manually\n   curl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n     -H 'Content-Type: application/json' \\\n     -d '{\n       \"input\": {\n         \"user\": \"user@example.com\",\n         \"action\": \"approve\",\n         \"resource\": \"approval-123\"\n       }\n     }'\n   ```\n\n2. **Verify policy is loaded**:\n   ```bash\n   # List loaded policies\n   curl http://localhost:8181/v1/policies\n\n   # Check specific policy\n   curl http://localhost:8181/v1/policies/acgs2\n   ```\n\n3. **Test policy in OPA REPL**:\n   ```bash\n   docker-compose exec opa /opa run --bundle /bundles\n   # > data.acgs2.allow\n   ```\n\n4. **Check OPA logs for errors**:\n   ```bash\n   docker-compose logs opa | grep -i error\n   ```\n\n5. **Reload policy bundles**:\n   ```bash\n   # Trigger bundle reload\n   curl -X POST http://localhost:8181/v1/config\n\n   # Or restart OPA\n   docker-compose restart opa\n   ```\n\n**Example**:\n```bash\n# Query error\ncurl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n  -d '{\"input\": {\"user\": \"test\"}}'\n# {\"error\": \"evaluation error\"}\n\n# Check policy\ncurl http://localhost:8181/v1/policies/acgs2\n# Policy found\n\n# Test with correct input format\ncurl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n  -d '{\n    \"input\": {\n      \"user\": \"user@example.com\",\n      \"action\": \"approve\",\n      \"resource\": \"approval-123\",\n      \"constitutional_hash\": \"cdd01ef066bc6cf2\"\n    }\n  }'\n# {\"result\": true}\n```\n\n**Related Errors**: ACGS-2401, ACGS-4401, ACGS-4403\n\n---\n\n### ACGS-4403: OPATimeoutError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n\n**Description**: OPA request timeout exceeded.\n\n**Common Causes**:\n- Complex policy evaluation (too many rules)\n- Large input data\n- OPA under heavy load\n- Network latency\n- OPA resource exhaustion\n\n**Symptoms**:\n```\nOPA timeout: request exceeded 5 seconds\nPolicy evaluation timeout\nOPA not responding\nRequest cancelled: timeout\n```\n\n**Resolution**:\n\n1. **Check OPA response time**:\n   ```bash\n   time curl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n     -d '{\"input\": {...}}'\n   # Should be < 100ms for simple policies\n   ```\n\n2. **Monitor OPA resource usage**:\n   ```bash\n   docker-compose stats opa\n   # Check CPU and memory usage\n   ```\n\n3. **Simplify policy if possible**:\n   ```bash\n   # Review policy complexity\n   # Reduce nested loops, large data sets\n   # Use indexing for faster lookups\n   ```\n\n4. **Increase timeout** (if complex policies justified):\n   ```bash\n   # In service config:\n   OPA_TIMEOUT=10  # 10 seconds (default usually 5)\n   OPA_REQUEST_TIMEOUT=10000  # milliseconds\n   ```\n\n5. **Scale OPA if under load**:\n   ```bash\n   # Add more OPA replicas (docker-compose scale)\n   docker-compose up -d --scale opa=3\n\n   # Add load balancer in front\n   ```\n\n6. **Check OPA logs for slow queries**:\n   ```bash\n   docker-compose logs opa | grep -i \"slow\\|timeout\"\n   ```\n\n**Policy Optimization**:\n```rego\n# Before (slow - nested loops)\nallow {\n  some i, j\n  user := data.users[i]\n  role := data.roles[j]\n  # ... complex logic\n}\n\n# After (fast - indexed lookup)\nallow {\n  user := data.users_by_email[input.user]\n  role := user.roles[_]\n  # ... simpler logic\n}\n```\n\n**Example**:\n```bash\n# Timeout error\ntime curl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n  -d '{\"input\": {...}}'\n# Timeout after 5.0 seconds\n\n# Check OPA stats\ndocker-compose stats opa\n# CPU: 95% (overloaded!)\n\n# Restart OPA\ndocker-compose restart opa\n\n# Verify performance\ntime curl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n  -d '{\"input\": {...}}'\n# 0.050s (50ms - good!)\n```\n\n**Performance Targets**:\n- **Simple policies**: < 50ms\n- **Complex policies**: < 500ms\n- **Critical**: < 1 second\n- **Timeout threshold**: 5 seconds (default)\n\n**Related Errors**: ACGS-4401, ACGS-4402, ACGS-7101\n\n---\n\n### ACGS-4404: OPAPolicyLoadError\n\n**Severity**: CRITICAL\n**Impact**: Deployment-Blocking\n**Scenario**: Policy syntax errors (DEPLOYMENT_FAILURE_SCENARIOS.md #2.3)\n\n**Description**: OPA policy loading failed due to syntax or validation errors.\n\n**Common Causes**:\n- Rego syntax error\n- Invalid policy structure\n- Missing package declaration\n- Circular import\n- Type error in policy\n\n**Symptoms**:\n```\nCRITICAL: Failed to load OPA policy\nRego syntax error: unexpected token\nPolicy compilation failed\nPackage not found: acgs2\nInvalid policy bundle\n```\n\n**Resolution**:\n\n1. **Check OPA logs for syntax errors**:\n   ```bash\n   docker-compose logs opa | grep -i error\n   # rego_parse_error: unexpected eof token\n   # Line 45: expected ']'\n   ```\n\n2. **Validate policy syntax**:\n   ```bash\n   # Test policy file locally\n   docker-compose exec opa /opa test /policies/*.rego -v\n\n   # Or use OPA CLI\n   opa check /path/to/policy.rego\n   ```\n\n3. **Check policy bundle structure**:\n   ```bash\n   # Verify bundle contains required files\n   docker-compose exec opa ls -la /bundles/\n   # Should have: .manifest, *.rego files\n   ```\n\n4. **Fix syntax errors**:\n   ```rego\n   # Before (syntax error)\n   package acgs2\n   allow {\n     input.user == \"admin\"  # Missing closing brace\n\n   # After (fixed)\n   package acgs2\n   allow {\n     input.user == \"admin\"\n   }\n   ```\n\n5. **Reload policy**:\n   ```bash\n   # After fixing policy, reload\n   docker-compose restart opa\n\n   # Or trigger bundle update (if using bundle service)\n   curl -X POST http://localhost:8181/v1/config\n   ```\n\n6. **Verify policy loaded successfully**:\n   ```bash\n   # Check loaded policies\n   curl http://localhost:8181/v1/policies\n   # Should list acgs2 policy\n\n   # Test policy\n   curl -X POST http://localhost:8181/v1/data/acgs2/allow \\\n     -d '{\"input\": {\"user\": \"admin\"}}'\n   # Should return result\n   ```\n\n**Common Rego Syntax Errors**:\n```rego\n# Missing package\n# Error: package declaration required\npackage acgs2  # Fix: Add package\n\n# Unclosed braces\nallow {\n  input.user == \"admin\"\n# Fix: Add closing }\n\n# Invalid operator\nallow {\n  input.user = \"admin\"  # Assignment instead of comparison\n}\n# Fix: Use == for comparison\n\n# Undefined variable\nallow {\n  user.role == \"admin\"  # 'user' not defined\n}\n# Fix: Use input.user.role\n```\n\n**Example**:\n```bash\n# Policy load error\ndocker-compose logs opa\n# ERROR: rego_parse_error: unexpected '}' token at line 23\n\n# Check policy file\ncat acgs2-core/opa/policies/acgs2.rego\n# Line 23: Extra closing brace\n\n# Fix policy\nvim acgs2-core/opa/policies/acgs2.rego\n# Remove extra }\n\n# Validate\ndocker-compose exec opa /opa check /policies/acgs2.rego\n# Success\n\n# Restart OPA\ndocker-compose restart opa\n\n# Verify loaded\ncurl http://localhost:8181/v1/policies/acgs2\n# Policy loaded successfully\n```\n\n**Constitutional Hash Validation**: Ensure policy includes constitutional hash validation:\n```rego\npackage acgs2\n\nconstitutional_hash := \"cdd01ef066bc6cf2\"\n\nallow {\n  input.constitutional_hash == constitutional_hash\n  # ... other rules\n}\n```\n\n**Related Errors**: ACGS-2401, ACGS-2402, ACGS-4401, ACGS-6201\n\n---\n\n## ACGS-5xxx: Runtime Errors\n\n**Category Description**: Errors occurring during normal system operation, including business logic, workflow, and processing errors.\n\n**Common Severity**: HIGH to MEDIUM\n\n**Related Components**: Approval chains, webhooks, message processing\n\n---\n\n### ACGS-5101: ApprovalChainResolutionError\n\n**Severity**: HIGH\n**Impact**: Service-Degraded\n**Location**: `acgs2-core/services/hitl_approvals/app/api/approvals.py:34`\n\n**Description**: Cannot resolve approval chain. Currently uses static fallback logic; dynamic OPA-based resolution not implemented (TODO).\n\n**Current Behavior**:\n- Static approval chains only\n- No dynamic chain resolution via OPA\n- Fallback to default chain if specified chain not found\n\n**Common Causes**:\n- Approval chain ID not found\n- OPA integration for dynamic chains not implemented\n- Invalid chain configuration\n- Chain template not loaded\n\n**Symptoms**:\n```\nWarning: Approval chain not found, using fallback\nDynamic chain resolution not implemented\nTODO: Implement OPA-based chain resolution\n```\n\n**Resolution**:\n\n1. **Check available approval chains**:\n   ```bash\n   # List configured chains\n   curl http://localhost:8080/api/v1/approval-chains\n   ```\n\n2. **Use valid chain ID** from configuration:\n   ```json\n   {\n     \"chain_id\": \"standard-approval\",\n     \"request_type\": \"policy_change\"\n   }\n   ```\n\n3. **For dynamic chain resolution**, track TODO:\n   - See `TODO_CATALOG.md` - HIGH priority\n   - Location: `approvals.py:34`\n   - Enhancement: Implement OPA-based dynamic chain resolution\n\n**Workaround**:\n```python\n# Use predefined chain IDs:\n# - \"standard-approval\"\n# - \"high-risk-approval\"\n# - \"emergency-approval\"\n\n# Example API request:\n{\n  \"request_id\": \"req-123\",\n  \"chain_id\": \"standard-approval\",\n  \"approvers\": [\"user1@example.com\", \"user2@example.com\"]\n}\n```\n\n**TODO Reference**: See `TODO_CATALOG.md` - HIGH priority\n\n**Related Errors**: ACGS-5102, ACGS-5111, ACGS-5113\n\n---\n\n### ACGS-5201: WebhookDeliveryError\n\n**Severity**: MEDIUM\n**Impact**: Service-Degraded\n**Exception**: `WebhookDeliveryError` (integration-service)\n\n**Description**: Webhook delivery failed. Automatic retries scheduled.\n\n**Common Causes**:\n- Destination endpoint unavailable\n- Network timeout\n- HTTP 5xx error from endpoint\n- SSL/TLS certificate issues\n\n**Symptoms**:\n```\nWarning: Webhook delivery failed, scheduling retry\nHTTP 503 Service Unavailable\nConnection timeout after 30 seconds\nRetry attempt 1 of 3\n```\n\n**Resolution**:\n\n1. **Check webhook destination is reachable**:\n   ```bash\n   # Test endpoint\n   curl -I https://webhook.example.com/events\n   ```\n\n2. **Verify SSL certificate is valid**:\n   ```bash\n   curl -v https://webhook.example.com/events 2>&1 | grep -i certificate\n   ```\n\n3. **Check retry queue**:\n   ```bash\n   # View pending webhook deliveries\n   curl http://localhost:8080/api/v1/webhooks/queue\n   ```\n\n4. **Monitor retry attempts**:\n   ```bash\n   docker-compose logs integration-service | grep -i webhook\n   # Look for successful delivery or exhausted retries\n   ```\n\n5. **Manually retry failed webhook**:\n   ```bash\n   curl -X POST http://localhost:8080/api/v1/webhooks/{delivery_id}/retry\n   ```\n\n**Retry Policy**:\n- **Attempts**: 3 retries\n- **Backoff**: Exponential (1s, 2s, 4s)\n- **Total time**: ~7 seconds\n- **429 Rate Limit**: Respects `Retry-After` header\n\n**Example**:\n```bash\n# Check failed webhooks\ncurl http://localhost:8080/api/v1/webhooks/failed | jq\n\n# Retry specific delivery\ndelivery_id=\"wh_abc123\"\ncurl -X POST http://localhost:8080/api/v1/webhooks/$delivery_id/retry\n\n# Monitor logs\ndocker-compose logs -f integration-service | grep $delivery_id\n```\n\n**Related Errors**: ACGS-5202, ACGS-5203, ACGS-5204, ACGS-5211\n\n---\n\n## ACGS-6xxx: Constitutional/Governance Errors\n\n**Category Description**: Errors related to constitutional governance, MACI role separation, deliberation, and alignment validation.\n\n**Common Severity**: CRITICAL (constitutional violations are security-critical)\n\n**Related Components**: Constitutional validation, MACI, deliberation layer\n\n---\n\n### ACGS-6201: MACIRoleViolationError\n\n**Severity**: CRITICAL (Security)\n**Impact**: Security-Violation\n**Exception**: `MACIRoleViolationError` (enhanced-agent-bus)\n\n**Description**: Agent attempted action outside its MACI role. This prevents agents from operating in multiple roles simultaneously (Monitor + Auditor + Implementer separation).\n\n**Purpose**: Prevent unauthorized cross-role operations and ensure proper separation of concerns.\n\n**Common Causes**:\n- Agent assigned multiple conflicting roles\n- Agent attempting action reserved for different role\n- Role configuration error\n- Exploit attempt (multi-role bypass)\n\n**Symptoms**:\n```\nCRITICAL: MACI role violation detected\nAgent: agent-123\nRole: Monitor\nAttempted action: implement_policy (requires Implementer role)\nRequest denied\n```\n\n**Resolution**:\n\n1. **Review agent role assignment**:\n   ```bash\n   # Check agent's assigned role\n   curl http://localhost:8000/api/v1/agents/agent-123/role\n   ```\n\n2. **Verify action is appropriate for role**:\n   ```\n   Monitor role: Can observe, cannot modify\n   Auditor role: Can review, cannot implement\n   Implementer role: Can implement, should not self-audit\n   ```\n\n3. **Correct role assignment if error**:\n   ```bash\n   # Reassign to correct role\n   curl -X PUT http://localhost:8000/api/v1/agents/agent-123/role \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"role\": \"Implementer\"}'\n   ```\n\n4. **If security incident**, audit:\n   ```bash\n   # Check audit logs for pattern of violations\n   grep \"MACIRoleViolationError\" logs/audit.log\n\n   # Review agent's recent actions\n   curl http://localhost:8000/api/v1/audit/agent/agent-123\n   ```\n\n**\u26a0\ufe0f Security Note**: This error indicates either:\n- Configuration mistake (benign)\n- Attempted security bypass (malicious)\n\nAlways investigate CRITICAL severity violations.\n\n**Example**:\n```bash\n# Agent \"monitor-1\" (Monitor role) tries to implement policy\nERROR: MACIRoleViolationError\nAgent: monitor-1\nCurrent role: Monitor\nAttempted action: implement_policy\nAllowed roles for action: [Implementer]\n\n# Fix: Use correct agent for implementation\nagent_id=\"implementer-1\"  # Agent with Implementer role\ncurl -X POST http://localhost:8000/api/v1/policies/implement \\\n  -H \"X-Agent-ID: $agent_id\" \\\n  -d '{\"policy_id\": \"policy-123\"}'\n```\n\n**Related Errors**: ACGS-6202, ACGS-6203, ACGS-6204\n\n---\n\n### ACGS-6202: MACISelfValidationError\n\n**Severity**: CRITICAL (Security)\n**Impact**: Security-Violation (G\u00f6del Bypass Prevention)\n**Exception**: `MACISelfValidationError` (enhanced-agent-bus)\n\n**Description**: Agent attempted to validate its own output. This violates the G\u00f6del incompleteness principle - a system cannot prove its own consistency.\n\n**Prevention Type**: `godel_bypass`\n\n**Purpose**: Prevent self-certification loophole that would allow agents to approve their own work without external validation.\n\n**Common Causes**:\n- Agent attempting to audit its own implementation\n- Configuration error assigning agent as its own reviewer\n- Exploit attempt (self-approval bypass)\n\n**Symptoms**:\n```\nCRITICAL: MACI self-validation attempt detected (G\u00f6del bypass)\nAgent: agent-123\nAttempted to validate own output: output-456\nPrevention type: godel_bypass\nRequest denied\n```\n\n**Resolution**:\n\n1. **Verify separate agents for implementation and validation**:\n   ```\n   Rule: Agent A implements \u2192 Agent B validates\n   Never: Agent A implements \u2192 Agent A validates\n   ```\n\n2. **Check approval chain configuration**:\n   ```bash\n   # Ensure different agents at each step\n   curl http://localhost:8080/api/v1/approval-chains/{chain_id}\n   ```\n\n3. **Correct agent assignment**:\n   ```json\n   {\n     \"implementation\": {\n       \"agent_id\": \"implementer-1\"\n     },\n     \"validation\": {\n       \"agent_id\": \"auditor-1\"  // Different agent\n     }\n   }\n   ```\n\n4. **If security incident**, investigate:\n   ```bash\n   # Check for pattern of self-validation attempts\n   grep \"MACISelfValidationError\" logs/security.log\n\n   # Review constitutional compliance\n   curl http://localhost:8000/api/v1/constitutional/compliance\n   ```\n\n**\u26a0\ufe0f Critical Security Principle**:\nThis is a fundamental safety mechanism. Allowing self-validation would:\n- Violate G\u00f6del incompleteness theorem\n- Create certification loophole\n- Compromise constitutional governance\n- Enable unchecked agent autonomy\n\n**Never bypass this check.**\n\n**Example**:\n```bash\n# Wrong: Same agent implementing and validating\n{\n  \"implementer_agent\": \"agent-1\",\n  \"validator_agent\": \"agent-1\"  \u2190 Same agent! Error!\n}\n\n# Right: Different agents\n{\n  \"implementer_agent\": \"implementer-1\",\n  \"validator_agent\": \"auditor-1\"  \u2190 Different agent \u2713\n}\n```\n\n**Related Errors**: ACGS-6201, ACGS-6203, ACGS-6401\n\n---\n\n## ACGS-7xxx: Performance/Resource Errors\n\n**Category Description**: Errors related to performance degradation, resource exhaustion, and throughput issues.\n\n**Common Severity**: HIGH (production impact) to LOW (informational)\n\n**Related Metrics**: Latency, throughput, CPU, memory, connections\n\n---\n\n### ACGS-7101: LatencyThresholdExceededError\n\n**Severity**: MEDIUM\n**Impact**: Performance-Degradation\n\n**Description**: P99 latency exceeds threshold (default: 5ms). System still functional but slower than target.\n\n**Threshold**: 5.0ms (configurable)\n\n**Common Causes**:\n- OPA policy evaluation slow\n- Database query performance degradation\n- Redis cache misses\n- Network latency\n- Resource contention\n\n**Symptoms**:\n```\nWarning: P99 latency exceeded threshold\nCurrent: 12.5ms\nThreshold: 5.0ms\nEndpoint: /api/v1/approvals/create\n```\n\n**Resolution**:\n\n1. **Check current latency metrics**:\n   ```bash\n   curl http://localhost:8080/metrics | grep latency\n   ```\n\n2. **Identify slow endpoints**:\n   ```bash\n   # Check Prometheus/Grafana dashboard\n   # Or query metrics API\n   curl http://localhost:8080/api/v1/metrics/latency?p99\n   ```\n\n3. **Check OPA performance**:\n   ```bash\n   # OPA decision latency\n   curl http://localhost:8181/metrics | grep decision_latency\n   ```\n\n4. **Review database query performance**:\n   ```sql\n   -- Check slow queries\n   SELECT query, mean_exec_time, calls\n   FROM pg_stat_statements\n   ORDER BY mean_exec_time DESC\n   LIMIT 10;\n   ```\n\n5. **Check Redis cache hit rate**:\n   ```bash\n   docker-compose exec redis redis-cli info stats | grep cache_hit_rate\n   # Target: >90% hit rate\n   ```\n\n6. **Monitor resource usage**:\n   ```bash\n   docker stats\n   # Check CPU/Memory usage\n   ```\n\n**Optimization Steps**:\n\n1. **Enable OPA decision caching**\n2. **Add database indexes** for frequent queries\n3. **Increase Redis cache TTL** for stable data\n4. **Review N+1 query patterns**\n5. **Enable query result caching**\n\n**Example**:\n```bash\n# Check latency trend\ncurl http://localhost:8080/api/v1/metrics/latency/trend?duration=1h\n\n# Identify hot paths\ncurl http://localhost:8080/api/v1/metrics/slow-requests\n\n# Response:\n{\n  \"slow_requests\": [\n    {\n      \"endpoint\": \"/api/v1/approvals/create\",\n      \"p99_latency_ms\": 12.5,\n      \"threshold_ms\": 5.0,\n      \"cause\": \"OPA policy evaluation\"\n    }\n  ]\n}\n\n# Fix: Enable OPA caching\n# In OPA config:\ndecision_logs:\n  cache:\n    max_size_bytes: 104857600  # 100MB\n```\n\n**Related Errors**: ACGS-7102, ACGS-7103, ACGS-7201\n\n---\n\n## ACGS-8xxx: Platform-Specific Errors\n\n**Category Description**: Errors specific to operating systems and platform configurations.\n\n**Common Severity**: LOW to MEDIUM (usually have workarounds)\n\n**Related Platforms**: Windows, macOS, Linux\n\n---\n\n### ACGS-8101: WindowsLineEndingError\n\n**Severity**: LOW\n**Impact**: Development-Issue\n\n**Description**: Windows line ending (CRLF) breaking shell scripts. Common when files created on Windows.\n\n**Common Causes**:\n- Git autocrlf=true setting\n- Files created/edited in Windows editors\n- Copy-paste from Windows to WSL\n- Windows text editor default settings\n\n**Symptoms**:\n```\nbash: ./script.sh: /bin/bash^M: bad interpreter\nSyntax error: unexpected end of file\n```\n\n**Resolution**:\n\n1. **Configure Git to use LF**:\n   ```bash\n   # Global setting\n   git config --global core.autocrlf input\n\n   # Repository setting\n   git config core.autocrlf input\n   ```\n\n2. **Convert existing files**:\n   ```bash\n   # Using dos2unix\n   dos2unix script.sh\n\n   # Using sed\n   sed -i 's/\\r$//' script.sh\n\n   # Using tr\n   tr -d '\\r' < script.sh > script_fixed.sh\n   ```\n\n3. **Fix all shell scripts**:\n   ```bash\n   find . -name \"*.sh\" -exec dos2unix {} \\;\n   ```\n\n4. **Add .gitattributes** to enforce LF:\n   ```\n   # .gitattributes\n   * text=auto\n   *.sh text eol=lf\n   *.py text eol=lf\n   *.md text eol=lf\n   ```\n\n5. **Check file line endings**:\n   ```bash\n   file script.sh\n   # Should show: \"ASCII text\" not \"ASCII text, with CRLF line terminators\"\n   ```\n\n**Prevention**:\n```bash\n# Add to project .gitattributes\ncat > .gitattributes <<EOF\n# Ensure LF for shell scripts\n*.sh text eol=lf\n*.bash text eol=lf\n\n# Ensure LF for Python\n*.py text eol=lf\n\n# Ensure LF for config files\n*.yml text eol=lf\n*.yaml text eol=lf\n*.json text eol=lf\nEOF\n\ngit add .gitattributes\ngit commit -m \"Enforce LF line endings\"\n```\n\n**Example**:\n```bash\n# Error when running script\n./deploy.sh\n# bash: ./deploy.sh: /bin/bash^M: bad interpreter\n\n# Fix:\ndos2unix deploy.sh\n# dos2unix: converting file deploy.sh to Unix format...\n\n# Or using sed:\nsed -i 's/\\r$//' deploy.sh\n\n# Verify:\nfile deploy.sh\n# deploy.sh: Bourne-Again shell script, ASCII text executable\n\n# Run successfully:\n./deploy.sh\n```\n\n**Frequency**: Common on Windows/WSL2 development\n\n**Related Errors**: ACGS-8102, ACGS-8103\n\n---\n\n### ACGS-8201: MacOSPortConflictError\n\n**Severity**: MEDIUM\n**Impact**: Deployment-Blocking\n\n**Description**: macOS-specific port conflict, typically port 8000 with Airplay Receiver.\n\n**Common Port**: 8000 (Airplay Receiver)\n\n**Symptoms**:\n```\nError: Port 8000 is already in use\nCannot bind to 0.0.0.0:8000\nControlCenter using port 8000 (Airplay)\n```\n\n**Resolution**:\n\n**Solution 1: Disable Airplay Receiver** (Recommended)\n1. Open **System Preferences**\n2. Click **Sharing**\n3. Uncheck **AirPlay Receiver**\n4. Restart Docker Compose:\n   ```bash\n   docker-compose restart enhanced-agent-bus\n   ```\n\n**Solution 2: Change Port Mapping**\n1. Edit `docker-compose.yml`:\n   ```yaml\n   services:\n     enhanced-agent-bus:\n       ports:\n         - \"8001:8000\"  # Changed from 8000:8000\n   ```\n\n2. Update client configuration:\n   ```bash\n   # Access on new port\n   curl http://localhost:8001/health\n   ```\n\n3. Update environment variables if needed:\n   ```bash\n   AGENT_BUS_URL=http://localhost:8001\n   ```\n\n**Verification**:\n```bash\n# Check port is free\nlsof -i :8000\n# Should return nothing\n\n# Start service\ndocker-compose up -d enhanced-agent-bus\n\n# Verify service running\ncurl http://localhost:8000/health\n# or\ncurl http://localhost:8001/health  # if port changed\n```\n\n**Example**:\n```bash\n# Identify Airplay using port\nlsof -i :8000\n# COMMAND     PID USER\n# ControlCe  1234 user\n\n# Disable Airplay:\n# System Preferences \u2192 Sharing \u2192 Uncheck \"AirPlay Receiver\"\n\n# Restart service\ndocker-compose restart enhanced-agent-bus\n\n# Verify\ncurl http://localhost:8000/health\n# {\"status\": \"healthy\"}\n```\n\n**Frequency**: Very common on macOS (Monterey and later)\n\n**Related Errors**: ACGS-3301, ACGS-8202\n\n---\n\n### ACGS-8301: LinuxPermissionError\n\n**Severity**: MEDIUM\n**Impact**: Deployment-Blocking\n\n**Description**: Linux file permissions issue, commonly Docker socket or volume mount permissions.\n\n**Common Causes**:\n- User not in docker group\n- File ownership issues on volume mounts\n- SELinux blocking operations\n- Incorrect file permissions (0644 instead of 0755 for executables)\n\n**Symptoms**:\n```\nPermission denied: /var/run/docker.sock\nCannot access volume mount: Permission denied\nGot permission denied while trying to connect to Docker daemon\n```\n\n**Resolution**:\n\n1. **Add user to docker group**:\n   ```bash\n   # Add current user\n   sudo usermod -aG docker $USER\n\n   # Log out and log back in, then verify:\n   groups\n   # Should include \"docker\"\n   ```\n\n2. **Fix Docker socket permissions**:\n   ```bash\n   # Check socket permissions\n   ls -l /var/run/docker.sock\n\n   # Should be: srw-rw---- 1 root docker\n\n   # Fix if needed:\n   sudo chmod 660 /var/run/docker.sock\n   sudo chown root:docker /var/run/docker.sock\n   ```\n\n3. **Fix volume mount permissions**:\n   ```bash\n   # Check ownership\n   ls -ld ./data\n\n   # Fix ownership to match container user\n   sudo chown -R 1000:1000 ./data\n\n   # Or match current user:\n   sudo chown -R $USER:$USER ./data\n   ```\n\n4. **For SELinux issues**, add :Z flag:\n   ```yaml\n   volumes:\n     - ./data:/app/data:Z  # Z for SELinux relabeling\n   ```\n\n5. **Fix executable permissions**:\n   ```bash\n   # Make script executable\n   chmod +x script.sh\n\n   # Fix all shell scripts:\n   find . -name \"*.sh\" -exec chmod +x {} \\;\n   ```\n\n**SELinux Specific**:\n```bash\n# Check SELinux status\ngetenforce\n# Enforcing / Permissive / Disabled\n\n# Check audit logs\nsudo ausearch -m AVC -ts recent\n\n# Allow Docker volume mounts (permanent):\nsudo semanage fcontext -a -t container_file_t \"/path/to/data(/.*)?\"\nsudo restorecon -Rv /path/to/data\n\n# Or use :Z in docker-compose:\nvolumes:\n  - ./data:/app/data:Z\n```\n\n**Example**:\n```bash\n# Permission denied error\ndocker-compose up\n# ERROR: Cannot connect to Docker daemon\n\n# Check user groups\ngroups\n# user : user\n\n# Fix: Add to docker group\nsudo usermod -aG docker $USER\n\n# Important: Log out and log back in!\n# Or use newgrp:\nnewgrp docker\n\n# Verify\ndocker ps\n# Should work without sudo\n\n# For volume mounts:\nsudo chown -R $USER:$USER ./data\nchmod -R 755 ./data\n\n# Restart\ndocker-compose up -d\n```\n\n**Frequency**: Very common on first-time Linux setup\n\n**Related Errors**: ACGS-8302, ACGS-3101\n\n---\n\n## Related Documentation\n\n### Core Documentation\n\n- **ERROR_CODE_TAXONOMY.md**: Complete error code taxonomy and structure\n- **ERROR_CODE_MAPPING.md**: Mapping of all exceptions to error codes\n- **ERROR_SEVERITY_CLASSIFICATION.md**: Severity levels and operational response procedures\n- **EXCEPTION_CATALOG.md**: Catalog of all 137 exception classes\n- **DEPLOYMENT_FAILURE_SCENARIOS.md**: Common deployment failure scenarios\n- **TODO_CATALOG.md**: TODO/FIXME comments and error impacts\n- **GAP_ANALYSIS.md**: Documentation coverage gaps\n\n### Operational Guides\n\n- **DEPLOYMENT_GUIDE.md**: Deployment procedures and configuration\n- **TROUBLESHOOTING.md**: General troubleshooting guide (quickstart)\n- **Diagnostic Runbooks**: Service-specific diagnostic procedures (Phase 4)\n\n### Service-Specific Documentation\n\n- **TROUBLESHOOTING_HITL_APPROVALS.md**: HITL Approvals service (Phase 4.2)\n- **TROUBLESHOOTING_INTEGRATION_SERVICE.md**: Integration service (Phase 4.2)\n- **TROUBLESHOOTING_AUDIT_SERVICE.md**: Audit service (Phase 4.2)\n\n---\n\n## Getting Help\n\n### Diagnostic Commands\n\n**Quick Health Check**:\n```bash\n# All services\ndocker-compose ps\n\n# Service logs\ndocker-compose logs <service-name> | tail -50\n\n# Health endpoints\ncurl http://localhost:8080/health  # API Gateway\ncurl http://localhost:8000/health  # Agent Bus\ncurl http://localhost:8181/health  # OPA\n```\n\n**System Status**:\n```bash\n# Docker status\ndocker info\ndocker-compose ps\n\n# Resource usage\ndocker stats\n\n# Network connectivity\ndocker-compose exec <service> ping <target-service>\n```\n\n**Configuration Check**:\n```bash\n# Environment variables\ndocker-compose config\n\n# Verify critical vars\ngrep -E \"CONSTITUTIONAL_HASH|OPA_URL|DATABASE_URL\" .env\n\n# Constitutional hash verification\necho $CONSTITUTIONAL_HASH\n# Should be: cdd01ef066bc6cf2\n```\n\n### Escalation Path\n\n1. **CRITICAL errors**: Immediate page on-call engineer\n2. **HIGH errors**: Alert engineering team (< 1 hour)\n3. **MEDIUM errors**: Create ticket (< 4 hours business hours)\n4. **LOW errors**: Add to backlog (best effort)\n\n### Support Channels\n\n- **Internal**: Check with team, review documentation\n- **Logs**: Always include relevant logs when reporting issues\n- **Context**: Include error code, severity, and steps to reproduce\n\n### Post-Incident\n\nFor CRITICAL and HIGH severity incidents:\n1. Document resolution in this guide\n2. Update troubleshooting procedures\n3. Create follow-up tasks for prevention\n4. Conduct blameless post-mortem (CRITICAL only)\n\n---\n\n## Statistics\n\n**Error Code Coverage**:\n- Total error codes documented: 250+\n- Exception classes covered: 137 (100%)\n- Deployment scenarios covered: 50+ (100%)\n- TODO-related errors: 10 (100%)\n\n**Severity Distribution**:\n- CRITICAL: ~45 codes (18%)\n- HIGH: ~95 codes (38%)\n- MEDIUM: ~90 codes (36%)\n- LOW: ~20 codes (8%)\n\n**Category Distribution**:\n- ACGS-1xxx (Configuration): 24 codes\n- ACGS-2xxx (Auth/Authz): 53 codes\n- ACGS-3xxx (Deployment): 24 codes\n- ACGS-4xxx (Integration): 29 codes\n- ACGS-5xxx (Runtime): 67 codes\n- ACGS-6xxx (Constitutional): 22 codes\n- ACGS-7xxx (Performance): 21 codes\n- ACGS-8xxx (Platform): 10 codes\n\n---\n\n## Version History\n\n| Version | Date | Changes |\n|---------|------|---------|\n| 1.0.0 | 2026-01-03 | Initial comprehensive error code reference |\n\n---\n\n**Constitutional Hash**: cdd01ef066bc6cf2\n**Document Status**: \u2705 Complete\n**Next Steps**: Implement detailed category-specific documentation (Phase 3.2-3.6)\n",
        "timestamp": "2026-01-04T05:35:51.134105"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "060-document-error-codes-and-troubleshooting-for-commo",
        "description": "The codebase has 13 TODO/FIXME comments across critical files including webhooks.py, approval_chain_engine.py, and config_validator.py. Additionally, there's no centralized documentation for error codes, failure modes, or troubleshooting guides. Users encountering errors have no reference for resolution.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:08.622953",
  "last_updated": "2026-01-04T05:35:51.239601"
}