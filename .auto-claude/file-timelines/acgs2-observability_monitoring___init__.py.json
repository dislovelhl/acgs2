{
  "file_path": "acgs2-observability/monitoring/__init__.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Production Monitoring Module\nConstitutional Hash: cdd01ef066bc6cf2\n\nReal-time production monitoring with psutil integration, Redis metrics, and PagerDuty alerting.\n\"\"\"\n\nimport asyncio\nimport logging\nimport uuid\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\n\nfrom .alerting import Alert, AlertSeverity, AlertStatus\n\n# Import centralized Redis config with fallback\ntry:\n    from shared.redis_config import get_redis_url\n\n    DEFAULT_REDIS_URL = get_redis_url()\nexcept ImportError:\n    DEFAULT_REDIS_URL = \"redis://localhost:6379\"\n\n# Constitutional compliance hash\nCONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass SystemMetrics:\n    \"\"\"System metrics data class.\"\"\"\n\n    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n    cpu_percent: float = 0.0\n    memory_percent: float = 0.0\n    disk_percent: float = 0.0\n    network_bytes_sent: int = 0\n    network_bytes_recv: int = 0\n    process_count: int = 0\n\n\n@dataclass\nclass RedisMetrics:\n    \"\"\"Redis metrics data class.\"\"\"\n\n    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n    connected_clients: int = 0\n    used_memory_mb: float = 0.0\n    used_memory_peak: int = 0\n    total_connections_received: int = 0\n    total_commands_processed: int = 0\n    keyspace_hits: int = 0\n    keyspace_misses: int = 0\n    hit_rate_percent: float = 0.0\n    total_keys: int = 0\n\n\nclass SystemMetricsCollector:\n    \"\"\"Collects system metrics using psutil.\"\"\"\n\n    def __init__(self):\n        self.constitutional_hash = CONSTITUTIONAL_HASH\n        self._psutil = None\n        try:\n            import psutil\n\n            self._psutil = psutil\n        except ImportError:\n            logger.warning(\"psutil not available, using mock metrics\")\n\n    async def collect_metrics(self) -> SystemMetrics:\n        \"\"\"Collect comprehensive system metrics.\"\"\"\n        if self._psutil:\n            # Defensive: net_io_counters() can return None in some environments (containers, etc.)\n            net_counters = self._psutil.net_io_counters()\n            network_bytes_sent = net_counters.bytes_sent if net_counters else 0\n            network_bytes_recv = net_counters.bytes_recv if net_counters else 0\n\n            return SystemMetrics(\n                cpu_percent=self._psutil.cpu_percent(interval=0.1),\n                memory_percent=self._psutil.virtual_memory().percent,\n                disk_percent=self._psutil.disk_usage(\"/\").percent,\n                network_bytes_sent=network_bytes_sent,\n                network_bytes_recv=network_bytes_recv,\n                process_count=len(self._psutil.pids()),\n            )\n        return SystemMetrics()\n\n    async def _collect_cpu_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect CPU-specific metrics.\"\"\"\n        if self._psutil:\n            return {\n                \"cpu_percent\": self._psutil.cpu_percent(interval=0.1),\n                \"cpu_count\": self._psutil.cpu_count(),\n                \"cpu_freq\": (\n                    getattr(self._psutil.cpu_freq(), \"current\", 0) if self._psutil.cpu_freq() else 0\n                ),\n            }\n        return {\"cpu_percent\": 0.0, \"cpu_count\": 1, \"cpu_freq\": 0}\n\n    async def _collect_memory_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect memory-specific metrics.\"\"\"\n        if self._psutil:\n            mem = self._psutil.virtual_memory()\n            return {\n                \"memory_total_gb\": mem.total / (1024**3),\n                \"memory_used_gb\": mem.used / (1024**3),\n                \"memory_percent\": mem.percent,\n                \"memory_available_gb\": mem.available / (1024**3),\n            }\n        return {\n            \"memory_total_gb\": 0,\n            \"memory_used_gb\": 0,\n            \"memory_percent\": 0.0,\n            \"memory_available_gb\": 0,\n        }\n\n    async def _collect_disk_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect disk-specific metrics.\"\"\"\n        if self._psutil:\n            disk = self._psutil.disk_usage(\"/\")\n            return {\n                \"disk_total_gb\": disk.total / (1024**3),\n                \"disk_used_gb\": disk.used / (1024**3),\n                \"disk_percent\": disk.percent,\n                \"disk_free_gb\": disk.free / (1024**3),\n            }\n        return {\"disk_total_gb\": 0, \"disk_used_gb\": 0, \"disk_percent\": 0.0, \"disk_free_gb\": 0}\n\n    async def _collect_network_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect network-specific metrics.\"\"\"\n        if self._psutil:\n            # Defensive: net_io_counters() can return None in some environments\n            net = self._psutil.net_io_counters()\n            if net is not None:\n                return {\n                    \"network_bytes_sent_mb\": net.bytes_sent / (1024**2),\n                    \"network_bytes_recv_mb\": net.bytes_recv / (1024**2),\n                    \"network_packets_sent\": net.packets_sent,\n                    \"network_packets_recv\": net.packets_recv,\n                }\n        return {\n            \"network_bytes_sent_mb\": 0,\n            \"network_bytes_recv_mb\": 0,\n            \"network_packets_sent\": 0,\n            \"network_packets_recv\": 0,\n        }\n\n    async def _collect_process_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect process-specific metrics.\"\"\"\n        if self._psutil:\n            pids = self._psutil.pids()\n            thread_count = 0\n            try:\n                for pid in pids[:100]:  # Sample first 100 to avoid performance issues\n                    try:\n                        p = self._psutil.Process(pid)\n                        thread_count += p.num_threads()\n                    except (self._psutil.NoSuchProcess, self._psutil.AccessDenied):\n                        pass\n            except Exception:\n                pass\n            return {\n                \"process_count\": len(pids),\n                \"thread_count\": thread_count if thread_count > 0 else len(pids),\n            }\n        return {\"process_count\": 1, \"thread_count\": 1}\n\n\nclass RedisMetricsCollector:\n    \"\"\"Collects Redis metrics.\"\"\"\n\n    def __init__(self, redis_url: str = DEFAULT_REDIS_URL):\n        self.constitutional_hash = CONSTITUTIONAL_HASH\n        self.redis_url = redis_url\n        self._redis_client = None\n        self._connected = False\n\n    async def connect(self) -> bool:\n        \"\"\"Connect to Redis.\"\"\"\n        try:\n            import redis.asyncio as aioredis\n\n            self._redis_client = await aioredis.from_url(self.redis_url)\n            self._connected = True\n            return True\n        except Exception as e:\n            logger.warning(f\"Failed to connect to Redis: {e}\")\n            return False\n\n    async def disconnect(self) -> None:\n        \"\"\"Disconnect from Redis.\"\"\"\n        if self._redis_client:\n            await self._redis_client.close()\n            self._redis_client = None\n            self._connected = False\n\n    async def collect_metrics(self) -> RedisMetrics:\n        \"\"\"Collect Redis metrics.\"\"\"\n        try:\n            if self._redis_client:\n                info = await self._redis_client.info()\n                hits = info.get(\"keyspace_hits\", 0)\n                misses = info.get(\"keyspace_misses\", 0)\n                total = hits + misses\n\n                return RedisMetrics(\n                    connected_clients=info.get(\"connected_clients\", 0),\n                    used_memory_mb=info.get(\"used_memory\", 0) / (1024**2),\n                    used_memory_peak=info.get(\"used_memory_peak\", 0),\n                    total_connections_received=info.get(\"total_connections_received\", 0),\n                    total_commands_processed=info.get(\"total_commands_processed\", 0),\n                    keyspace_hits=hits,\n                    keyspace_misses=misses,\n                    hit_rate_percent=(hits / total * 100) if total > 0 else 0.0,\n                    total_keys=(\n                        info.get(\"db0\", {}).get(\"keys\", 0)\n                        if isinstance(info.get(\"db0\"), dict)\n                        else 0\n                    ),\n                )\n        except Exception as e:\n            logger.warning(f\"Failed to collect Redis metrics: {e}\")\n        return RedisMetrics()\n\n\nclass PagerDutyAlerting:\n    \"\"\"PagerDuty integration for alerting.\"\"\"\n\n    def __init__(self, integration_key: str = \"\"):\n        self.constitutional_hash = CONSTITUTIONAL_HASH\n        self.integration_key = integration_key\n        self.enabled = bool(integration_key)\n\n    async def send_alert(\n        self,\n        severity: AlertSeverity,\n        title: str,\n        description: str,\n        source: str,\n    ) -> bool:\n        \"\"\"Send alert to PagerDuty.\"\"\"\n        if not self.enabled:\n            logger.debug(\"PagerDuty alerting disabled (no integration key)\")\n            return False\n\n        # Map severity to PagerDuty format\n        pd_severity = self._map_severity(severity)\n\n        # In production, would send to PagerDuty API\n        logger.info(f\"PagerDuty alert [{pd_severity}]: {title} - {description}\")\n        return True\n\n    def _map_severity(self, severity: AlertSeverity) -> str:\n        \"\"\"Map AlertSeverity to PagerDuty severity string.\"\"\"\n        mapping = {\n            AlertSeverity.INFO: \"info\",\n            AlertSeverity.WARNING: \"warning\",\n            AlertSeverity.ERROR: \"error\",\n            AlertSeverity.CRITICAL: \"critical\",\n        }\n        return mapping.get(severity, \"info\")\n\n\nclass AlertManager:\n    \"\"\"Manages alerts with deduplication and rate limiting.\"\"\"\n\n    def __init__(\n        self,\n        pagerduty_alerting: Optional[PagerDutyAlerting] = None,\n        dedup_window_minutes: int = 5,\n        max_alerts_per_minute: int = 10,\n    ):\n        self.constitutional_hash = CONSTITUTIONAL_HASH\n        self.pagerduty = pagerduty_alerting\n        self.dedup_window_minutes = dedup_window_minutes\n        self.max_alerts_per_minute = max_alerts_per_minute\n        self._active_alerts: Dict[str, Alert] = {}\n        self._alert_history: List[Alert] = []\n        self._rate_limit_window: List[datetime] = []\n        self._dedup_cache: Dict[str, datetime] = {}\n\n    @property\n    def active_alerts(self) -> Dict[str, Alert]:\n        \"\"\"Get active alerts.\"\"\"\n        return {k: v for k, v in self._active_alerts.items() if v.status == AlertStatus.TRIGGERED}\n\n    @property\n    def alert_history(self) -> List[Alert]:\n        \"\"\"Get alert history.\"\"\"\n        return self._alert_history\n\n    async def trigger_alert(\n        self,\n        severity: AlertSeverity,\n        title: str,\n        description: str,\n        source: str,\n    ) -> Optional[Alert]:\n        \"\"\"Trigger an alert with deduplication and rate limiting.\"\"\"\n        # Check deduplication\n        dedup_key = f\"{severity.value}:{source}:{title}\"\n        now = datetime.now(timezone.utc)\n\n        if dedup_key in self._dedup_cache:\n            last_alert_time = self._dedup_cache[dedup_key]\n            if (now - last_alert_time).total_seconds() < (self.dedup_window_minutes * 60):\n                return None  # Deduplicated\n\n        # Check rate limiting\n        self._rate_limit_window = [\n            t for t in self._rate_limit_window if (now - t).total_seconds() < 60\n        ]\n        if len(self._rate_limit_window) >= self.max_alerts_per_minute:\n            return None  # Rate limited\n\n        # Create alert\n        alert = Alert(\n            alert_id=str(uuid.uuid4()),\n            severity=severity,\n            title=title,\n            description=description,\n            source=source,\n            timestamp=now,\n            status=AlertStatus.TRIGGERED,\n        )\n\n        # Store alert\n        self._active_alerts[alert.alert_id] = alert\n        self._alert_history.append(alert)\n        self._dedup_cache[dedup_key] = now\n        self._rate_limit_window.append(now)\n\n        # Send to PagerDuty if configured\n        if self.pagerduty:\n            await self.pagerduty.send_alert(\n                severity=severity,\n                title=title,\n                description=description,\n                source=source,\n            )\n\n        return alert\n\n    async def resolve_alert(self, alert_id: str) -> bool:\n        \"\"\"Resolve an alert.\"\"\"\n        if alert_id in self._active_alerts:\n            alert = self._active_alerts[alert_id]\n            alert.resolve()\n            del self._active_alerts[alert_id]\n            return True\n        return False\n\n    async def acknowledge_alert(self, alert_id: str) -> bool:\n        \"\"\"Acknowledge an alert.\"\"\"\n        if alert_id in self._active_alerts:\n            self._active_alerts[alert_id].acknowledge()\n            return True\n        return False\n\n    def get_active_alerts(self, severity: Optional[AlertSeverity] = None) -> List[Alert]:\n        \"\"\"Get active alerts, optionally filtered by severity.\"\"\"\n        alerts = [a for a in self._active_alerts.values() if a.status == AlertStatus.TRIGGERED]\n        if severity:\n            alerts = [a for a in alerts if a.severity == severity]\n        return alerts\n\n    def get_alert_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get alert statistics.\"\"\"\n        alerts_by_severity = {}\n        for alert in self._alert_history:\n            sev = alert.severity.value\n            alerts_by_severity[sev] = alerts_by_severity.get(sev, 0) + 1\n\n        return {\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            \"total_alerts\": len(self._alert_history),\n            \"active_alerts\": len(self.active_alerts),\n            \"alerts_by_severity\": alerts_by_severity,\n        }\n\n\nclass ProductionMonitor:\n    \"\"\"Production monitoring orchestrator.\"\"\"\n\n    def __init__(\n        self,\n        system_collector: Optional[SystemMetricsCollector] = None,\n        redis_collector: Optional[RedisMetricsCollector] = None,\n        alert_manager: Optional[AlertManager] = None,\n        pagerduty_alerting: Optional[PagerDutyAlerting] = None,\n        monitoring_interval_seconds: int = 60,\n    ):\n        self.constitutional_hash = CONSTITUTIONAL_HASH\n        self.system_collector = system_collector or SystemMetricsCollector()\n        self.redis_collector = redis_collector or RedisMetricsCollector()\n        self.alert_manager = alert_manager or AlertManager(pagerduty_alerting=pagerduty_alerting)\n        self.monitoring_interval_seconds = monitoring_interval_seconds\n        self._running = False\n        self._monitoring_task: Optional[asyncio.Task] = None\n        self.metrics_history: List[Dict[str, Any]] = []\n        self._thresholds = {\n            \"cpu_warning\": 80,\n            \"cpu_critical\": 85,\n            \"memory_warning\": 80,\n            \"memory_critical\": 90,\n            \"disk_warning\": 80,\n            \"disk_critical\": 90,\n            \"redis_hit_rate_constitutional\": 85,  # Constitutional requirement\n        }\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Check if monitoring is running.\"\"\"\n        return self._running\n\n    async def start(self) -> None:\n        \"\"\"Start monitoring.\"\"\"\n        self._running = True\n        self._monitoring_task = asyncio.create_task(self._monitoring_loop())\n        logger.info(\"Production monitoring started\")\n\n    async def stop(self) -> None:\n        \"\"\"Stop monitoring.\"\"\"\n        self._running = False\n        if self._monitoring_task:\n            self._monitoring_task.cancel()\n            try:\n                await self._monitoring_task\n            except asyncio.CancelledError:\n                pass\n        logger.info(\"Production monitoring stopped\")\n\n    async def _monitoring_loop(self) -> None:\n        \"\"\"Main monitoring loop.\"\"\"\n        while self._running:\n            try:\n                metrics = await self._collect_metrics()\n                self.metrics_history.append(metrics)\n                # Keep only last 1000 entries\n                if len(self.metrics_history) > 1000:\n                    self.metrics_history = self.metrics_history[-1000:]\n            except Exception as e:\n                logger.error(f\"Error in monitoring loop: {e}\")\n            await asyncio.sleep(self.monitoring_interval_seconds)\n\n    async def _collect_metrics(self) -> Dict[str, Any]:\n        \"\"\"Collect all metrics.\"\"\"\n        system_metrics = await self.system_collector.collect_metrics()\n        redis_metrics = await self.redis_collector.collect_metrics()\n\n        return {\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            \"system\": {\n                \"cpu_percent\": system_metrics.cpu_percent,\n                \"memory_percent\": system_metrics.memory_percent,\n                \"disk_percent\": system_metrics.disk_percent,\n            },\n            \"redis\": {\n                \"hit_rate_percent\": redis_metrics.hit_rate_percent,\n                \"connected_clients\": redis_metrics.connected_clients,\n                \"used_memory_mb\": redis_metrics.used_memory_mb,\n            },\n        }\n\n    async def _check_system_thresholds(self, metrics: Any) -> None:\n        \"\"\"Check system metrics against thresholds.\"\"\"\n        cpu = getattr(metrics, \"cpu_percent\", 0)\n        memory = getattr(metrics, \"memory_percent\", 0)\n        disk = getattr(metrics, \"disk_percent\", 0)\n\n        if cpu >= self._thresholds[\"cpu_critical\"]:\n            await self.alert_manager.trigger_alert(\n                severity=AlertSeverity.CRITICAL,\n                title=\"Critical CPU Usage\",\n                description=f\"CPU usage at {cpu}% exceeds critical threshold\",\n                source=\"system_monitor\",\n            )\n        elif cpu >= self._thresholds[\"cpu_warning\"]:\n            await self.alert_manager.trigger_alert(\n                severity=AlertSeverity.WARNING,\n                title=\"High CPU Usage\",\n                description=f\"CPU usage at {cpu}%\",\n                source=\"system_monitor\",\n            )\n\n        if memory >= self._thresholds[\"memory_critical\"]:\n            await self.alert_manager.trigger_alert(\n                severity=AlertSeverity.CRITICAL,\n                title=\"Critical Memory Usage\",\n                description=f\"Memory usage at {memory}%\",\n                source=\"system_monitor\",\n            )\n\n        if disk >= self._thresholds[\"disk_critical\"]:\n            await self.alert_manager.trigger_alert(\n                severity=AlertSeverity.CRITICAL,\n                title=\"Critical Disk Usage\",\n                description=f\"Disk usage at {disk}%\",\n                source=\"system_monitor\",\n            )\n\n    async def _check_redis_thresholds(self, metrics: Any) -> None:\n        \"\"\"Check Redis metrics against thresholds.\"\"\"\n        hit_rate = getattr(metrics, \"hit_rate_percent\", 100)\n\n        if hit_rate < self._thresholds[\"redis_hit_rate_constitutional\"]:\n            await self.alert_manager.trigger_alert(\n                severity=AlertSeverity.CRITICAL,\n                title=\"Constitutional Violation: Low Cache Hit Rate\",\n                description=f\"Redis hit rate {hit_rate}% below constitutional requirement of 85%\",\n                source=\"redis_monitor\",\n            )\n\n    def get_monitoring_status(self) -> Dict[str, Any]:\n        \"\"\"Get current monitoring status.\"\"\"\n        return {\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            \"is_running\": self._running,\n            \"monitoring_interval_seconds\": self.monitoring_interval_seconds,\n            \"metrics_history_count\": len(self.metrics_history),\n            \"thresholds\": self._thresholds,\n        }\n\n    def get_current_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get most recent metrics.\"\"\"\n        if self.metrics_history:\n            latest = self.metrics_history[-1]\n            return {\n                **latest,\n                \"alerts\": {\n                    \"active_count\": len(self.alert_manager.active_alerts),\n                    \"total_count\": len(self.alert_manager.alert_history),\n                },\n            }\n        return {\n            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            \"system\": {},\n            \"redis\": {},\n            \"alerts\": {\n                \"active_count\": 0,\n                \"total_count\": 0,\n            },\n        }\n\n\n# Export all public classes\n__all__ = [\n    \"CONSTITUTIONAL_HASH\",\n    \"SystemMetrics\",\n    \"RedisMetrics\",\n    \"SystemMetricsCollector\",\n    \"RedisMetricsCollector\",\n    \"AlertManager\",\n    \"PagerDutyAlerting\",\n    \"ProductionMonitor\",\n    \"Alert\",\n    \"AlertSeverity\",\n    \"AlertStatus\",\n]\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.240904",
  "last_updated": "2026-01-04T05:35:58.499354"
}