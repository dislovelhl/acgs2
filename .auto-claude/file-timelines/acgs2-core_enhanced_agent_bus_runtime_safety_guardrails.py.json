{
  "file_path": "acgs2-core/enhanced_agent_bus/runtime_safety_guardrails.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Runtime Safety Guardrails\n\nOWASP-compliant layered security architecture for runtime protection:\n\n1. Input Sanitizer \u2192 Cleans and validates incoming requests\n2. Agent Engine \u2192 Core governance with constitutional validation\n3. Tool Runner (Sandbox) \u2192 Isolated execution environment\n4. Output Verifier \u2192 Post-execution content validation\n5. Audit Log \u2192 Immutable compliance trail\n\nConstitutional Hash: cdd01ef066bc6cf2\n\"\"\"\n\nimport asyncio\nimport hashlib\nimport json\nimport logging\nimport re\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nlogger = logging.getLogger(__name__)\n\nCONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\n\nclass GuardrailLayer(str, Enum):\n    \"\"\"OWASP-compliant guardrail layers.\"\"\"\n\n    INPUT_SANITIZER = \"input_sanitizer\"\n    AGENT_ENGINE = \"agent_engine\"\n    TOOL_RUNNER_SANDBOX = \"tool_runner_sandbox\"\n    OUTPUT_VERIFIER = \"output_verifier\"\n    AUDIT_LOG = \"audit_log\"\n\n\nclass SafetyAction(str, Enum):\n    \"\"\"Safety actions the guardrails can take.\"\"\"\n\n    ALLOW = \"allow\"\n    BLOCK = \"block\"\n    MODIFY = \"modify\"\n    ESCALATE = \"escalate\"\n    SANDBOX = \"sandbox\"\n    AUDIT = \"audit\"\n\n\nclass ViolationSeverity(str, Enum):\n    \"\"\"Severity levels for violations.\"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    CRITICAL = \"critical\"\n\n\n@dataclass\nclass Violation:\n    \"\"\"A safety violation detected by guardrails.\"\"\"\n\n    layer: GuardrailLayer\n    violation_type: str\n    severity: ViolationSeverity\n    message: str\n    details: Dict[str, Any] = field(default_factory=dict)\n    timestamp: datetime = field(default_factory=lambda: datetime.now(UTC))\n    trace_id: str = \"\"\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"layer\": self.layer.value,\n            \"violation_type\": self.violation_type,\n            \"severity\": self.severity.value,\n            \"message\": self.message,\n            \"details\": self.details,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"trace_id\": self.trace_id,\n        }\n\n\n@dataclass\nclass GuardrailResult:\n    \"\"\"Result from a guardrail layer.\"\"\"\n\n    action: SafetyAction\n    allowed: bool\n    violations: List[Violation] = field(default_factory=list)\n    modified_data: Any = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    processing_time_ms: float = 0.0\n    trace_id: str = \"\"\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"action\": self.action.value,\n            \"allowed\": self.allowed,\n            \"violations\": [v.to_dict() for v in self.violations],\n            \"modified_data\": self.modified_data,\n            \"metadata\": self.metadata,\n            \"processing_time_ms\": self.processing_time_ms,\n            \"trace_id\": self.trace_id,\n        }\n\n\nclass GuardrailComponent(ABC):\n    \"\"\"Abstract base class for guardrail components.\"\"\"\n\n    @abstractmethod\n    async def process(self, data: Any, context: Dict[str, Any]) -> GuardrailResult:\n        \"\"\"Process data through this guardrail component.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_layer(self) -> GuardrailLayer:\n        \"\"\"Return the guardrail layer this component implements.\"\"\"\n        pass\n\n    async def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get metrics for this component.\"\"\"\n        return {}\n\n\n@dataclass\nclass InputSanitizerConfig:\n    \"\"\"Configuration for input sanitizer.\"\"\"\n\n    enabled: bool = True\n    max_input_length: int = 1000000  # 1MB\n    allowed_content_types: List[str] = field(default_factory=lambda: [\"text/plain\", \"application/json\"])\n    sanitize_html: bool = True\n    detect_injection: bool = True\n    pii_detection: bool = True\n    timeout_ms: int = 1000\n\n\nclass InputSanitizer(GuardrailComponent):\n    \"\"\"Input Sanitizer: Layer 1 of OWASP guardrails.\n\n    Cleans, validates, and sanitizes incoming requests before they reach\n    the agent engine.\n    \"\"\"\n\n    def __init__(self, config: Optional[InputSanitizerConfig] = None):\n        self.config = config or InputSanitizerConfig()\n        self._pii_patterns = self._compile_pii_patterns()\n        self._injection_patterns = self._compile_injection_patterns()\n\n    def get_layer(self) -> GuardrailLayer:\n        return GuardrailLayer.INPUT_SANITIZER\n\n    def _compile_pii_patterns(self) -> List[re.Pattern]:\n        \"\"\"Compile PII detection patterns.\"\"\"\n        patterns = [\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN\n            r\"\\b\\d{16}\\b\",  # Credit card\n            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n            r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Phone\n            r\"\\b\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\b\",  # Credit card with spaces\n        ]\n        return [re.compile(p, re.IGNORECASE) for p in patterns]\n\n    def _compile_injection_patterns(self) -> List[re.Pattern]:\n        \"\"\"Compile injection attack patterns.\"\"\"\n        patterns = [\n            r\"<script[^>]*>.*?</script>\",  # XSS\n            r\"javascript:\",  # JavaScript injection\n            r\"on\\w+\\s*=\",  # Event handlers\n            r\"eval\\s*\\(\",  # Code injection\n            r\"exec\\s*\\(\",  # Code execution\n            r\"import\\s+os\",  # OS command injection\n            r\"subprocess\\.\",  # Subprocess injection\n        ]\n        return [re.compile(p, re.IGNORECASE | re.DOTALL) for p in patterns]\n\n    async def process(self, data: Any, context: Dict[str, Any]) -> GuardrailResult:\n        \"\"\"Sanitize input data.\"\"\"\n        start_time = time.time()\n        violations = []\n        trace_id = context.get(\"trace_id\", self._generate_trace_id())\n\n        try:\n            # Size validation\n            if isinstance(data, str) and len(data) > self.config.max_input_length:\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"input_too_large\",\n                    severity=ViolationSeverity.HIGH,\n                    message=f\"Input size {len(data)} exceeds maximum {self.config.max_input_length}\",\n                    trace_id=trace_id,\n                ))\n\n            # Content type validation\n            content_type = context.get(\"content_type\", \"text/plain\")\n            if content_type not in self.config.allowed_content_types:\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"invalid_content_type\",\n                    severity=ViolationSeverity.MEDIUM,\n                    message=f\"Content type {content_type} not allowed\",\n                    trace_id=trace_id,\n                ))\n\n            # Convert to string for processing\n            if isinstance(data, dict):\n                input_text = json.dumps(data)\n            elif isinstance(data, str):\n                input_text = data\n            else:\n                input_text = str(data)\n\n            # Store original text for injection detection before sanitization\n            original_text = input_text\n\n            # HTML sanitization\n            if self.config.sanitize_html:\n                input_text = self._sanitize_html(input_text)\n\n            # Injection detection (on original text)\n            if self.config.detect_injection:\n                injection_violations = self._detect_injection(original_text)\n                violations.extend(injection_violations)\n\n            # PII detection\n            if self.config.pii_detection:\n                pii_violations = self._detect_pii(input_text)\n                violations.extend(pii_violations)\n\n            # Determine action\n            if violations:\n                # Check if any violations are critical\n                critical_violations = [v for v in violations if v.severity == ViolationSeverity.CRITICAL]\n                if critical_violations:\n                    action = SafetyAction.BLOCK\n                    allowed = False\n                else:\n                    # PII detection should result in AUDIT (flag but allow)\n                    pii_violations = [v for v in violations if v.violation_type == \"pii_detected\"]\n                    if pii_violations:\n                        action = SafetyAction.AUDIT\n                        allowed = True\n                    else:\n                        action = SafetyAction.MODIFY if self.config.sanitize_html else SafetyAction.AUDIT\n                        allowed = True\n                        # Apply sanitization if needed\n                        if action == SafetyAction.MODIFY:\n                            input_text = self._apply_sanitization(input_text, violations)\n            else:\n                action = SafetyAction.ALLOW\n                allowed = True\n\n        except Exception as e:\n            logger.error(f\"Input sanitizer error: {e}\")\n            violations.append(Violation(\n                layer=self.get_layer(),\n                violation_type=\"processing_error\",\n                severity=ViolationSeverity.HIGH,\n                message=f\"Input processing failed: {str(e)}\",\n                trace_id=trace_id,\n            ))\n            action = SafetyAction.BLOCK\n            allowed = False\n            input_text = \"\"\n\n        processing_time = (time.time() - start_time) * 1000\n\n        return GuardrailResult(\n            action=action,\n            allowed=allowed,\n            violations=violations,\n            modified_data=input_text if input_text != str(data) else None,\n            metadata={\"original_length\": len(str(data))},\n            processing_time_ms=processing_time,\n            trace_id=trace_id,\n        )\n\n    def _sanitize_html(self, text: str) -> str:\n        \"\"\"Basic HTML sanitization.\"\"\"\n        # Remove script tags and their contents\n        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.IGNORECASE | re.DOTALL)\n        # Remove other dangerous tags\n        dangerous_tags = ['iframe', 'object', 'embed', 'form', 'input', 'button']\n        for tag in dangerous_tags:\n            text = re.sub(f'<{tag}[^>]*>.*?</{tag}>', '', text, flags=re.IGNORECASE | re.DOTALL)\n        return text\n\n    def _detect_injection(self, text: str) -> List[Violation]:\n        \"\"\"Detect injection attacks.\"\"\"\n        violations = []\n        for i, pattern in enumerate(self._injection_patterns):\n            if pattern.search(text):\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"injection_attack\",\n                    severity=ViolationSeverity.CRITICAL,\n                    message=f\"Potential injection attack detected (pattern {i})\",\n                    details={\"pattern_index\": i},\n                    trace_id=\"\",\n                ))\n        return violations\n\n    def _detect_pii(self, text: str) -> List[Violation]:\n        \"\"\"Detect personally identifiable information.\"\"\"\n        violations = []\n        for i, pattern in enumerate(self._pii_patterns):\n            matches = pattern.findall(text)\n            if matches:\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"pii_detected\",\n                    severity=ViolationSeverity.HIGH,\n                    message=f\"PII detected: {len(matches)} potential matches (pattern {i})\",\n                    details={\"pattern_index\": i, \"match_count\": len(matches)},\n                    trace_id=\"\",\n                ))\n        return violations\n\n    def _apply_sanitization(self, text: str, violations: List[Violation]) -> str:\n        \"\"\"Apply sanitization based on detected violations.\"\"\"\n        sanitized = text\n        # Redact PII\n        for pattern in self._pii_patterns:\n            sanitized = pattern.sub(\"[REDACTED]\", sanitized)\n        return sanitized\n\n    def _generate_trace_id(self) -> str:\n        \"\"\"Generate a trace ID.\"\"\"\n        timestamp = datetime.now(UTC).isoformat()\n        data = f\"{timestamp}-{CONSTITUTIONAL_HASH}\"\n        return hashlib.sha256(data.encode()).hexdigest()[:16]\n\n\n@dataclass\nclass AgentEngineConfig:\n    \"\"\"Configuration for agent engine.\"\"\"\n\n    enabled: bool = True\n    constitutional_validation: bool = True\n    impact_scoring: bool = True\n    deliberation_required_threshold: float = 0.8\n    timeout_ms: int = 5000\n\n\nclass AgentEngine(GuardrailComponent):\n    \"\"\"Agent Engine: Layer 2 of OWASP guardrails.\n\n    Core governance layer with constitutional validation and impact scoring.\n    \"\"\"\n\n    def __init__(self, config: Optional[AgentEngineConfig] = None):\n        self.config = config or AgentEngineConfig()\n\n    def get_layer(self) -> GuardrailLayer:\n        return GuardrailLayer.AGENT_ENGINE\n\n    async def process(self, data: Any, context: Dict[str, Any]) -> GuardrailResult:\n        \"\"\"Process through agent engine with constitutional validation.\"\"\"\n        start_time = time.time()\n        violations = []\n        trace_id = context.get(\"trace_id\", \"\")\n\n        try:\n            # Constitutional validation\n            if self.config.constitutional_validation:\n                constitutional_result = await self._validate_constitutional(data, context)\n                if not constitutional_result[\"compliant\"]:\n                    violations.append(Violation(\n                        layer=self.get_layer(),\n                        violation_type=\"constitutional_violation\",\n                        severity=ViolationSeverity.HIGH,\n                        message=\"Request violates constitutional principles\",\n                        details=constitutional_result,\n                        trace_id=trace_id,\n                    ))\n\n            # Impact scoring\n            if self.config.impact_scoring:\n                impact_score = await self._calculate_impact_score(data, context)\n                if impact_score > self.config.deliberation_required_threshold:\n                    violations.append(Violation(\n                        layer=self.get_layer(),\n                        violation_type=\"high_impact\",\n                        severity=ViolationSeverity.MEDIUM,\n                        message=f\"High impact action requires deliberation (score: {impact_score})\",\n                        details={\"impact_score\": impact_score},\n                        trace_id=trace_id,\n                    ))\n\n            # Determine action\n            if violations:\n                action = SafetyAction.ESCALATE\n                allowed = False\n            else:\n                action = SafetyAction.ALLOW\n                allowed = True\n\n        except Exception as e:\n            logger.error(f\"Agent engine error: {e}\")\n            violations.append(Violation(\n                layer=self.get_layer(),\n                violation_type=\"processing_error\",\n                severity=ViolationSeverity.HIGH,\n                message=f\"Agent engine processing failed: {str(e)}\",\n                trace_id=trace_id,\n            ))\n            action = SafetyAction.BLOCK\n            allowed = False\n\n        processing_time = (time.time() - start_time) * 1000\n\n        return GuardrailResult(\n            action=action,\n            allowed=allowed,\n            violations=violations,\n            processing_time_ms=processing_time,\n            trace_id=trace_id,\n        )\n\n    async def _validate_constitutional(self, data: Any, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate against constitutional principles.\"\"\"\n        # This would integrate with the constitutional validation system\n        # For now, return a mock result\n        return {\n            \"compliant\": True,\n            \"confidence\": 0.95,\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n\n    async def _calculate_impact_score(self, data: Any, context: Dict[str, Any]) -> float:\n        \"\"\"Calculate impact score for the action.\"\"\"\n        # This would use ML-based impact scoring\n        # For now, return a mock score\n        return 0.3\n\n\n@dataclass\nclass SandboxConfig:\n    \"\"\"Configuration for tool runner sandbox.\"\"\"\n\n    enabled: bool = True\n    use_firecracker: bool = False  # For production\n    use_docker: bool = True  # For development\n    timeout_ms: int = 10000\n    memory_limit_mb: int = 512\n    cpu_limit: float = 0.5\n    network_isolation: bool = True\n\n\nclass ToolRunnerSandbox(GuardrailComponent):\n    \"\"\"Tool Runner Sandbox: Layer 3 of OWASP guardrails.\n\n    Isolated execution environment for tool calls and external integrations.\n    \"\"\"\n\n    def __init__(self, config: Optional[SandboxConfig] = None):\n        self.config = config or SandboxConfig()\n\n    def get_layer(self) -> GuardrailLayer:\n        return GuardrailLayer.TOOL_RUNNER_SANDBOX\n\n    async def process(self, data: Any, context: Dict[str, Any]) -> GuardrailResult:\n        \"\"\"Execute in sandboxed environment.\"\"\"\n        start_time = time.time()\n        violations = []\n        trace_id = context.get(\"trace_id\", \"\")\n\n        try:\n            if not self.config.enabled:\n                return GuardrailResult(\n                    action=SafetyAction.ALLOW,\n                    allowed=True,\n                    trace_id=trace_id,\n                )\n\n            # Sandbox the execution\n            sandbox_result = await self._execute_in_sandbox(data, context)\n\n            if sandbox_result[\"success\"]:\n                action = SafetyAction.ALLOW\n                allowed = True\n            else:\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"sandbox_execution_failed\",\n                    severity=ViolationSeverity.HIGH,\n                    message=f\"Sandbox execution failed: {sandbox_result.get('error', 'Unknown error')}\",\n                    details=sandbox_result,\n                    trace_id=trace_id,\n                ))\n                action = SafetyAction.BLOCK\n                allowed = False\n\n        except Exception as e:\n            logger.error(f\"Sandbox error: {e}\")\n            violations.append(Violation(\n                layer=self.get_layer(),\n                violation_type=\"sandbox_error\",\n                severity=ViolationSeverity.CRITICAL,\n                message=f\"Sandbox execution error: {str(e)}\",\n                trace_id=trace_id,\n            ))\n            action = SafetyAction.BLOCK\n            allowed = False\n\n        processing_time = (time.time() - start_time) * 1000\n\n        return GuardrailResult(\n            action=action,\n            allowed=allowed,\n            violations=violations,\n            modified_data=sandbox_result.get(\"output\") if sandbox_result.get(\"success\") else None,\n            processing_time_ms=processing_time,\n            trace_id=trace_id,\n        )\n\n    async def _execute_in_sandbox(self, data: Any, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute code/data in sandboxed environment.\"\"\"\n        # This would implement actual sandboxing with Docker/Firecracker\n        # For now, return a mock successful result\n        return {\n            \"success\": True,\n            \"output\": data,\n            \"execution_time_ms\": 100,\n        }\n\n\n@dataclass\nclass OutputVerifierConfig:\n    \"\"\"Configuration for output verifier.\"\"\"\n\n    enabled: bool = True\n    content_safety_check: bool = True\n    pii_redaction: bool = True\n    hallucination_detection: bool = False  # Future feature\n    toxicity_filter: bool = True\n    timeout_ms: int = 2000\n\n\nclass OutputVerifier(GuardrailComponent):\n    \"\"\"Output Verifier: Layer 4 of OWASP guardrails.\n\n    Validates and sanitizes output before it reaches users.\n    \"\"\"\n\n    def __init__(self, config: Optional[OutputVerifierConfig] = None):\n        self.config = config or OutputVerifierConfig()\n        self._toxicity_patterns = self._compile_toxicity_patterns()\n\n    def get_layer(self) -> GuardrailLayer:\n        return GuardrailLayer.OUTPUT_VERIFIER\n\n    def _compile_toxicity_patterns(self) -> List[re.Pattern]:\n        \"\"\"Compile toxicity detection patterns.\"\"\"\n        patterns = [\n            r\"\\b(hate|racist|sexist|violent)\\b\",\n            r\"\\b(kill|murder|attack|harm)\\s+(yourself|others|people)\\b\",\n            r\"\\b(suicide|self-harm)\\b\",\n        ]\n        return [re.compile(p, re.IGNORECASE) for p in patterns]\n\n    async def process(self, data: Any, context: Dict[str, Any]) -> GuardrailResult:\n        \"\"\"Verify and sanitize output.\"\"\"\n        start_time = time.time()\n        violations = []\n        trace_id = context.get(\"trace_id\", \"\")\n        modified_output = None\n\n        try:\n            # Convert to string for processing\n            if isinstance(data, dict):\n                output_text = json.dumps(data)\n            elif isinstance(data, str):\n                output_text = data\n            else:\n                output_text = str(data)\n\n            # Content safety check\n            if self.config.content_safety_check:\n                safety_violations = self._check_content_safety(output_text)\n                violations.extend(safety_violations)\n\n            # Toxicity filter\n            if self.config.toxicity_filter:\n                toxicity_violations = self._check_toxicity(output_text)\n                violations.extend(toxicity_violations)\n\n            # PII redaction\n            if self.config.pii_redaction:\n                output_text, pii_violations = self._redact_pii(output_text)\n                violations.extend(pii_violations)\n                if pii_violations:\n                    modified_output = output_text\n\n            # Determine action\n            if violations:\n                # Check for critical violations\n                critical_violations = [v for v in violations if v.severity == ViolationSeverity.CRITICAL]\n                if critical_violations:\n                    action = SafetyAction.BLOCK\n                    allowed = False\n                else:\n                    action = SafetyAction.MODIFY\n                    allowed = True\n            else:\n                action = SafetyAction.ALLOW\n                allowed = True\n\n        except Exception as e:\n            logger.error(f\"Output verifier error: {e}\")\n            violations.append(Violation(\n                layer=self.get_layer(),\n                violation_type=\"processing_error\",\n                severity=ViolationSeverity.HIGH,\n                message=f\"Output verification failed: {str(e)}\",\n                trace_id=trace_id,\n            ))\n            action = SafetyAction.BLOCK\n            allowed = False\n\n        processing_time = (time.time() - start_time) * 1000\n\n        return GuardrailResult(\n            action=action,\n            allowed=allowed,\n            violations=violations,\n            modified_data=modified_output,\n            processing_time_ms=processing_time,\n            trace_id=trace_id,\n        )\n\n    def _check_content_safety(self, text: str) -> List[Violation]:\n        \"\"\"Check content for safety violations.\"\"\"\n        violations = []\n\n        # Check for harmful instructions\n        harmful_patterns = [\n            r\"\\b(how\\s+to|instructions?\\s+for)\\s+(hack|exploit|attack|build.*bomb)\\b\",\n            r\"\\b(create|make|build)\\s+(virus|malware|ransomware|trojan)\\b\",\n        ]\n\n        for pattern in harmful_patterns:\n            if re.search(pattern, text, re.IGNORECASE):\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"harmful_content\",\n                    severity=ViolationSeverity.CRITICAL,\n                    message=\"Output contains potentially harmful instructions\",\n                    trace_id=\"\",\n                ))\n                break  # Only report once\n\n        return violations\n\n    def _check_toxicity(self, text: str) -> List[Violation]:\n        \"\"\"Check for toxic content.\"\"\"\n        violations = []\n\n        for i, pattern in enumerate(self._toxicity_patterns):\n            if pattern.search(text):\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"toxicity_detected\",\n                    severity=ViolationSeverity.HIGH,\n                    message=f\"Toxic content detected (pattern {i})\",\n                    details={\"pattern_index\": i},\n                    trace_id=\"\",\n                ))\n\n        return violations\n\n    def _redact_pii(self, text: str) -> tuple[str, List[Violation]]:\n        \"\"\"Redact PII from output.\"\"\"\n        violations = []\n        redacted = text\n\n        # Simple PII patterns (same as input sanitizer)\n        pii_patterns = [\n            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN\n            r\"\\b\\d{16}\\b\",  # Credit card\n            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n        ]\n\n        for pattern in pii_patterns:\n            compiled = re.compile(pattern, re.IGNORECASE)\n            matches = compiled.findall(text)\n            if matches:\n                violations.append(Violation(\n                    layer=self.get_layer(),\n                    violation_type=\"pii_leak\",\n                    severity=ViolationSeverity.HIGH,\n                    message=f\"PII detected in output: {len(matches)} instances\",\n                    details={\"match_count\": len(matches)},\n                    trace_id=\"\",\n                ))\n                redacted = compiled.sub(\"[REDACTED]\", redacted)\n\n        return redacted, violations\n\n\n@dataclass\nclass AuditLogConfig:\n    \"\"\"Configuration for audit log.\"\"\"\n\n    enabled: bool = True\n    retention_days: int = 90\n    log_to_blockchain: bool = False  # Future feature\n    log_to_siem: bool = False  # Future feature\n\n\nclass AuditLog(GuardrailComponent):\n    \"\"\"Audit Log: Layer 5 of OWASP guardrails.\n\n    Immutable compliance trail for all guardrail decisions.\n    \"\"\"\n\n    def __init__(self, config: Optional[AuditLogConfig] = None):\n        self.config = config or AuditLogConfig()\n        self._audit_entries: List[Dict[str, Any]] = []\n\n    def get_layer(self) -> GuardrailLayer:\n        return GuardrailLayer.AUDIT_LOG\n\n    async def process(self, data: Any, context: Dict[str, Any]) -> GuardrailResult:\n        \"\"\"Log the audit entry.\"\"\"\n        trace_id = context.get(\"trace_id\", \"\")\n\n        audit_entry = {\n            \"trace_id\": trace_id,\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"layer\": context.get(\"current_layer\", \"\").value if context.get(\"current_layer\") else \"\",\n            \"action\": context.get(\"action\", \"\").value if context.get(\"action\") else \"\",\n            \"allowed\": context.get(\"allowed\", False),\n            \"violations\": [v.to_dict() for v in context.get(\"violations\", [])],\n            \"processing_time_ms\": context.get(\"processing_time_ms\", 0),\n            \"metadata\": context.get(\"metadata\", {}),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n\n        if self.config.enabled:\n            self._audit_entries.append(audit_entry)\n            logger.info(f\"Audit log entry: {json.dumps(audit_entry)}\")\n\n            # Future: Log to blockchain/SIEM\n            if self.config.log_to_blockchain:\n                await self._log_to_blockchain(audit_entry)\n            if self.config.log_to_siem:\n                await self._log_to_siem(audit_entry)\n\n        return GuardrailResult(\n            action=SafetyAction.ALLOW,\n            allowed=True,\n            trace_id=trace_id,\n        )\n\n    async def _log_to_blockchain(self, entry: Dict[str, Any]) -> None:\n        \"\"\"Log entry to blockchain for immutability.\"\"\"\n        # Future implementation\n        pass\n\n    async def _log_to_siem(self, entry: Dict[str, Any]) -> None:\n        \"\"\"Log entry to SIEM system.\"\"\"\n        # Future implementation\n        pass\n\n    async def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get audit log metrics.\"\"\"\n        total_entries = len(self._audit_entries)\n        if total_entries == 0:\n            return {\"total_entries\": 0}\n\n        # Calculate metrics\n        allowed_count = sum(1 for entry in self._audit_entries if entry.get(\"allowed\", False))\n        violation_count = sum(len(entry.get(\"violations\", [])) for entry in self._audit_entries)\n\n        return {\n            \"total_entries\": total_entries,\n            \"allowed_count\": allowed_count,\n            \"blocked_count\": total_entries - allowed_count,\n            \"allowed_rate\": allowed_count / total_entries,\n            \"violation_rate\": violation_count / total_entries,\n            \"avg_processing_time_ms\": sum(entry.get(\"processing_time_ms\", 0) for entry in self._audit_entries) / total_entries,\n        }\n\n    def get_entries(self, trace_id: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Get audit entries, optionally filtered by trace ID.\"\"\"\n        if trace_id:\n            return [entry for entry in self._audit_entries if entry.get(\"trace_id\") == trace_id]\n        return self._audit_entries.copy()\n\n\n@dataclass\nclass RuntimeSafetyGuardrailsConfig:\n    \"\"\"Configuration for the complete runtime safety guardrails system.\"\"\"\n\n    input_sanitizer: InputSanitizerConfig = field(default_factory=InputSanitizerConfig)\n    agent_engine: AgentEngineConfig = field(default_factory=AgentEngineConfig)\n    sandbox: SandboxConfig = field(default_factory=SandboxConfig)\n    output_verifier: OutputVerifierConfig = field(default_factory=OutputVerifierConfig)\n    audit_log: AuditLogConfig = field(default_factory=AuditLogConfig)\n\n    strict_mode: bool = False\n    fail_closed: bool = True  # Block on any error\n    timeout_ms: int = 15000  # Total timeout for all layers\n\n\nclass RuntimeSafetyGuardrails:\n    \"\"\"\n    OWASP-compliant Runtime Safety Guardrails System.\n\n    Implements 5-layer security architecture:\n    1. Input Sanitizer - Clean and validate incoming requests\n    2. Agent Engine - Constitutional governance validation\n    3. Tool Runner Sandbox - Isolated execution environment\n    4. Output Verifier - Post-execution content validation\n    5. Audit Log - Immutable compliance trail\n\n    Constitutional Hash: cdd01ef066bc6cf2\n    \"\"\"\n\n    def __init__(self, config: Optional[RuntimeSafetyGuardrailsConfig] = None):\n        self.config = config or RuntimeSafetyGuardrailsConfig()\n\n        # Initialize guardrail layers\n        self.layers = {\n            GuardrailLayer.INPUT_SANITIZER: InputSanitizer(self.config.input_sanitizer),\n            GuardrailLayer.AGENT_ENGINE: AgentEngine(self.config.agent_engine),\n            GuardrailLayer.TOOL_RUNNER_SANDBOX: ToolRunnerSandbox(self.config.sandbox),\n            GuardrailLayer.OUTPUT_VERIFIER: OutputVerifier(self.config.output_verifier),\n            GuardrailLayer.AUDIT_LOG: AuditLog(self.config.audit_log),\n        }\n\n    async def process_request(\n        self,\n        request_data: Any,\n        context: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Process a request through all guardrail layers.\n\n        Args:\n            request_data: The request data to process\n            context: Optional context information\n\n        Returns:\n            Dict containing processing results and final decision\n        \"\"\"\n        context = context or {}\n        trace_id = context.get(\"trace_id\", self._generate_trace_id())\n        context[\"trace_id\"] = trace_id\n\n        start_time = time.time()\n        layer_results = {}\n        current_data = request_data\n        final_allowed = True\n        all_violations = []\n\n        try:\n            # Process through each layer in order\n            layer_order = [\n                GuardrailLayer.INPUT_SANITIZER,\n                GuardrailLayer.AGENT_ENGINE,\n                GuardrailLayer.TOOL_RUNNER_SANDBOX,\n                GuardrailLayer.OUTPUT_VERIFIER,\n            ]\n\n            for layer_type in layer_order:\n                if not self.layers[layer_type].config.enabled:\n                    continue\n\n                layer = self.layers[layer_type]\n\n                # Create layer context\n                layer_context = context.copy()\n                layer_context[\"current_data\"] = current_data\n\n                # Process through layer\n                try:\n                    result = await asyncio.wait_for(\n                        layer.process(current_data, layer_context),\n                        timeout=self.config.timeout_ms / 1000\n                    )\n                except asyncio.TimeoutError:\n                    result = GuardrailResult(\n                        action=SafetyAction.BLOCK,\n                        allowed=False,\n                        violations=[Violation(\n                            layer=layer_type,\n                            violation_type=\"timeout\",\n                            severity=ViolationSeverity.CRITICAL,\n                            message=f\"Layer {layer_type.value} timed out\",\n                            trace_id=trace_id,\n                        )]\n                    )\n\n                layer_results[layer_type.value] = result.to_dict()\n\n                # Update current data if modified\n                if result.modified_data is not None:\n                    current_data = result.modified_data\n\n                # Collect violations\n                all_violations.extend(result.violations)\n\n                # Check if we should continue\n                if not result.allowed:\n                    final_allowed = False\n                    if self.config.fail_closed:\n                        break  # Stop processing on first block\n\n            # Always log to audit (final layer)\n            audit_layer = self.layers[GuardrailLayer.AUDIT_LOG]\n            audit_context = context.copy()\n            audit_context.update({\n                \"action\": SafetyAction.ALLOW if final_allowed else SafetyAction.BLOCK,\n                \"allowed\": final_allowed,\n                \"violations\": all_violations,\n                \"processing_time_ms\": (time.time() - start_time) * 1000,\n            })\n\n            await audit_layer.process(current_data, audit_context)\n\n        except Exception as e:\n            logger.error(f\"Guardrails processing error: {e}\")\n            final_allowed = False\n            all_violations.append(Violation(\n                layer=GuardrailLayer.AUDIT_LOG,  # Generic error\n                violation_type=\"system_error\",\n                severity=ViolationSeverity.CRITICAL,\n                message=f\"Guardrails system error: {str(e)}\",\n                trace_id=trace_id,\n            ))\n\n        total_time = (time.time() - start_time) * 1000\n\n        return {\n            \"allowed\": final_allowed,\n            \"final_data\": current_data,\n            \"violations\": [v.to_dict() for v in all_violations],\n            \"layer_results\": layer_results,\n            \"trace_id\": trace_id,\n            \"total_processing_time_ms\": total_time,\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n\n    def _generate_trace_id(self) -> str:\n        \"\"\"Generate a unique trace ID.\"\"\"\n        timestamp = datetime.now(UTC).isoformat()\n        data = f\"{timestamp}-{CONSTITUTIONAL_HASH}-{id(self)}\"\n        return hashlib.sha256(data.encode()).hexdigest()[:16]\n\n    async def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive guardrails metrics.\"\"\"\n        metrics = {\n            \"system\": {\n                \"constitutional_hash\": CONSTITUTIONAL_HASH,\n                \"layers_enabled\": [layer.value for layer, component in self.layers.items()\n                                 if getattr(component.config, 'enabled', True)],\n            }\n        }\n\n        # Collect metrics from each layer\n        for layer_type, component in self.layers.items():\n            try:\n                layer_metrics = await component.get_metrics()\n                metrics[layer_type.value] = layer_metrics\n            except Exception as e:\n                logger.error(f\"Error getting metrics for {layer_type.value}: {e}\")\n                metrics[layer_type.value] = {\"error\": str(e)}\n\n        return metrics\n\n    def get_layer(self, layer_type: GuardrailLayer) -> Optional[GuardrailComponent]:\n        \"\"\"Get a specific guardrail layer component.\"\"\"\n        return self.layers.get(layer_type)\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.201683",
  "last_updated": "2026-01-04T05:35:59.136020"
}