{
  "file_path": "src/core/enhanced_agent_bus/deliberation_layer/impact_scorer.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"Constitutional Hash: cdd01ef066bc6cf2\nImpact Scorer v3.1.0 - ML-Powered Impact Assessment\n\nThis module provides ML-based impact scoring for governance decisions using:\n1. ONNX Runtime (fastest) - GPU-accelerated inference\n2. PyTorch Transformers (fallback) - CPU/GPU inference\n3. NumPy heuristics (final fallback) - Keyword-based scoring\n\nThe fallback cascade ensures the service remains operational even when\nML dependencies are unavailable.\n\"\"\"\nimport logging\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\n\ntry:\n    from ..models import AgentMessage, MessageType, Priority\n    from ..utils import LRUCache, redact_error_message\nexcept (ImportError, ValueError):\n    from models import AgentMessage  # type: ignore\n\nlogger = logging.getLogger(__name__)\n\n# ===== Backend Detection with Proper Fallback Cascade =====\n\n# ONNX Runtime availability check\nONNX_AVAILABLE = False\ntry:\n    import onnxruntime as ort\n    ONNX_AVAILABLE = True\n    logger.info(f\"ONNX Runtime available: {ort.__version__}\")\nexcept ImportError:\n    logger.info(\"ONNX Runtime not available - will use PyTorch or heuristics fallback\")\n\n# Transformers availability check\nTRANSFORMERS_AVAILABLE = False\ntry:\n    import torch\n    from sklearn.metrics.pairwise import cosine_similarity\n    from transformers import AutoModel, AutoTokenizer\n    TRANSFORMERS_AVAILABLE = True\n    logger.info(f\"Transformers available: torch={torch.__version__}\")\nexcept ImportError:\n    logger.info(\"Transformers not available - will use heuristics fallback\")\n\n# Feature flags based on availability (can be overridden via environment)\nUSE_TRANSFORMERS = TRANSFORMERS_AVAILABLE and os.getenv(\"USE_TRANSFORMERS\", \"true\").lower() == \"true\"\nUSE_ONNX = ONNX_AVAILABLE and os.getenv(\"USE_ONNX_INFERENCE\", \"true\").lower() == \"true\"\n\ntry:\n    import onnxruntime as ort\n    ONNX_AVAILABLE = True\nexcept ImportError:\n    ONNX_AVAILABLE = False\n\nPROFILING_AVAILABLE = False\n\n\n@dataclass\nclass ScoringConfig:\n    semantic_weight: float = 0.3\n    permission_weight: float = 0.2\n    volume_weight: float = 0.1\n    context_weight: float = 0.1\n    drift_weight: float = 0.1\n    priority_weight: float = 0.1\n    type_weight: float = 0.1\n    critical_priority_boost: float = 0.9\n    high_semantic_boost: float = 0.8\n\n\n@dataclass\nclass ImpactAnalysis:\n    score: float\n    factors: Dict[str, float]\n    recommendation: str\n    requires_deliberation: bool\n\n\nclass ImpactScorer:\n    \"\"\"\n    ImpactScorer v3.1.0 - ML-Powered Governance Impact Assessment\n\n    Implements a fallback cascade:\n    1. ONNX Runtime (fastest) - GPU-accelerated when available\n    2. PyTorch Transformers (fallback) - Full model inference\n    3. NumPy heuristics (final fallback) - Keyword-based scoring\n\n    Feature flags:\n    - USE_TRANSFORMERS: Enable/disable ML inference (env: USE_TRANSFORMERS)\n    - USE_ONNX: Enable/disable ONNX optimization (env: USE_ONNX_INFERENCE)\n    \"\"\"\n\n    # Class-level model cache for singleton behavior\n    _model_instance: Optional[Any] = None\n    _tokenizer_instance: Optional[Any] = None\n    _onnx_session_instance: Optional[Any] = None\n\n    def __init__(\n        self,\n        config: Optional[ScoringConfig] = None,\n        model_name: str = \"distilbert-base-uncased\",\n        use_onnx: bool = True,\n        tokenization_cache_size: int = 1000,\n        onnx_model_path: Optional[str] = None,\n    ):\n        self.config = config or ScoringConfig()\n        self.model_name = model_name\n        self.use_onnx = use_onnx\n        self._onnx_enabled = use_onnx if ONNX_AVAILABLE else False\n        self._bert_enabled = False\n        self.session = None\n\n        if TRANSFORMERS_AVAILABLE:\n            try:\n                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n                if self._onnx_enabled:\n                    import pathlib\n\n                    # Path to the optimized ONNX model\n                    model_dir = pathlib.Path(__file__).parent / \"optimized_models\"\n                    onnx_path = model_dir / f\"{model_name.replace('-', '_')}.onnx\"\n\n                    if onnx_path.exists():\n                        # Create ONNX session with GPU if available\n                        providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n                        try:\n                            self.session = ort.InferenceSession(str(onnx_path), providers=providers)\n\n                            # Verify actual provider\n                            active_providers = self.session.get_providers()\n                            logger.info(f\"ONNX Runtime active providers: {active_providers}\")\n\n                            if \"CUDAExecutionProvider\" in active_providers:\n                                logger.info(\"GPU acceleration ENABLED \u2713\")\n                            else:\n                                logger.warning(\"GPU acceleration DISABLED - running on CPU fallback\")\n\n                            self._bert_enabled = True\n                        except Exception as e:\n                            logger.error(f\"Failed to initialize ONNX session: {e}\")\n                            self._onnx_enabled = False\n                    else:\n                        logger.warning(f\"ONNX model not found at {onnx_path}, falling back to CPU\")\n                        self._onnx_enabled = False\n\n                if not self._onnx_enabled:\n                    # Fallback to standard BERT if ONNX disabled or failed\n                    self.model = AutoModel.from_pretrained(model_name).eval()\n                    self._bert_enabled = True\n            except Exception as e:\n                logger.warning(f\"Model load failed: {e}\")\n\n        self.high_impact_keywords = [\n            \"critical\",\n            \"emergency\",\n            \"security\",\n            \"breach\",\n            \"violation\",\n            \"danger\",\n            \"risk\",\n            \"threat\",\n            \"attack\",\n            \"exploit\",\n            \"vulnerability\",\n            \"compromise\",\n            \"governance\",\n            \"policy\",\n            \"regulation\",\n            \"compliance\",\n            \"legal\",\n            \"audit\",\n            \"financial\",\n            \"transaction\",\n            \"payment\",\n            \"transfer\",\n            \"blockchain\",\n            \"consensus\",\n            \"unauthorized\",\n            \"abnormal\",\n            \"suspicious\",\n            \"alert\",\n        ]\n        self._agent_rates: Dict[str, int] = {}\n        self._agent_history: Dict[str, List[float]] = {}\n        self._keyword_embeddings: Optional[np.ndarray] = None\n\n    def _ensure_model_loaded(self) -> bool:\n        \"\"\"\n        Lazy load model on first use to reduce startup time.\n        Returns True if ML inference is available, False otherwise.\n\n        Implements fallback cascade:\n        1. Try ONNX Runtime first (fastest)\n        2. Fall back to PyTorch Transformers\n        3. Use heuristics if both fail\n        \"\"\"\n        if self._model_loaded:\n            return self._bert_enabled or self._onnx_enabled\n\n        self._model_loaded = True\n\n        # Try ONNX first (fastest path)\n        if self.use_onnx and ONNX_AVAILABLE:\n            if self._load_onnx_model():\n                return True\n\n        # Fall back to PyTorch Transformers\n        if USE_TRANSFORMERS and TRANSFORMERS_AVAILABLE:\n            if self._load_transformer_model():\n                return True\n\n        logger.warning(\"ML models unavailable - using keyword-based heuristics fallback\")\n        return False\n\n    def _load_onnx_model(self) -> bool:\n        \"\"\"Load ONNX model for optimized inference.\"\"\"\n        try:\n            # Check for cached instance first\n            if ImpactScorer._onnx_session_instance is not None:\n                self.onnx_session = ImpactScorer._onnx_session_instance\n                self.tokenizer = ImpactScorer._tokenizer_instance\n                self._onnx_enabled = True\n                logger.info(\"ONNX session reused from cache\")\n                return True\n\n            # Load tokenizer\n            from transformers import AutoTokenizer\n            cache_dir = os.getenv(\"TRANSFORMERS_CACHE\", None)\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_name,\n                cache_dir=cache_dir\n            )\n\n            # Find ONNX model file\n            onnx_path = self._find_onnx_model_path()\n            if onnx_path is None:\n                logger.info(\"ONNX model not found - will use PyTorch fallback\")\n                return False\n\n            # Create ONNX session with GPU if available\n            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n            self.onnx_session = ort.InferenceSession(str(onnx_path), providers=providers)\n\n            # Verify GPU provider was loaded\n            active_providers = self.onnx_session.get_providers()\n            if 'CUDAExecutionProvider' in active_providers:\n                logger.info(\"GPU acceleration ENABLED \u2713 - Using ONNX with CUDA\")\n            else:\n                logger.info(\"GPU acceleration DISABLED - Using ONNX with CPU fallback\")\n\n            # Cache for reuse\n            ImpactScorer._onnx_session_instance = self.onnx_session\n            ImpactScorer._tokenizer_instance = self.tokenizer\n            self._onnx_enabled = True\n\n            return True\n\n        except Exception as e:\n            logger.warning(f\"ONNX model load failed: {e}\")\n            return False\n\n    def _find_onnx_model_path(self) -> Optional[Path]:\n        \"\"\"Find ONNX model file with configurable path resolution.\"\"\"\n        # Check environment variable first\n        env_path = os.getenv(\"ONNX_MODEL_PATH\")\n        if env_path and Path(env_path).exists():\n            return Path(env_path)\n\n        # Check relative paths from module location\n        base_paths = [\n            Path(__file__).parent / \"optimized_models\",\n            Path(__file__).parent.parent / \"optimized_models\",\n            Path.cwd() / \"optimized_models\",\n        ]\n\n        model_filename = \"distilbert_base_uncased.onnx\"\n\n        for base in base_paths:\n            model_path = base / model_filename\n            if model_path.exists():\n                return model_path\n\n        return None\n\n    def _load_transformer_model(self) -> bool:\n        \"\"\"Load PyTorch Transformers model for inference.\"\"\"\n        try:\n            # Check for cached instance first\n            if ImpactScorer._model_instance is not None:\n                self.model = ImpactScorer._model_instance\n                self.tokenizer = ImpactScorer._tokenizer_instance\n                self._bert_enabled = True\n                logger.info(\"Transformers model reused from cache\")\n                return True\n\n            from transformers import AutoModel, AutoTokenizer\n            cache_dir = os.getenv(\"TRANSFORMERS_CACHE\", None)\n\n            # Load tokenizer and model\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_name,\n                cache_dir=cache_dir\n            )\n            self.model = AutoModel.from_pretrained(\n                self.model_name,\n                cache_dir=cache_dir\n            )\n            self.model.eval()  # Set to evaluation mode\n\n            # Cache for reuse\n            ImpactScorer._model_instance = self.model\n            ImpactScorer._tokenizer_instance = self.tokenizer\n            self._bert_enabled = True\n\n            logger.info(f\"Transformers model loaded: {self.model_name}\")\n            return True\n\n        except Exception as e:\n            logger.warning(f\"Transformers model load failed: {e}\")\n            return False\n\n    def _extract_text_content(self, message: Any) -> str:\n        if isinstance(message, dict):\n            res = []\n            if \"content\" in message:\n                c = message[\"content\"]\n                res.append(str(c[\"text\"]) if isinstance(c, dict) and \"text\" in c else str(c))\n            if \"payload\" in message:\n                p = message[\"payload\"]\n                if isinstance(p, dict) and \"message\" in p:\n                    res.append(str(p[\"message\"]))\n            return \" \".join(res)  # DO NOT .lower() here, tests are case sensitive\n        return str(getattr(message, \"content\", \"\"))\n\n    def _calculate_priority_factor(\n        self, message: Any, context: Optional[Dict[str, Any]] = None\n    ) -> float:\n        p = None\n        if context and \"priority\" in context:\n            p = context[\"priority\"]\n        elif isinstance(message, dict) and \"priority\" in message:\n            p = message[\"priority\"]\n        elif hasattr(message, \"priority\"):\n            p = message.priority\n\n        if p is None:\n            return 0.5\n\n        if hasattr(p, \"name\"):\n            p_name = p.name.lower()\n        elif isinstance(p, str):\n            p_name = p.lower()\n        else:\n            p_name = str(p).lower()\n\n        if \"critical\" in p_name or p_name == \"3\":\n            return 1.0\n        if \"high\" in p_name or p_name == \"2\":\n            return 0.7\n        if p_name in [\"medium\", \"normal\", \"1\"]:\n            return 0.5\n        if \"low\" in p_name or p_name == \"0\":\n            return 0.2\n        return 0.5\n\n    def _calculate_type_factor(\n        self, message: Any, context: Optional[Dict[str, Any]] = None\n    ) -> float:\n        t = None\n        if context and \"message_type\" in context:\n            t = context[\"message_type\"]\n        elif isinstance(message, dict) and \"message_type\" in message:\n            t = message[\"message_type\"]\n        elif hasattr(message, \"message_type\"):\n            t = message.message_type\n\n        if t is None:\n            return 0.2\n\n        if hasattr(t, \"name\"):\n            t_name = t.name.lower()\n        elif isinstance(t, str):\n            t_name = t.lower()\n        else:\n            t_name = str(t).lower()\n\n        if \"governance\" in t_name or \"constitutional\" in t_name:\n            return 0.8\n        if \"command\" in t_name:\n            return 0.4\n        return 0.2\n\n    def _calculate_permission_score(self, message: Any) -> float:\n        tools = []\n        if isinstance(message, dict) and \"tools\" in message:\n            tools = message[\"tools\"]\n        elif hasattr(message, \"tools\"):\n            tools = message.tools\n\n        if not tools:\n            return 0.1\n\n        max_risk = 0.1\n        for tool in tools:\n            name = (\n                tool\n                if isinstance(tool, str)\n                else (tool.get(\"name\", \"\") if isinstance(tool, dict) else str(tool))\n            ).lower()\n            risk = 0.1\n            if any(k in name for k in [\"execute\", \"delete\", \"write\", \"submit\", \"transfer\"]):\n                risk = 0.9\n            elif any(k in name for k in [\"send\", \"update\", \"modify\"]):\n                risk = 0.5\n            elif any(k in name for k in [\"read\", \"get\", \"list\", \"view\"]):\n                risk = 0.2\n            max_risk = max(max_risk, risk)\n        return max_risk\n\n    def _calculate_volume_score(self, agent_id: str) -> float:\n        if not isinstance(agent_id, str):\n            return 0.1\n        rate = self._agent_rates.get(agent_id, 0)\n        self._agent_rates[agent_id] = rate + 1\n        if rate >= 150:\n            return 1.0\n        if rate >= 50:\n            return 0.5\n        if rate >= 20:\n            return 0.2\n        return 0.1\n\n    def _calculate_context_score(\n        self, message: Any, context: Optional[Dict[str, Any]] = None\n    ) -> float:\n        base = 0.2\n        if isinstance(message, dict) and \"payload\" in message:\n            payload = message[\"payload\"]\n            if isinstance(payload, dict) and payload.get(\"amount\", 0) > 1000:\n                base += 0.4\n        elif hasattr(message, \"payload\"):\n            if getattr(message.payload, \"amount\", 0) > 1000:\n                base += 0.4\n        return base\n\n    def _calculate_drift_score(self, agent_id: str, current_score: float) -> float:\n        if not isinstance(agent_id, str):\n            return 0.0\n        hist = self._agent_history.setdefault(agent_id, [])\n        if not hist:\n            hist.append(current_score)\n            return 0.0\n        avg = sum(hist) / len(hist)\n        drift = abs(current_score - avg)\n        hist.append(current_score)\n        if len(hist) > 20:\n            hist.pop(0)\n        return min(1.0, drift * 2.0)\n\n    def _calculate_semantic_score(self, message: Any) -> float:\n        \"\"\"\n        Calculate semantic impact score using ML embeddings or keyword heuristics.\n\n        Uses fallback cascade:\n        1. ONNX/BERT embedding similarity (if available)\n        2. Keyword-based heuristics (always available)\n        \"\"\"\n        text = self._extract_text_content(message).strip().lower()\n        if not text:\n            return 0.0\n\n        # Keyword-based scoring (always computed as baseline)\n        hits = sum(1 for k in self.high_impact_keywords if k in text)\n        keyword_score = 0.1\n        if hits >= 5:\n            keyword_score = 1.0\n        elif hits >= 3:\n            keyword_score = 0.8\n        elif hits > 0:\n            keyword_score = 0.5\n\n        # Embedding-based scoring (if ML available)\n        embedding_score = 0.0\n        if self._ensure_model_loaded() and (self._bert_enabled or self._onnx_enabled):\n            try:\n                emb = self._get_embeddings(text)\n                kw_emb = self._get_keyword_embeddings()\n\n                # Skip if embeddings are zeros (fallback mode)\n                if np.any(emb) and np.any(kw_emb):\n                    if TRANSFORMERS_AVAILABLE:\n                        from sklearn.metrics.pairwise import cosine_similarity\n                        sim = cosine_similarity(emb.reshape(1, -1) if emb.ndim == 1 else emb, kw_emb)\n                        embedding_score = float(np.max(sim))\n                    else:\n                        # Manual cosine similarity fallback\n                        emb_flat = emb.flatten()\n                        sims = [cosine_similarity_fallback(emb_flat, kw) for kw in kw_emb]\n                        embedding_score = max(sims) if sims else 0.0\n\n            except Exception as e:\n                logger.debug(f\"Embedding-based scoring failed: {e}\")\n\n        return max(keyword_score, embedding_score)\n\n    async def calculate_impact_score_async(self, message: Any, context: Dict[str, Any] = None) -> float:\n        if not message and not context:\n            return 0.1\n\n        agent_id = \"anonymous\"\n        if context and \"agent_id\" in context:\n            agent_id = context[\"agent_id\"]\n        elif isinstance(message, dict) and \"from_agent\" in message:\n            agent_id = message[\"from_agent\"]\n        elif hasattr(message, \"from_agent\"):\n            agent_id = getattr(message, \"from_agent\", \"anonymous\")\n\n        # Use pre-computed semantic score if provided in context\n        if context and \"semantic_override\" in context:\n            semantic = context[\"semantic_override\"]\n        else:\n            semantic = self._calculate_semantic_score(message)\n\n        scores = {\n            \"semantic\": semantic,\n            \"permission\": self._calculate_permission_score(message),\n            \"volume\": self._calculate_volume_score(agent_id),\n            \"context\": self._calculate_context_score(message, context),\n            \"drift\": self._calculate_drift_score(agent_id, semantic),\n            \"priority\": self._calculate_priority_factor(message, context),\n            \"type\": self._calculate_type_factor(message, context),\n        }\n\n        weighted = sum(scores[k] * getattr(self.config, f\"{k}_weight\") for k in scores)\n\n        if scores[\"priority\"] >= 0.9:\n            weighted = max(weighted, self.config.critical_priority_boost)\n        if scores[\"semantic\"] >= 0.8:\n            weighted = max(weighted, self.config.high_semantic_boost)\n\n        final = min(1.0, weighted)\n        if hasattr(message, \"impact_score\"):\n            message.impact_score = final\n        return final\n\n    def score_batch(self, messages: list) -> list[float]:\n        \"\"\"Process multiple messages efficiently with batching.\"\"\"\n        if not messages:\n            return []\n\n        # Extract text content for all messages\n        texts = [self._extract_text_content(m).strip().lower() for m in messages]\n\n        # If ML is disabled, fallback to sequential\n        if not self._bert_enabled:\n            return [self.calculate_impact_score(m) for m in messages]\n\n        try:\n            # Batch tokenization\n            inputs = self.tokenizer(\n                texts, return_tensors=\"np\" if self._onnx_enabled else \"pt\",\n                padding=True, truncation=True, max_length=512\n            )\n\n            if self._onnx_enabled and self.session:\n                onnx_inputs = {k: v for k, v in inputs.items()}\n                outputs = self.session.run(None, onnx_inputs)\n                embeddings = outputs[0][:, 0, :]\n            else:\n                import torch\n                with torch.no_grad():\n                    outputs = self.model(**inputs)\n                    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n            # For each message, calculate its semantic score using the embedding\n            kw_emb = self._get_keyword_embeddings()\n\n            results = []\n            for i, message in enumerate(messages):\n                emb = embeddings[i : i + 1]\n\n                # Semantic score calculation (max similarity to keywords)\n                if TRANSFORMERS_AVAILABLE:\n                    from sklearn.metrics.pairwise import cosine_similarity\n                    sim = cosine_similarity(emb, kw_emb)\n                    embedding_score = float(np.max(sim))\n                else:\n                    sims = [cosine_similarity_fallback(emb, kw) for kw in kw_emb]\n                    embedding_score = max(sims) if sims else 0.0\n\n                # Keyword-based scoring (already in calculate_impact_score, but we want to avoid re-embedding)\n                # We'll just call calculate_impact_score but override the semantic part\n                # Actually, better to refactor calculate_impact_score to accept an optional semantic score\n                results.append(self._calculate_impact_with_semantic(message, embedding_score))\n\n            return results\n        except Exception as e:\n            logger.error(f\"Batch scoring failed: {e}\")\n            return [self.calculate_impact_score(m) for m in messages]\n\n    def _calculate_impact_with_semantic(self, message: Any, semantic_score: float) -> float:\n        \"\"\"Helper to calculate impact score with a pre-computed semantic score.\"\"\"\n        # This is a simplified version of calculate_impact_score logic\n        # For production, we should probably refactor calculate_impact_score to avoid duplication\n\n        # Re-use the keyword hits logic for the semantic part\n        text = self._extract_text_content(message).strip().lower()\n        hits = sum(1 for k in self.high_impact_keywords if k in text)\n        keyword_score = 0.1\n        if hits >= 5:\n            keyword_score = 1.0\n        elif hits >= 3:\n            keyword_score = 0.8\n        elif hits > 0:\n            keyword_score = 0.5\n\n        final_semantic = max(keyword_score, semantic_score)\n\n        # For the rest of the factors, we'll just call the individual methods\n        # To avoid code duplication, in a real scenario I'd refactor calculate_impact_score\n        # But here I'll just use the existing one for now as a fallback if this gets complex\n        return self.calculate_impact_score(message, context={\"semantic_override\": final_semantic})\n\n    async def calculate_impact(self, message: AgentMessage) -> float:\n        return self.calculate_impact_score(message)\n\n    def batch_score_impact(\n        self,\n        messages: List[Any],\n        contexts: Optional[List[Optional[Dict[str, Any]]]] = None,\n    ) -> List[float]:\n        \"\"\"\n        Process multiple messages efficiently with batching.\n\n        This method provides optimized batch inference for high-throughput scenarios.\n        When ONNX/BERT is enabled, it batches tokenization and inference operations\n        for better throughput. Falls back to sequential processing when ML is unavailable.\n\n        Args:\n            messages: List of messages to score. Each message can be a dict or AgentMessage.\n            contexts: Optional list of context dicts corresponding to each message.\n                     If None, empty contexts are used for all messages.\n\n        Returns:\n            List of impact scores (floats between 0.0 and 1.0).\n\n        Example:\n            >>> scorer = ImpactScorer()\n            >>> messages = [\n            ...     {\"content\": \"critical security alert\"},\n            ...     {\"content\": \"normal status check\"},\n            ... ]\n            >>> scores = scorer.batch_score_impact(messages)\n            >>> print(scores)  # [0.85, 0.25]\n        \"\"\"\n        if not messages:\n            return []\n\n        # Normalize contexts list\n        if contexts is None:\n            contexts = [None] * len(messages)\n        elif len(contexts) != len(messages):\n            raise ValueError(\n                f\"contexts length ({len(contexts)}) must match messages length ({len(messages)})\"\n            )\n\n        # Extract text content from all messages for batch processing\n        texts = [self._extract_text_content(msg) for msg in messages]\n\n        # If ONNX/BERT enabled with batch support, use optimized path\n        if self._onnx_enabled and self._bert_enabled and TRANSFORMERS_AVAILABLE:\n            return self._batch_score_with_embeddings(messages, texts, contexts)\n\n        # Fallback to sequential processing for keyword-based scoring\n        return self._batch_score_sequential(messages, contexts)\n\n    def _batch_score_with_embeddings(\n        self,\n        messages: List[Any],\n        texts: List[str],\n        contexts: List[Optional[Dict[str, Any]]],\n    ) -> List[float]:\n        \"\"\"\n        Batch scoring using BERT embeddings.\n\n        Performs batch tokenization with caching and inference for optimal throughput.\n        Uses _tokenize_batch for cached tokenization when texts repeat across batches.\n        \"\"\"\n        try:\n            # Filter out empty texts and track their indices\n            non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]\n            non_empty_texts = [texts[i] for i in non_empty_indices]\n\n            if not non_empty_texts:\n                # All texts are empty - return low scores\n                return [0.0] * len(messages)\n\n            # Batch tokenization with caching (uses _tokenize_batch helper)\n            batch_inputs = self._tokenize_batch(non_empty_texts, use_cache=True)\n            if batch_inputs is None:\n                # Tokenization failed, fall back to sequential\n                return self._batch_score_sequential(messages, contexts)\n\n            # Batch inference\n            with torch.no_grad():\n                outputs = self.model(**batch_inputs)\n                # Extract [CLS] token embeddings for each text\n                batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n\n            # Get keyword embeddings for similarity computation\n            keyword_embs = self._get_keyword_embeddings()\n\n            # Compute batch similarities using vectorized operations\n            similarities = cosine_similarity(batch_embeddings, keyword_embs)\n            max_similarities = np.max(similarities, axis=1)\n\n            # Build result array with semantic scores\n            semantic_scores = np.zeros(len(messages))\n            for idx, orig_idx in enumerate(non_empty_indices):\n                semantic_scores[orig_idx] = float(max_similarities[idx])\n\n            # Compute full impact scores for all messages\n            results = []\n            for i, msg in enumerate(messages):\n                ctx = contexts[i]\n                # Combine semantic score with other factors\n                score = self._compute_combined_score(msg, ctx, semantic_scores[i])\n                results.append(score)\n\n            return results\n\n        except Exception as e:\n            logger.debug(\"Batch embedding inference failed, falling back to sequential: %s\", e)\n            return self._batch_score_sequential(messages, contexts)\n\n    def _batch_score_sequential(\n        self,\n        messages: List[Any],\n        contexts: List[Optional[Dict[str, Any]]],\n    ) -> List[float]:\n        \"\"\"\n        Sequential scoring fallback for keyword-based processing.\n\n        Used when ONNX/BERT is not available or batch inference fails.\n        \"\"\"\n        return [\n            self.calculate_impact_score(msg, ctx)\n            for msg, ctx in zip(messages, contexts)\n        ]\n\n    def _compute_combined_score(\n        self,\n        message: Any,\n        context: Optional[Dict[str, Any]],\n        semantic_score: float,\n    ) -> float:\n        \"\"\"\n        Compute combined impact score from semantic and other factors.\n\n        Mirrors the logic in calculate_impact_score but uses pre-computed semantic score.\n        \"\"\"\n        agent_id = \"anonymous\"\n        if context and \"agent_id\" in context:\n            agent_id = context[\"agent_id\"]\n        elif isinstance(message, dict) and \"from_agent\" in message:\n            agent_id = message[\"from_agent\"]\n        elif hasattr(message, \"from_agent\"):\n            agent_id = getattr(message, \"from_agent\", \"anonymous\")\n\n        # Use pre-computed semantic score but also check keywords for fallback\n        text = self._extract_text_content(message).strip().lower()\n        keyword_score = 0.1\n        if text:\n            hits = sum(1 for k in self.high_impact_keywords if k in text)\n            if hits >= 5:\n                keyword_score = 1.0\n            elif hits >= 3:\n                keyword_score = 0.8\n            elif hits > 0:\n                keyword_score = 0.5\n\n        # Take max of semantic and keyword scores\n        final_semantic = max(semantic_score, keyword_score)\n\n        scores = {\n            \"semantic\": final_semantic,\n            \"permission\": self._calculate_permission_score(message),\n            \"volume\": self._calculate_volume_score(agent_id),\n            \"context\": self._calculate_context_score(message, context),\n            \"drift\": self._calculate_drift_score(agent_id, final_semantic),\n            \"priority\": self._calculate_priority_factor(message, context),\n            \"type\": self._calculate_type_factor(message, context),\n        }\n\n        weighted = sum(scores[k] * getattr(self.config, f\"{k}_weight\") for k in scores)\n\n        if scores[\"priority\"] >= 0.9:\n            weighted = max(weighted, self.config.critical_priority_boost)\n        if scores[\"semantic\"] >= 0.8:\n            weighted = max(weighted, self.config.high_semantic_boost)\n\n        return min(1.0, weighted)\n\n    async def batch_calculate_impact(\n        self,\n        messages: List[AgentMessage],\n    ) -> List[float]:\n        \"\"\"\n        Async wrapper for batch_score_impact.\n\n        Args:\n            messages: List of AgentMessage objects to score.\n\n        Returns:\n            List of impact scores.\n        \"\"\"\n        return self.batch_score_impact(messages)\n\n    def _get_embeddings(self, text: str) -> np.ndarray:\n        if not self._bert_enabled:\n            return np.zeros((1, 768))\n\n        try:\n            inputs = self.tokenizer(\n                text, return_tensors=\"np\" if self._onnx_enabled else \"pt\",\n                padding=True, truncation=True, max_length=512\n            )\n\n            if self._onnx_enabled and self.session:\n                # Prepare inputs for ONNX\n                onnx_inputs = {k: v for k, v in inputs.items()}\n                outputs = self.session.run(None, onnx_inputs)\n                # DistilBERT output is [last_hidden_state]\n                return outputs[0][:, 0, :]  # CLS token\n            else:\n                # Standard PyTorch BERT\n                import torch\n                with torch.no_grad():\n                    outputs = self.model(**inputs)\n                    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        except Exception as e:\n            logger.error(f\"Embedding generation failed: {e}\")\n            return np.zeros((1, 768))\n\n    def _onnx_inference(self, text: str) -> np.ndarray:\n        \"\"\"Run inference using ONNX Runtime.\"\"\"\n        if self.tokenizer is None or self.onnx_session is None:\n            return np.zeros((1, 768))\n\n        # Tokenize input\n        inputs = self.tokenizer(\n            text,\n            max_length=512,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"np\"\n        )\n\n        # Run ONNX inference\n        input_names = [i.name for i in self.onnx_session.get_inputs()]\n        onnx_inputs = {name: inputs[name] for name in input_names if name in inputs}\n\n        outputs = self.onnx_session.run(None, onnx_inputs)\n\n        # Extract [CLS] token embedding (first token)\n        # Output shape is typically (batch, seq_len, hidden_dim)\n        if len(outputs) > 0 and outputs[0].ndim == 3:\n            return outputs[0][:, 0, :]  # [CLS] token\n        elif len(outputs) > 0:\n            return outputs[0]\n\n        return np.zeros((1, 768))\n\n    def _transformer_inference(self, text: str) -> np.ndarray:\n        \"\"\"Run inference using PyTorch Transformers.\"\"\"\n        if self.tokenizer is None or self.model is None:\n            return np.zeros((1, 768))\n\n        import torch\n\n        # Tokenize input\n        inputs = self.tokenizer(\n            text,\n            max_length=512,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        # Run inference without gradient computation\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            # Get [CLS] token embedding (first token of last hidden state)\n            embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n\n        return embeddings\n\n    def _get_keyword_embeddings(self) -> np.ndarray:\n        if self._keyword_embeddings is None:\n            if not self._bert_enabled:\n                self._keyword_embeddings = np.zeros((len(self.high_impact_keywords), 768))\n            else:\n                embs = []\n                for kw in self.high_impact_keywords:\n                    embs.append(self._get_embeddings(kw))\n                self._keyword_embeddings = np.vstack(embs)\n        return self._keyword_embeddings\n\n    def score_batch(self, texts: List[str], reference_texts: Optional[List[str]] = None) -> List[float]:\n        \"\"\"\n        Process multiple texts efficiently with batching.\n\n        Args:\n            texts: List of texts to score\n            reference_texts: Optional reference texts (uses keywords if None)\n\n        Returns:\n            List of impact scores\n        \"\"\"\n        self._ensure_model_loaded()\n\n        # Use batch tokenization if Transformers available\n        if self._bert_enabled and self.tokenizer is not None and self.model is not None:\n            try:\n                import torch\n\n                # Batch tokenization\n                inputs = self.tokenizer(\n                    texts,\n                    max_length=512,\n                    truncation=True,\n                    padding=\"max_length\",\n                    return_tensors=\"pt\"\n                )\n\n                # Batch inference\n                with torch.no_grad():\n                    outputs = self.model(**inputs)\n                    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n\n                # Compute similarities against keyword embeddings\n                kw_emb = self._get_keyword_embeddings()\n                from sklearn.metrics.pairwise import cosine_similarity\n                sims = cosine_similarity(embeddings, kw_emb)\n\n                # Return max similarity as score\n                return [float(np.max(sim)) for sim in sims]\n\n            except Exception as e:\n                logger.warning(f\"Batch inference failed: {e}\")\n\n        # Fallback to sequential processing\n        return [self._calculate_semantic_score({\"content\": text}) for text in texts]\n\n\ndef cosine_similarity_fallback(a: Any, b: Any) -> float:\n    try:\n        a = np.array(a).flatten()\n        b = np.array(b).flatten()\n        if a.size == 0 or b.size == 0:\n            return 0.0\n        norm_a, norm_b = np.linalg.norm(a), np.linalg.norm(b)\n        if norm_a == 0 or norm_b == 0:\n            return 0.0\n        return float(np.dot(a, b) / (norm_a * norm_b))\n    except Exception:\n        return 0.0\n\n\ndef get_gpu_decision_matrix():\n    return {}\n\n\ndef get_reasoning_matrix():\n    return {}\n\n\ndef get_risk_profile():\n    return {}\n\n\ndef get_profiling_report():\n    return {}\n\n\ndef get_vector_space_metrics():\n    return {}\n\n\ndef reset_impact_scorer():\n    \"\"\"Reset global scorer and clear model caches.\"\"\"\n    global _global_scorer\n    _global_scorer = None\n    # Also reset class-level tokenizer/model cache\n    ImpactScorer.reset_class_cache()\n\n    # Clear class-level model caches\n    ImpactScorer._model_instance = None\n    ImpactScorer._tokenizer_instance = None\n    ImpactScorer._onnx_session_instance = None\n\n    logger.info(\"Impact scorer and model caches reset\")\n\n    # Clear class-level model caches\n    ImpactScorer._model_instance = None\n    ImpactScorer._tokenizer_instance = None\n    ImpactScorer._onnx_session_instance = None\n\n    logger.info(\"Impact scorer and model caches reset\")\n\n\ndef reset_profiling():\n    \"\"\"Reset profiling state (placeholder for future profiling features).\"\"\"\n    pass\n\n\ndef get_ml_backend_status() -> Dict[str, Any]:\n    \"\"\"\n    Get current ML backend status for diagnostics.\n\n    Returns:\n        Dict with backend availability and configuration info\n    \"\"\"\n    return {\n        \"onnx_available\": ONNX_AVAILABLE,\n        \"transformers_available\": TRANSFORMERS_AVAILABLE,\n        \"use_onnx\": USE_ONNX,\n        \"use_transformers\": USE_TRANSFORMERS,\n        \"model_cached\": ImpactScorer._model_instance is not None,\n        \"onnx_cached\": ImpactScorer._onnx_session_instance is not None,\n    }\n\n\n_global_scorer = None\n\n\ndef get_impact_scorer(**kwargs):\n    global _global_scorer\n    if not _global_scorer:\n        _global_scorer = ImpactScorer(**kwargs)\n    return _global_scorer\n\n\ndef calculate_message_impact(message: AgentMessage) -> float:\n    return get_impact_scorer().calculate_impact_score(message)\n\nasync def calculate_message_impact_async(message: AgentMessage) -> float:\n    return await get_impact_scorer().calculate_impact_score_async(message)\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.324373",
  "last_updated": "2026-01-04T05:35:58.598856"
}