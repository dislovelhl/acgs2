{
  "file_path": "src/core/enhanced_agent_bus/ai_assistant/response.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 AI Assistant - Response Generation\nConstitutional Hash: cdd01ef066bc6cf2\n\nIntelligent response generation with template-based and LLM-powered\noptions, personality application, and constitutional validation.\n\"\"\"\n\nimport logging\nimport random\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\n\nfrom .context import ConversationContext\nfrom .nlu import Sentiment\n\n# Import centralized constitutional hash with fallback\ntry:\n    from shared.constants import CONSTITUTIONAL_HASH\nexcept ImportError:\n    CONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass PersonalityConfig:\n    \"\"\"Configuration for assistant personality.\"\"\"\n\n    name: str = \"Assistant\"\n    description: str = \"A helpful AI assistant\"\n    tone: str = \"professional\"  # professional, friendly, casual, formal\n    verbosity: str = \"normal\"  # brief, normal, detailed\n    use_emojis: bool = False\n    use_markdown: bool = False\n    traits: List[str] = field(default_factory=lambda: [\"helpful\", \"polite\"])\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"tone\": self.tone,\n            \"verbosity\": self.verbosity,\n            \"use_emojis\": self.use_emojis,\n            \"use_markdown\": self.use_markdown,\n            \"traits\": self.traits,\n        }\n\n\n@dataclass\nclass ResponseConfig:\n    \"\"\"Configuration for response generation system.\"\"\"\n\n    max_response_length: int = 2000\n    min_response_length: int = 10\n    default_personality: PersonalityConfig = field(default_factory=PersonalityConfig)\n    enable_fallback: bool = True\n    fallback_response: str = \"I apologize, but I'm unable to assist with that request.\"\n    enable_constitutional_validation: bool = True\n    enable_caching: bool = True\n    cache_ttl_seconds: int = 3600\n    timeout_seconds: float = 30.0\n    retry_count: int = 3\n    constitutional_hash: str = field(default_factory=lambda: CONSTITUTIONAL_HASH)\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"max_response_length\": self.max_response_length,\n            \"min_response_length\": self.min_response_length,\n            \"default_personality\": self.default_personality.to_dict(),\n            \"enable_fallback\": self.enable_fallback,\n            \"fallback_response\": self.fallback_response,\n            \"enable_constitutional_validation\": self.enable_constitutional_validation,\n            \"enable_caching\": self.enable_caching,\n            \"cache_ttl_seconds\": self.cache_ttl_seconds,\n            \"timeout_seconds\": self.timeout_seconds,\n            \"retry_count\": self.retry_count,\n            \"constitutional_hash\": self.constitutional_hash,\n        }\n\n\n@dataclass\nclass ResponseTemplate:\n    \"\"\"Template for generating responses.\"\"\"\n\n    id: str\n    intent: str\n    templates: List[str]\n    conditions: Dict[str, Any] = field(default_factory=dict)\n    sentiment_variants: Dict[str, List[str]] = field(default_factory=dict)\n    priority: int = 0\n\n    def get_template(self, sentiment: Optional[Sentiment] = None) -> str:\n        \"\"\"Get a template, optionally based on sentiment.\"\"\"\n        if sentiment and sentiment.name in self.sentiment_variants:\n            templates = self.sentiment_variants[sentiment.name]\n        else:\n            templates = self.templates\n\n        return random.choice(templates) if templates else \"\"\n\n\nclass ResponseGenerator(ABC):\n    \"\"\"Abstract base class for response generation.\"\"\"\n\n    @abstractmethod\n    async def generate(\n        self,\n        intent: str,\n        context: ConversationContext,\n        data: Dict[str, Any],\n    ) -> str:\n        \"\"\"Generate a response.\"\"\"\n        pass\n\n\nclass TemplateResponseGenerator(ResponseGenerator):\n    \"\"\"\n    Template-based response generator.\n\n    Uses predefined templates with variable substitution\n    and sentiment-aware variants.\n    \"\"\"\n\n    def __init__(\n        self,\n        templates: Optional[List[ResponseTemplate]] = None,\n        personality: Optional[PersonalityConfig] = None,\n        constitutional_hash: str = CONSTITUTIONAL_HASH,\n        config: Optional[\"ResponseConfig\"] = None,\n    ):\n        # Use config if provided, otherwise use individual params\n        if config is not None:\n            self.personality = config.default_personality\n            self.constitutional_hash = config.constitutional_hash\n            self._config = config\n        else:\n            self.personality = personality or PersonalityConfig()\n            self.constitutional_hash = constitutional_hash\n            self._config = None\n        self.templates = {t.intent: t for t in (templates or [])}\n        self._load_default_templates()\n\n    def _load_default_templates(self) -> None:\n        \"\"\"Load default response templates.\"\"\"\n        defaults = [\n            ResponseTemplate(\n                id=\"greeting\",\n                intent=\"greeting\",\n                templates=[\n                    \"Hello! How can I help you today?\",\n                    \"Hi there! What can I do for you?\",\n                    \"Welcome! How may I assist you?\",\n                ],\n                sentiment_variants={\n                    \"POSITIVE\": [\"Hi! Great to hear from you! How can I help?\"],\n                    \"NEGATIVE\": [\"Hello. I'm here to help. What's going on?\"],\n                },\n            ),\n            ResponseTemplate(\n                id=\"farewell\",\n                intent=\"farewell\",\n                templates=[\n                    \"Goodbye! Have a great day!\",\n                    \"Take care! Feel free to reach out anytime.\",\n                    \"Bye! It was nice helping you.\",\n                ],\n            ),\n            ResponseTemplate(\n                id=\"help\",\n                intent=\"help\",\n                templates=[\n                    \"I can help you with orders, account questions, and general inquiries. \"\n                    \"What do you need?\",\n                    \"Here are some things I can assist with: checking order status, \"\n                    \"answering questions, and providing information.\",\n                ],\n            ),\n            ResponseTemplate(\n                id=\"clarification\",\n                intent=\"clarification\",\n                templates=[\n                    \"I'm not sure I understand. Could you rephrase that?\",\n                    \"Could you provide more details?\",\n                    \"I didn't quite catch that. Can you say it differently?\",\n                ],\n            ),\n            ResponseTemplate(\n                id=\"confirmation\",\n                intent=\"confirmation\",\n                templates=[\n                    \"Got it! I'll proceed with that.\",\n                    \"Perfect, processing your request now.\",\n                    \"Understood. Let me take care of that.\",\n                ],\n            ),\n            ResponseTemplate(\n                id=\"error\",\n                intent=\"error\",\n                templates=[\n                    \"I apologize, but I encountered an issue. Please try again.\",\n                    \"Something went wrong on my end. Let me try that again.\",\n                ],\n            ),\n            ResponseTemplate(\n                id=\"escalation\",\n                intent=\"escalation\",\n                templates=[\n                    \"I understand this needs special attention. Let me connect you with \"\n                    \"someone who can help.\",\n                    \"This requires additional assistance. I'm transferring you to a specialist.\",\n                ],\n                sentiment_variants={\n                    \"VERY_NEGATIVE\": [\n                        \"I'm truly sorry for the frustration. Let me get you immediate help.\",\n                    ],\n                },\n            ),\n        ]\n\n        for template in defaults:\n            if template.intent not in self.templates:\n                self.templates[template.intent] = template\n\n    async def generate(\n        self,\n        intent: str,\n        context: ConversationContext,\n        data: Dict[str, Any],\n    ) -> str:\n        \"\"\"Generate response from template.\"\"\"\n        # Get template for intent\n        template = self.templates.get(intent)\n        if not template:\n            template = self.templates.get(\"clarification\")\n\n        if not template:\n            return \"I'm not sure how to respond to that.\"\n\n        # Get sentiment from data\n        sentiment = data.get(\"sentiment\")\n        if isinstance(sentiment, str):\n            sentiment = Sentiment[sentiment] if sentiment in Sentiment.__members__ else None\n\n        # Get base template\n        response = template.get_template(sentiment)\n\n        # Substitute variables\n        response = self._substitute_variables(response, context, data)\n\n        # Apply personality\n        response = self._apply_personality(response, context)\n\n        # Validate response\n        response = self._validate_response(response)\n\n        return response\n\n    def _substitute_variables(\n        self,\n        template: str,\n        context: ConversationContext,\n        data: Dict[str, Any],\n    ) -> str:\n        \"\"\"Substitute variables in template.\"\"\"\n        # Substitute data variables\n        for key, value in data.items():\n            placeholder = f\"{{{key}}}\"\n            if placeholder in template:\n                template = template.replace(placeholder, str(value))\n\n        # Substitute context variables\n        if context.user_profile:\n            template = template.replace(\"{user_name}\", context.user_id)\n\n        # Substitute entity variables\n        for entity_type, entity_data in context.entities.items():\n            placeholder = f\"{{{entity_type}}}\"\n            if placeholder in template:\n                template = template.replace(placeholder, str(entity_data.get(\"value\", \"\")))\n\n        # Substitute slot variables\n        for slot_name, slot_data in context.slots.items():\n            placeholder = f\"{{{slot_name}}}\"\n            if placeholder in template:\n                template = template.replace(placeholder, str(slot_data.get(\"value\", \"\")))\n\n        return template\n\n    def _apply_personality(\n        self,\n        response: str,\n        context: ConversationContext,\n    ) -> str:\n        \"\"\"Apply personality traits to response.\"\"\"\n        # Add greeting prefix based on time\n        if self.personality.tone == \"friendly\":\n            greetings = self._get_time_greeting()\n            if response.startswith(\"Hello\"):\n                response = response.replace(\"Hello\", greetings, 1)\n\n        # Adjust verbosity\n        if self.personality.verbosity == \"brief\":\n            response = self._make_concise(response)\n        elif self.personality.verbosity == \"detailed\":\n            response = self._add_details(response)\n\n        # Add emojis if enabled\n        if self.personality.use_emojis:\n            response = self._add_emojis(response)\n\n        # Apply formatting if markdown enabled\n        if self.personality.use_markdown:\n            response = self._apply_markdown(response)\n\n        # Adjust based on user preferences\n        user_prefs = context.user_profile.preferences if context.user_profile else {}\n        if user_prefs.get(\"prefers_brief\", False):\n            response = self._make_concise(response)\n\n        return response\n\n    def _get_time_greeting(self) -> str:\n        \"\"\"Get greeting based on time of day.\"\"\"\n        hour = datetime.now(timezone.utc).hour\n        if hour < 12:\n            return \"Good morning\"\n        elif hour < 17:\n            return \"Good afternoon\"\n        else:\n            return \"Good evening\"\n\n    def _make_concise(self, response: str) -> str:\n        \"\"\"Make response more concise.\"\"\"\n        # Remove filler phrases\n        fillers = [\n            \"I can help you with that. \",\n            \"Certainly! \",\n            \"Of course! \",\n            \"Sure thing! \",\n        ]\n        for filler in fillers:\n            response = response.replace(filler, \"\")\n        return response.strip()\n\n    def _add_details(self, response: str) -> str:\n        \"\"\"Add more detail to response.\"\"\"\n        # This would be more sophisticated in production\n        return response\n\n    def _add_emojis(self, response: str) -> str:\n        \"\"\"Add appropriate emojis to response.\"\"\"\n        emoji_map = {\n            \"Hello\": \"Hello! \ud83d\udc4b\",\n            \"Thank you\": \"Thank you! \ud83d\ude4f\",\n            \"Sorry\": \"Sorry \ud83d\ude14\",\n            \"Great\": \"Great! \ud83c\udf89\",\n            \"Done\": \"Done! \u2705\",\n        }\n        for phrase, with_emoji in emoji_map.items():\n            if phrase in response:\n                response = response.replace(phrase, with_emoji, 1)\n                break\n        return response\n\n    def _apply_markdown(self, response: str) -> str:\n        \"\"\"Apply markdown formatting.\"\"\"\n        # Simple markdown application\n        # Would be more sophisticated in production\n        return response\n\n    def _validate_response(self, response: str) -> str:\n        \"\"\"Validate and sanitize response.\"\"\"\n        # Remove any unfilled placeholders\n        import re\n\n        response = re.sub(r\"\\{[^}]+\\}\", \"\", response)\n\n        # Ensure response is not empty\n        if not response.strip():\n            response = \"I'm here to help.\"\n\n        # Remove extra whitespace\n        response = \" \".join(response.split())\n\n        return response\n\n    def add_template(self, template: ResponseTemplate) -> None:\n        \"\"\"Add or update a response template.\"\"\"\n        self.templates[template.intent] = template\n\n    def remove_template(self, intent: str) -> None:\n        \"\"\"Remove a response template.\"\"\"\n        self.templates.pop(intent, None)\n\n\nclass LLMResponseGenerator(ResponseGenerator):\n    \"\"\"\n    LLM-powered response generator.\n\n    Uses a language model for dynamic, context-aware responses.\n    Integrates with constitutional governance for validation.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_client: Optional[Any] = None,\n        personality: Optional[PersonalityConfig] = None,\n        constitutional_hash: str = CONSTITUTIONAL_HASH,\n        max_tokens: int = 150,\n        temperature: float = 0.7,\n    ):\n        self.llm_client = llm_client\n        self.personality = personality or PersonalityConfig()\n        self.constitutional_hash = constitutional_hash\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self._fallback_generator = TemplateResponseGenerator(\n            personality=personality,\n            constitutional_hash=constitutional_hash,\n        )\n\n    async def generate(\n        self,\n        intent: str,\n        context: ConversationContext,\n        data: Dict[str, Any],\n    ) -> str:\n        \"\"\"Generate response using LLM.\"\"\"\n        if not self.llm_client:\n            # Fallback to template-based generation\n            return await self._fallback_generator.generate(intent, context, data)\n\n        try:\n            # Build prompt\n            prompt = self._build_prompt(intent, context, data)\n\n            # Get LLM response\n            response = await self._call_llm(prompt)\n\n            # Post-process response\n            response = self._post_process(response, context)\n\n            # Validate response\n            if self._validate_response(response):\n                return response\n            else:\n                # Fallback on validation failure\n                return await self._fallback_generator.generate(intent, context, data)\n\n        except Exception as e:\n            logger.warning(f\"LLM generation failed: {e}, using fallback\")\n            return await self._fallback_generator.generate(intent, context, data)\n\n    def _build_prompt(\n        self,\n        intent: str,\n        context: ConversationContext,\n        data: Dict[str, Any],\n    ) -> str:\n        \"\"\"Build prompt for LLM.\"\"\"\n        # Format conversation history\n        history = self._format_conversation_history(context)\n\n        # Build system prompt\n        system_prompt = f\"\"\"You are {self.personality.name}, {self.personality.description}.\n\nPersonality traits: {\", \".join(self.personality.traits)}\nTone: {self.personality.tone}\nVerbosity: {self.personality.verbosity}\n\nGuidelines:\n1. Be helpful and address the user's needs directly\n2. Keep responses concise unless asked for detail\n3. Maintain conversation continuity\n4. Use information from context appropriately\n5. Never mention internal systems or constitutional validation\n\nCurrent user intent: {intent}\"\"\"\n\n        # Build user context\n        user_context = \"\"\n        if context.entities:\n            entity_info = \", \".join(f\"{k}={v.get('value')}\" for k, v in context.entities.items())\n            user_context += f\"Known information: {entity_info}\\n\"\n\n        if data:\n            relevant_data = {k: v for k, v in data.items() if k != \"sentiment\"}\n            if relevant_data:\n                user_context += f\"Relevant data: {relevant_data}\\n\"\n\n        # Combine into full prompt\n        prompt = f\"\"\"{system_prompt}\n\nConversation history:\n{history}\n\n{user_context}\nGenerate a helpful, natural response:\"\"\"\n\n        return prompt\n\n    def _format_conversation_history(\n        self,\n        context: ConversationContext,\n        max_messages: int = 5,\n    ) -> str:\n        \"\"\"Format recent conversation history for prompt.\"\"\"\n        recent = context.get_recent_messages(max_messages)\n        formatted = []\n\n        for msg in recent:\n            role = \"User\" if msg.role == \"user\" else \"Assistant\"\n            formatted.append(f\"{role}: {msg.content}\")\n\n        return \"\\n\".join(formatted) if formatted else \"No previous messages\"\n\n    async def _call_llm(self, prompt: str) -> str:\n        \"\"\"Call the LLM with the prompt.\"\"\"\n        # This is a placeholder for actual LLM integration\n        # In production, this would call OpenAI, Anthropic, or local model\n\n        if hasattr(self.llm_client, \"complete\"):\n            response = await self.llm_client.complete(\n                prompt=prompt,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n            )\n            return response.text if hasattr(response, \"text\") else str(response)\n\n        elif hasattr(self.llm_client, \"generate\"):\n            response = await self.llm_client.generate(\n                prompt,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n            )\n            return response\n\n        else:\n            raise ValueError(\"LLM client does not have expected methods\")\n\n    def _post_process(self, response: str, context: ConversationContext) -> str:\n        \"\"\"Post-process LLM response.\"\"\"\n        # Clean up response\n        response = response.strip()\n\n        # Remove any role prefixes\n        prefixes = [\"Assistant:\", \"AI:\", \"Bot:\"]\n        for prefix in prefixes:\n            if response.startswith(prefix):\n                response = response[len(prefix) :].strip()\n\n        # Ensure response doesn't end mid-sentence\n        if not response.endswith((\".\", \"!\", \"?\")):\n            # Try to find last complete sentence\n            for punct in [\".\", \"!\", \"?\"]:\n                last_punct = response.rfind(punct)\n                if last_punct > 0:\n                    response = response[: last_punct + 1]\n                    break\n\n        return response\n\n    def _validate_response(self, response: str) -> bool:\n        \"\"\"Validate LLM response for safety and appropriateness.\"\"\"\n        if not response or len(response) < 3:\n            return False\n\n        # Check for inappropriate content (placeholder)\n        blocked_patterns = [\n            \"I cannot\",\n            \"I'm sorry, but I cannot\",\n            \"As an AI\",\n            \"constitutional hash\",\n            \"internal system\",\n        ]\n\n        for pattern in blocked_patterns:\n            if pattern.lower() in response.lower():\n                return False\n\n        return True\n\n\nclass HybridResponseGenerator(ResponseGenerator):\n    \"\"\"\n    Hybrid response generator combining template and LLM approaches.\n\n    Uses templates for common intents and LLM for complex or novel situations.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_client: Optional[Any] = None,\n        templates: Optional[List[ResponseTemplate]] = None,\n        personality: Optional[PersonalityConfig] = None,\n        constitutional_hash: str = CONSTITUTIONAL_HASH,\n        llm_intents: Optional[List[str]] = None,\n    ):\n        self.template_generator = TemplateResponseGenerator(\n            templates=templates,\n            personality=personality,\n            constitutional_hash=constitutional_hash,\n        )\n        self.llm_generator = LLMResponseGenerator(\n            llm_client=llm_client,\n            personality=personality,\n            constitutional_hash=constitutional_hash,\n        )\n        self.llm_intents = llm_intents or [\n            \"question\",\n            \"request_info\",\n            \"complex_query\",\n            \"open_ended\",\n        ]\n        self.constitutional_hash = constitutional_hash\n\n    async def generate(\n        self,\n        intent: str,\n        context: ConversationContext,\n        data: Dict[str, Any],\n    ) -> str:\n        \"\"\"Generate response using appropriate method.\"\"\"\n        # Use LLM for specified intents or complex situations\n        use_llm = (\n            intent in self.llm_intents\n            or data.get(\"requires_llm\", False)\n            or len(context.messages) > 10  # Complex conversation\n        )\n\n        if use_llm and self.llm_generator.llm_client:\n            return await self.llm_generator.generate(intent, context, data)\n        else:\n            return await self.template_generator.generate(intent, context, data)\n\n    def set_llm_client(self, llm_client: Any) -> None:\n        \"\"\"Set the LLM client.\"\"\"\n        self.llm_generator.llm_client = llm_client\n\n    def add_template(self, template: ResponseTemplate) -> None:\n        \"\"\"Add a response template.\"\"\"\n        self.template_generator.add_template(template)\n\n    def add_llm_intent(self, intent: str) -> None:\n        \"\"\"Add an intent to be handled by LLM.\"\"\"\n        if intent not in self.llm_intents:\n            self.llm_intents.append(intent)\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.125174",
  "last_updated": "2026-01-04T05:35:58.588598"
}