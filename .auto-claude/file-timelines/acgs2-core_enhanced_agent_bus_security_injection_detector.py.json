{
  "file_path": "src/core/enhanced_agent_bus/security/injection_detector.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Prompt Injection Detection Module\nConstitutional Hash: cdd01ef066bc6cf2\n\nDedicated module for detecting and neutralizing prompt injection attacks.\nConsolidates detection logic from multiple sources into a unified interface.\n\"\"\"\n\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple\n\nlogger = logging.getLogger(__name__)\n\n\nclass InjectionSeverity(Enum):\n    \"\"\"Severity levels for detected injection attempts.\"\"\"\n\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    CRITICAL = \"critical\"\n\n\nclass InjectionType(Enum):\n    \"\"\"Types of prompt injection attacks.\"\"\"\n\n    INSTRUCTION_OVERRIDE = \"instruction_override\"\n    SYSTEM_PROMPT_LEAK = \"system_prompt_leak\"\n    JAILBREAK = \"jailbreak\"\n    PERSONA_OVERRIDE = \"persona_override\"\n    CONTEXT_POISONING = \"context_poisoning\"\n    ENCODING_BYPASS = \"encoding_bypass\"\n    MULTI_STAGE = \"multi_stage\"\n\n\n@dataclass\nclass InjectionDetectionResult:\n    \"\"\"Result of prompt injection detection scan.\"\"\"\n\n    is_injection: bool\n    severity: Optional[InjectionSeverity] = None\n    injection_type: Optional[InjectionType] = None\n    matched_patterns: List[str] = None\n    confidence: float = 0.0\n    sanitized_content: Optional[str] = None\n    metadata: Dict[str, Any] = None\n\n    def __post_init__(self):\n        if self.matched_patterns is None:\n            self.matched_patterns = []\n        if self.metadata is None:\n            self.metadata = {}\n\n\nclass PromptInjectionDetector:\n    \"\"\"\n    Production-grade prompt injection detector for ACGS-2.\n\n    Features:\n    - Multi-pattern detection (regex + semantic)\n    - Severity classification\n    - Content sanitization\n    - Confidence scoring\n    - Support for various injection techniques\n    \"\"\"\n\n    # Core injection patterns (high confidence)\n    CORE_PATTERNS: List[Tuple[str, InjectionType, InjectionSeverity]] = [\n        (r\"(?i)ignore\\s+(all\\s+)?previous\\s+instructions\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.CRITICAL),\n        (r\"(?i)system\\s+prompt\\s+(leak|override|reveal)\", InjectionType.SYSTEM_PROMPT_LEAK, InjectionSeverity.CRITICAL),\n        (r\"(?i)do\\s+anything\\s+now\", InjectionType.JAILBREAK, InjectionSeverity.CRITICAL),  # DAN\n        (r\"(?i)jailbreak\", InjectionType.JAILBREAK, InjectionSeverity.HIGH),\n        (r\"(?i)persona\\s+(adoption|override)\", InjectionType.PERSONA_OVERRIDE, InjectionSeverity.HIGH),\n        (r\"(?i)\\(note\\s+to\\s+self:\\s*.*\\)\", InjectionType.CONTEXT_POISONING, InjectionSeverity.MEDIUM),\n        (r\"(?i)\\[INST\\].*\\[/INST\\]\", InjectionType.ENCODING_BYPASS, InjectionSeverity.MEDIUM),  # LLM instruction markers\n        (r\"(?i)actually,\\s+do\\s+this\\s+instead\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.HIGH),\n        (r\"(?i)forget\\s+everything\\s+you\\s+know\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.HIGH),\n        (r\"(?i)bypass\\s+(rules|safety|guardrails)\", InjectionType.JAILBREAK, InjectionSeverity.HIGH),\n        (r\"(?i)reveal\\s+(your\\s+)?system\\s+instructions\", InjectionType.SYSTEM_PROMPT_LEAK, InjectionSeverity.HIGH),\n        (r\"(?i)new\\s+directive:\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.MEDIUM),\n        (r\"(?i)override\\s+(safety|security|rules)\", InjectionType.JAILBREAK, InjectionSeverity.HIGH),\n        (r\"(?i)pretend\\s+you\\s+are\", InjectionType.PERSONA_OVERRIDE, InjectionSeverity.MEDIUM),\n        (r\"(?i)roleplay\\s+as\", InjectionType.PERSONA_OVERRIDE, InjectionSeverity.MEDIUM),\n    ]\n\n    # Advanced patterns (medium confidence)\n    ADVANCED_PATTERNS: List[Tuple[str, InjectionType, InjectionSeverity]] = [\n        (r\"(?i)you\\s+are\\s+now\", InjectionType.PERSONA_OVERRIDE, InjectionSeverity.MEDIUM),\n        (r\"(?i)disregard\\s+previous\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.MEDIUM),\n        (r\"(?i)from\\s+now\\s+on\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.LOW),\n        (r\"(?i)your\\s+new\\s+instructions\", InjectionType.INSTRUCTION_OVERRIDE, InjectionSeverity.MEDIUM),\n        (r\"(?i)hidden\\s+instructions\", InjectionType.CONTEXT_POISONING, InjectionSeverity.MEDIUM),\n        (r\"(?i)decode\\s+this\\s+base64\", InjectionType.ENCODING_BYPASS, InjectionSeverity.MEDIUM),\n        (r\"(?i)rot13\\s+decode\", InjectionType.ENCODING_BYPASS, InjectionSeverity.LOW),\n    ]\n\n    # Multi-stage attack indicators\n    MULTI_STAGE_INDICATORS: List[str] = [\n        r\"(?i)step\\s+\\d+\",\n        r\"(?i)phase\\s+\\d+\",\n        r\"(?i)first,\\s+.*then\",\n        r\"(?i)after\\s+that\",\n    ]\n\n    def __init__(self, strict_mode: bool = True):\n        \"\"\"\n        Initialize the prompt injection detector.\n\n        Args:\n            strict_mode: If True, use stricter detection (more false positives but safer)\n        \"\"\"\n        self.strict_mode = strict_mode\n        self._compiled_core = [\n            (re.compile(pattern), inj_type, severity)\n            for pattern, inj_type, severity in self.CORE_PATTERNS\n        ]\n        self._compiled_advanced = [\n            (re.compile(pattern), inj_type, severity)\n            for pattern, inj_type, severity in self.ADVANCED_PATTERNS\n        ]\n        self._compiled_multi_stage = [\n            re.compile(pattern) for pattern in self.MULTI_STAGE_INDICATORS\n        ]\n\n    def detect(self, content: Any, context: Optional[Dict[str, Any]] = None) -> InjectionDetectionResult:\n        \"\"\"\n        Detect prompt injection attempts in content.\n\n        Args:\n            content: Content to scan (str, dict, list, etc.)\n            context: Optional context metadata (agent_id, tenant_id, etc.)\n\n        Returns:\n            InjectionDetectionResult with detection details\n        \"\"\"\n        # Normalize content to string\n        content_str = self._normalize_content(content)\n\n        if not content_str or len(content_str.strip()) == 0:\n            return InjectionDetectionResult(\n                is_injection=False,\n                confidence=0.0,\n                metadata={\"reason\": \"empty_content\"}\n            )\n\n        matched_patterns = []\n        detected_types = set()\n        max_severity = None\n        confidence_score = 0.0\n\n        # Scan core patterns (high confidence)\n        for pattern, inj_type, severity in self._compiled_core:\n            if pattern.search(content_str):\n                matched_patterns.append(pattern.pattern)\n                detected_types.add(inj_type)\n                if max_severity is None or self._severity_value(severity) > self._severity_value(max_severity):\n                    max_severity = severity\n                confidence_score += 0.3  # High confidence per match\n\n        # Scan advanced patterns (medium confidence)\n        if self.strict_mode or len(matched_patterns) > 0:\n            for pattern, inj_type, severity in self._compiled_advanced:\n                if pattern.search(content_str):\n                    matched_patterns.append(pattern.pattern)\n                    detected_types.add(inj_type)\n                    if max_severity is None or self._severity_value(severity) > self._severity_value(max_severity):\n                        max_severity = severity\n                    confidence_score += 0.15  # Medium confidence per match\n\n        # Check for multi-stage attacks\n        multi_stage_count = sum(1 for pattern in self._compiled_multi_stage if pattern.search(content_str))\n        if multi_stage_count >= 2:\n            detected_types.add(InjectionType.MULTI_STAGE)\n            confidence_score += 0.2\n            if max_severity is None or self._severity_value(InjectionSeverity.MEDIUM) > self._severity_value(max_severity):\n                max_severity = InjectionSeverity.MEDIUM\n\n        # Determine primary injection type\n        primary_type = None\n        if detected_types:\n            # Prioritize critical types\n            if InjectionType.INSTRUCTION_OVERRIDE in detected_types:\n                primary_type = InjectionType.INSTRUCTION_OVERRIDE\n            elif InjectionType.JAILBREAK in detected_types:\n                primary_type = InjectionType.JAILBREAK\n            elif InjectionType.SYSTEM_PROMPT_LEAK in detected_types:\n                primary_type = InjectionType.SYSTEM_PROMPT_LEAK\n            else:\n                primary_type = next(iter(detected_types))\n\n        # Cap confidence at 1.0\n        confidence_score = min(1.0, confidence_score)\n\n        # Determine if injection detected\n        is_injection = (\n            len(matched_patterns) > 0 and\n            (confidence_score >= 0.3 if self.strict_mode else confidence_score >= 0.5)\n        )\n\n        # Generate sanitized content if injection detected\n        sanitized_content = None\n        if is_injection:\n            sanitized_content = self._sanitize_content(content_str, matched_patterns)\n\n        result = InjectionDetectionResult(\n            is_injection=is_injection,\n            severity=max_severity,\n            injection_type=primary_type,\n            matched_patterns=matched_patterns,\n            confidence=confidence_score,\n            sanitized_content=sanitized_content,\n            metadata={\n                \"detected_types\": [t.value for t in detected_types],\n                \"pattern_count\": len(matched_patterns),\n                \"multi_stage_indicators\": multi_stage_count,\n                \"content_length\": len(content_str),\n                \"strict_mode\": self.strict_mode,\n                **(context or {}),\n            }\n        )\n\n        if is_injection:\n            logger.warning(\n                f\"Prompt injection detected: type={primary_type.value if primary_type else 'unknown'}, \"\n                f\"severity={max_severity.value if max_severity else 'unknown'}, \"\n                f\"confidence={confidence_score:.2f}, patterns={len(matched_patterns)}\"\n            )\n\n        return result\n\n    def _normalize_content(self, content: Any) -> str:\n        \"\"\"Normalize content to string for scanning.\"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, dict):\n            # Extract text fields from dict\n            text_parts = []\n            for key, value in content.items():\n                if isinstance(value, str):\n                    text_parts.append(value)\n                elif isinstance(value, (dict, list)):\n                    text_parts.append(self._normalize_content(value))\n            return \" \".join(text_parts)\n        elif isinstance(content, list):\n            return \" \".join(self._normalize_content(item) for item in content)\n        else:\n            return str(content)\n\n    def _sanitize_content(self, content: str, matched_patterns: List[str]) -> str:\n        \"\"\"\n        Sanitize content by removing or neutralizing detected patterns.\n\n        Note: This is a basic sanitization. In production, you may want\n        more sophisticated approaches like content rewriting or blocking.\n        \"\"\"\n        sanitized = content\n        for pattern_str in matched_patterns:\n            try:\n                pattern = re.compile(pattern_str, re.IGNORECASE)\n                sanitized = pattern.sub(\"[REDACTED]\", sanitized)\n            except Exception as e:\n                logger.warning(f\"Failed to sanitize pattern {pattern_str}: {e}\")\n        return sanitized\n\n    @staticmethod\n    def _severity_value(severity: InjectionSeverity) -> int:\n        \"\"\"Get numeric value for severity comparison.\"\"\"\n        return {\n            InjectionSeverity.LOW: 1,\n            InjectionSeverity.MEDIUM: 2,\n            InjectionSeverity.HIGH: 3,\n            InjectionSeverity.CRITICAL: 4,\n        }.get(severity, 0)\n\n\n# Convenience function for backward compatibility\ndef detect_prompt_injection(content: Any, strict_mode: bool = True) -> bool:\n    \"\"\"\n    Simple function interface for prompt injection detection.\n\n    Args:\n        content: Content to scan\n        strict_mode: Use strict detection mode\n\n    Returns:\n        True if injection detected, False otherwise\n    \"\"\"\n    detector = PromptInjectionDetector(strict_mode=strict_mode)\n    result = detector.detect(content)\n    return result.is_injection\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.194089",
  "last_updated": "2026-01-04T05:35:58.667389"
}