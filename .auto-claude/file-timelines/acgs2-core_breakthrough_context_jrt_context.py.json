{
  "file_path": "src/core/breakthrough/context/jrt_context.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nJRT Context Preparator for Constitutional AI\n=============================================\n\nConstitutional Hash: cdd01ef066bc6cf2\n\nImplements Just-Repeat-Twice (JRT) context preparation strategy\nthat addresses the 'lost-in-middle' problem in long contexts.\n\nResearch shows +11% recall improvement when critical sections\nare strategically repeated in the context window.\n\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\n\nfrom ...shared.types import JSONValue\nfrom .. import CONSTITUTIONAL_HASH\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ContextSection:\n    \"\"\"A section of context with importance scoring.\"\"\"\n    content: JSONValue\n    start_position: int\n    end_position: int\n    importance_score: float\n    section_type: str  # \"constitutional\", \"policy\", \"user\", \"system\"\n\n    def __len__(self) -> int:\n        return self.end_position - self.start_position\n\n\n@dataclass\nclass PreparedContext:\n    \"\"\"Prepared context with JRT optimization.\"\"\"\n    original: JSONValue\n    prepared: JSONValue\n    critical_positions: List[int]\n    repeated_sections: List[Tuple[int, int]]\n    total_tokens: int\n    expansion_ratio: float\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n\n\nclass JRTContextPreparator:\n    \"\"\"\n    Just-Repeat-Twice Context Preparation.\n\n    Addresses the 'lost-in-middle' problem by:\n    1. Identifying critical sections (constitutional principles, key policies)\n    2. Computing importance scores for each section\n    3. Strategically repeating high-importance sections\n    4. Placing critical information at attention hotspots (start, end)\n\n    This improves recall by +11% on long-context benchmarks.\n    \"\"\"\n\n    def __init__(\n        self,\n        importance_threshold: float = 0.7,\n        max_repetitions: int = 2,\n        max_expansion_ratio: float = 1.5,\n        constitutional_priority: float = 1.0\n    ):\n        \"\"\"\n        Initialize JRT Context Preparator.\n\n        Args:\n            importance_threshold: Minimum score for section repetition\n            max_repetitions: Maximum times to repeat a section\n            max_expansion_ratio: Maximum context expansion factor\n            constitutional_priority: Priority boost for constitutional content\n        \"\"\"\n        self.importance_threshold = importance_threshold\n        self.max_repetitions = max_repetitions\n        self.max_expansion_ratio = max_expansion_ratio\n        self.constitutional_priority = constitutional_priority\n\n        # Section type weights\n        self._type_weights = {\n            \"constitutional\": 1.0,\n            \"policy\": 0.9,\n            \"system\": 0.7,\n            \"user\": 0.6,\n        }\n\n        logger.info(\n            f\"Initialized JRTContextPreparator with \"\n            f\"threshold={importance_threshold}, max_ratio={max_expansion_ratio}\"\n        )\n\n    async def prepare(\n        self,\n        context: JSONValue,\n        sections: Optional[List[ContextSection]] = None\n    ) -> PreparedContext:\n        \"\"\"\n        Prepare context with JRT optimization.\n\n        Args:\n            context: Raw input context\n            sections: Optional pre-identified sections\n\n        Returns:\n            PreparedContext with optimized layout\n        \"\"\"\n        # Identify sections if not provided\n        if sections is None:\n            sections = await self._identify_sections(context)\n\n        # Score sections for importance\n        scored_sections = await self._score_sections(sections)\n\n        # Select sections for repetition\n        sections_to_repeat = [\n            s for s in scored_sections\n            if s.importance_score >= self.importance_threshold\n        ]\n\n        # Apply JRT strategy\n        prepared, repeated_ranges = await self._apply_jrt(\n            context,\n            sections_to_repeat\n        )\n\n        # Compute critical positions for attention focusing\n        critical_positions = self._compute_critical_positions(\n            prepared,\n            sections_to_repeat\n        )\n\n        original_len = len(str(context))\n        prepared_len = len(str(prepared))\n\n        return PreparedContext(\n            original=context,\n            prepared=prepared,\n            critical_positions=critical_positions,\n            repeated_sections=repeated_ranges,\n            total_tokens=prepared_len,\n            expansion_ratio=prepared_len / max(original_len, 1),\n        )\n\n    async def _identify_sections(\n        self,\n        context: JSONValue\n    ) -> List[ContextSection]:\n        \"\"\"\n        Automatically identify sections in context.\n\n        Uses heuristics and patterns to detect:\n        - Constitutional principles (hash markers)\n        - Policy definitions (keywords)\n        - System prompts (structure)\n        - User content (residual)\n        \"\"\"\n        sections = []\n\n        if isinstance(context, str):\n            # Look for constitutional markers\n            if CONSTITUTIONAL_HASH in context:\n                sections.append(ContextSection(\n                    content=context,\n                    start_position=0,\n                    end_position=len(context),\n                    importance_score=1.0,\n                    section_type=\"constitutional\"\n                ))\n            else:\n                # Default section\n                sections.append(ContextSection(\n                    content=context,\n                    start_position=0,\n                    end_position=len(context),\n                    importance_score=0.5,\n                    section_type=\"user\"\n                ))\n        elif isinstance(context, dict):\n            # Handle structured context\n            pos = 0\n            for key, value in context.items():\n                section_type = self._infer_section_type(key)\n                content_len = len(str(value))\n                sections.append(ContextSection(\n                    content=value,\n                    start_position=pos,\n                    end_position=pos + content_len,\n                    importance_score=self._type_weights.get(section_type, 0.5),\n                    section_type=section_type\n                ))\n                pos += content_len\n\n        return sections\n\n    def _infer_section_type(self, key: str) -> str:\n        \"\"\"Infer section type from key name.\"\"\"\n        key_lower = key.lower()\n        if any(k in key_lower for k in [\"constitutional\", \"principle\", \"hash\"]):\n            return \"constitutional\"\n        if any(k in key_lower for k in [\"policy\", \"rule\", \"constraint\"]):\n            return \"policy\"\n        if any(k in key_lower for k in [\"system\", \"config\", \"setting\"]):\n            return \"system\"\n        return \"user\"\n\n    async def _score_sections(\n        self,\n        sections: List[ContextSection]\n    ) -> List[ContextSection]:\n        \"\"\"\n        Score sections for importance.\n\n        Considers:\n        - Section type (constitutional > policy > system > user)\n        - Position (start/end typically more important)\n        - Content characteristics\n        \"\"\"\n        scored = []\n\n        for section in sections:\n            # Base score from type\n            base_score = self._type_weights.get(section.section_type, 0.5)\n\n            # Apply constitutional priority boost\n            if section.section_type == \"constitutional\":\n                base_score *= self.constitutional_priority\n\n            # Position bonus (U-shaped: start and end matter more)\n            # This will be computed relative to full context later\n\n            # Create new section with updated score\n            scored.append(ContextSection(\n                content=section.content,\n                start_position=section.start_position,\n                end_position=section.end_position,\n                importance_score=min(base_score, 1.0),\n                section_type=section.section_type\n            ))\n\n        return scored\n\n    async def _apply_jrt(\n        self,\n        context: JSONValue,\n        sections_to_repeat: List[ContextSection]\n    ) -> Tuple[JSONValue, List[Tuple[int, int]]]:\n        \"\"\"\n        Apply JRT strategy by repeating critical sections.\n\n        Strategy:\n        1. Place constitutional content at start\n        2. Include original context\n        3. Repeat critical sections at end (recency bias)\n        \"\"\"\n        if not sections_to_repeat:\n            return context, []\n\n        repeated_ranges = []\n\n        if isinstance(context, str):\n            # Build prepared string\n            parts = []\n\n            # Add critical sections at start (primacy effect)\n            for section in sections_to_repeat:\n                if section.section_type == \"constitutional\":\n                    parts.append(f\"[CRITICAL] {section.content}\")\n                    repeated_ranges.append((0, len(parts[-1])))\n\n            # Add original context\n            parts.append(context)\n\n            # Add critical sections at end (recency effect)\n            for section in sections_to_repeat[:self.max_repetitions]:\n                parts.append(f\"[REPEATED] {section.content}\")\n                current_pos = sum(len(p) for p in parts[:-1])\n                repeated_ranges.append((current_pos, current_pos + len(parts[-1])))\n\n            prepared = \"\\n\".join(parts)\n\n            # Check expansion ratio\n            if len(prepared) / len(context) > self.max_expansion_ratio:\n                # Trim to stay within ratio\n                max_len = int(len(context) * self.max_expansion_ratio)\n                prepared = prepared[:max_len]\n\n            return prepared, repeated_ranges\n\n        # For non-string contexts, return as-is\n        return context, []\n\n    def _compute_critical_positions(\n        self,\n        prepared: JSONValue,\n        critical_sections: List[ContextSection]\n    ) -> List[int]:\n        \"\"\"\n        Compute positions that should receive focused attention.\n\n        These positions are passed to the attention mechanism\n        to ensure they influence the output.\n        \"\"\"\n        positions = []\n\n        if isinstance(prepared, str):\n            # Mark positions of repeated sections\n            for section in critical_sections:\n                content_str = str(section.content)\n                pos = 0\n                while True:\n                    idx = prepared.find(content_str, pos)\n                    if idx == -1:\n                        break\n                    positions.extend(range(idx, idx + len(content_str)))\n                    pos = idx + 1\n\n        return sorted(set(positions))\n\n\nclass AdaptiveJRTPreparator(JRTContextPreparator):\n    \"\"\"\n    Adaptive JRT Preparator that adjusts strategy based on context.\n\n    Uses feedback from processing to optimize repetition strategy.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._feedback_history: List[Dict[str, float]] = []\n        self._adaptation_threshold = 100  # Adapt after N samples\n\n    async def record_feedback(\n        self,\n        prepared_context: PreparedContext,\n        recall_score: float,\n        processing_time_ms: float\n    ) -> None:\n        \"\"\"Record feedback for adaptation.\"\"\"\n        self._feedback_history.append({\n            \"expansion_ratio\": prepared_context.expansion_ratio,\n            \"recall_score\": recall_score,\n            \"processing_time_ms\": processing_time_ms,\n            \"num_repetitions\": len(prepared_context.repeated_sections),\n        })\n\n        # Adapt if enough samples\n        if len(self._feedback_history) >= self._adaptation_threshold:\n            await self._adapt_parameters()\n\n    async def _adapt_parameters(self) -> None:\n        \"\"\"Adapt parameters based on feedback history.\"\"\"\n        if not self._feedback_history:\n            return\n\n        # Compute average metrics\n        avg_recall = sum(f[\"recall_score\"] for f in self._feedback_history) / len(self._feedback_history)\n        avg_time = sum(f[\"processing_time_ms\"] for f in self._feedback_history) / len(self._feedback_history)\n\n        # Adjust importance threshold based on recall\n        if avg_recall < 0.9:\n            # Lower threshold to include more sections\n            self.importance_threshold = max(0.5, self.importance_threshold - 0.05)\n        elif avg_recall > 0.95 and avg_time > 10:\n            # Raise threshold to reduce overhead\n            self.importance_threshold = min(0.9, self.importance_threshold + 0.05)\n\n        logger.info(\n            f\"Adapted JRT parameters: threshold={self.importance_threshold:.2f}, \"\n            f\"avg_recall={avg_recall:.2f}, avg_time={avg_time:.2f}ms\"\n        )\n\n        # Clear history for next adaptation cycle\n        self._feedback_history = []\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.184737",
  "last_updated": "2026-01-04T05:35:58.619088"
}