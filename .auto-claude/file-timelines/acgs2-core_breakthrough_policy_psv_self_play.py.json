{
  "file_path": "acgs2-core/breakthrough/policy/psv_self_play.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nPSV Self-Play - Constitutional AI Self-Improvement\n===================================================\n\nConstitutional Hash: cdd01ef066bc6cf2\n\nImplements PSV-Verus self-play for continuous improvement:\n- Propose: Generate increasingly challenging policy specifications\n- Solve: Attempt to create verified implementations\n- Verify: Validate solutions and learn from outcomes\n- Self-Play: Improve system through iterative refinement\n\nDesign Principles:\n- Self-improving through adversarial policy generation\n- Continuous learning from verification failures\n- Progressive difficulty scaling\n- Constitutional constraint preservation\n\"\"\"\n\nimport hashlib\nimport logging\nimport random\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Set\n\nfrom ...shared.types import JSONDict\nfrom .. import CONSTITUTIONAL_HASH\nfrom .verified_policy_generator import (\n    PolicyVerificationError,\n    VerifiedPolicy,\n    VerifiedPolicyGenerator,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass SelfPlayMode(Enum):\n    \"\"\"Modes of self-play operation.\"\"\"\n\n    TRAINING = \"training\"  # Generate training data\n    IMPROVEMENT = \"improvement\"  # Improve existing capabilities\n    EXPLORATION = \"exploration\"  # Explore new problem spaces\n    COMPETITION = \"competition\"  # Competitive self-play\n\n\nclass DifficultyLevel(Enum):\n    \"\"\"Difficulty levels for self-play challenges.\"\"\"\n\n    NOVICE = 1  # Basic policies\n    INTERMEDIATE = 2  # Moderate complexity\n    ADVANCED = 3  # Complex policies\n    EXPERT = 4  # Very challenging\n    MASTER = 5  # Near-impossible\n\n\n@dataclass\nclass SelfPlayChallenge:\n    \"\"\"A challenge generated for self-play.\"\"\"\n\n    challenge_id: str\n    natural_language_spec: str\n    difficulty_level: DifficultyLevel\n    category: str\n    constraints: List[str]\n    success_criteria: List[str]\n    generated_at: float = field(default_factory=time.time)\n\n    # Results\n    attempted_solutions: List[JSONDict] = field(default_factory=list)\n    best_solution: Optional[JSONDict] = None\n    success_achieved: bool = False\n    improvement_score: float = 0.0\n\n    def __post_init__(self):\n        if not self.challenge_id:\n            self.challenge_id = hashlib.sha256(\n                f\"{self.natural_language_spec}_{self.difficulty_level.value}_{self.generated_at}\".encode()\n            ).hexdigest()[:16]\n\n\n@dataclass\nclass SelfPlayRound:\n    \"\"\"A complete round of PSV self-play.\"\"\"\n\n    round_id: str\n    mode: SelfPlayMode\n    challenges: List[SelfPlayChallenge] = field(default_factory=list)\n    started_at: float = field(default_factory=time.time)\n    completed_at: Optional[float] = None\n\n    # Results\n    challenges_attempted: int = 0\n    challenges_solved: int = 0\n    average_difficulty: float = 0.0\n    improvement_metrics: Dict[str, float] = field(default_factory=dict)\n\n    def __post_init__(self):\n        if not self.round_id:\n            self.round_id = hashlib.sha256(\n                f\"round_{self.mode.value}_{self.started_at}\".encode()\n            ).hexdigest()[:16]\n\n\n@dataclass\nclass PSVAgent:\n    \"\"\"A PSV self-play agent.\"\"\"\n\n    agent_id: str\n    capabilities: Set[str] = field(default_factory=set)\n    performance_history: List[Dict[str, float]] = field(default_factory=list)\n    specialization_areas: Set[str] = field(default_factory=set)\n\n    # Learning state\n    skill_levels: Dict[str, float] = field(default_factory=dict)\n    adaptation_rate: float = 0.1\n    last_improvement: float = field(default_factory=time.time)\n\n    def __post_init__(self):\n        if not self.agent_id:\n            self.agent_id = hashlib.sha256(f\"psv_agent_{time.time()}\".encode()).hexdigest()[:12]\n\n\nclass PSVSelfPlay:\n    \"\"\"\n    PSV-Verus Self-Play System for Constitutional AI Improvement.\n\n    Implements continuous self-improvement through:\n    - Progressive challenge generation\n    - Solution attempt and verification\n    - Learning from successes and failures\n    - Adaptation to improving capabilities\n\n    This enables the system to get better at governance policy generation over time.\n    \"\"\"\n\n    def __init__(\n        self,\n        policy_generator: VerifiedPolicyGenerator,\n        max_rounds_per_session: int = 10,\n        improvement_threshold: float = 0.05,  # 5% improvement required\n        adaptation_enabled: bool = True,\n    ):\n        \"\"\"\n        Initialize PSV Self-Play system.\n\n        Args:\n            policy_generator: The verified policy generator to improve\n            max_rounds_per_session: Maximum self-play rounds per session\n            improvement_threshold: Minimum improvement required to continue\n            adaptation_enabled: Whether to enable adaptive difficulty\n        \"\"\"\n        self.policy_generator = policy_generator\n        self.max_rounds_per_session = max_rounds_per_session\n        self.improvement_threshold = improvement_threshold\n        self.adaptation_enabled = adaptation_enabled\n\n        # Self-play state\n        self.active_round: Optional[SelfPlayRound] = None\n        self.completed_rounds: List[SelfPlayRound] = []\n\n        # Challenge generation\n        self.challenge_templates: Dict[str, JSONDict] = {}\n        self.difficulty_progression: Dict[DifficultyLevel, JSONDict] = {}\n        self._initialize_challenge_templates()\n\n        # Learning and adaptation\n        self.psv_agents: Dict[str, PSVAgent] = {}\n        self.performance_baseline: Dict[str, float] = {}\n        self.learning_history: List[JSONDict] = []\n\n        # Performance tracking\n        self._metrics = {\n            \"total_rounds\": 0,\n            \"total_challenges\": 0,\n            \"challenges_solved\": 0,\n            \"average_success_rate\": 0.0,\n            \"difficulty_progression\": 0.0,\n            \"last_improvement\": time.time(),\n        }\n\n        logger.info(\"Initialized PSV Self-Play system\")\n\n    def _initialize_challenge_templates(self):\n        \"\"\"Initialize challenge templates for different categories.\"\"\"\n\n        # Security policies\n        self.challenge_templates[\"security\"] = {\n            \"base_template\": \"A security policy that {requirement} while maintaining {constraint}\",\n            \"requirements\": [\n                \"prevents unauthorized access to sensitive data\",\n                \"ensures data integrity across distributed systems\",\n                \"provides audit trails for all security events\",\n                \"implements zero-trust authentication\",\n                \"prevents privilege escalation attacks\",\n                \"ensures secure communication channels\",\n            ],\n            \"constraints\": [\n                \"minimal performance impact\",\n                \"backward compatibility\",\n                \"user-friendly operation\",\n                \"compliance with privacy regulations\",\n                \"scalability to millions of users\",\n            ],\n        }\n\n        # Governance policies\n        self.challenge_templates[\"governance\"] = {\n            \"base_template\": \"A governance policy that {requirement} while ensuring {constraint}\",\n            \"requirements\": [\n                \"balances executive efficiency with judicial oversight\",\n                \"enables democratic decision making at scale\",\n                \"maintains constitutional compliance across all operations\",\n                \"prevents concentration of power\",\n                \"ensures transparency in decision processes\",\n                \"facilitates stakeholder participation\",\n            ],\n            \"constraints\": [\n                \"sub-millisecond response times\",\n                \"mathematical verifiability\",\n                \"resistance to manipulation\",\n                \"cross-cultural applicability\",\n                \"minimal resource consumption\",\n            ],\n        }\n\n        # Compliance policies\n        self.challenge_templates[\"compliance\"] = {\n            \"base_template\": \"A compliance policy that {requirement} and {additional_requirement}\",\n            \"requirements\": [\n                \"ensures GDPR compliance for data processing\",\n                \"maintains HIPAA compliance for health data\",\n                \"follows SOX requirements for financial reporting\",\n                \"implements CIS security benchmarks\",\n                \"ensures accessibility compliance (WCAG)\",\n                \"maintains ISO 27001 information security\",\n            ],\n            \"additional_requirements\": [\n                \"automates compliance monitoring\",\n                \"provides audit-ready documentation\",\n                \"minimizes false positives\",\n                \"supports multi-jurisdictional compliance\",\n                \"integrates with existing systems\",\n            ],\n        }\n\n        # Initialize difficulty progression\n        for level in DifficultyLevel:\n            self.difficulty_progression[level] = {\n                \"min_complexity\": level.value,\n                \"max_categories\": min(level.value, len(self.challenge_templates)),\n                \"allow_compound\": level.value >= 3,\n                \"require_verification\": level.value >= 2,\n                \"time_limit_seconds\": 300 * level.value,\n            }\n\n    async def start_self_play_round(\n        self,\n        mode: SelfPlayMode = SelfPlayMode.IMPROVEMENT,\n        target_difficulty: DifficultyLevel = DifficultyLevel.INTERMEDIATE,\n    ) -> SelfPlayRound:\n        \"\"\"\n        Start a new self-play round.\n\n        Args:\n            mode: Self-play mode\n            target_difficulty: Target difficulty level\n\n        Returns:\n            New self-play round\n        \"\"\"\n        round_obj = SelfPlayRound(round_id=\"\", mode=mode)\n\n        # Generate challenges for this round\n        num_challenges = min(5, len(self.challenge_templates))  # 5 challenges per round\n\n        for _ in range(num_challenges):\n            challenge = await self._generate_challenge(target_difficulty, mode)\n            if challenge:\n                round_obj.challenges.append(challenge)\n\n        self.active_round = round_obj\n\n        logger.info(\n            f\"Started self-play round {round_obj.round_id} with \"\n            f\"{len(round_obj.challenges)} challenges\"\n        )\n        return round_obj\n\n    async def _generate_challenge(\n        self, target_difficulty: DifficultyLevel, mode: SelfPlayMode\n    ) -> Optional[SelfPlayChallenge]:\n        \"\"\"Generate a single challenge.\"\"\"\n\n        # Select category based on mode\n        if mode == SelfPlayMode.EXPLORATION:\n            category = random.choice(list(self.challenge_templates.keys()))\n        elif mode == SelfPlayMode.IMPROVEMENT:\n            # Focus on weaker areas\n            category = self._select_improvement_category()\n        else:\n            category = random.choice(list(self.challenge_templates.keys()))\n\n        template = self.challenge_templates[category]\n        difficulty_config = self.difficulty_progression[target_difficulty]\n\n        # Generate natural language specification\n        if category == \"security\":\n            requirement = random.choice(template[\"requirements\"])\n            constraint = random.choice(template[\"constraints\"])\n            spec = template[\"base_template\"].format(requirement=requirement, constraint=constraint)\n\n        elif category == \"governance\":\n            requirement = random.choice(template[\"requirements\"])\n            constraint = random.choice(template[\"constraints\"])\n            spec = template[\"base_template\"].format(requirement=requirement, constraint=constraint)\n\n        elif category == \"compliance\":\n            requirement = random.choice(template[\"requirements\"])\n            additional = random.choice(template[\"additional_requirements\"])\n            spec = template[\"base_template\"].format(\n                requirement=requirement, additional_requirement=additional\n            )\n\n        else:\n            return None\n\n        # Add complexity based on difficulty\n        if difficulty_config[\"allow_compound\"] and random.random() < 0.3:\n            spec += \" and also handles edge cases involving conflicting requirements\"\n\n        # Generate constraints and success criteria\n        constraints = [\n            \"Must be mathematically verifiable\",\n            \"Must maintain constitutional compliance\",\n            f\"Must be implementable within {difficulty_config['time_limit_seconds']} seconds\",\n        ]\n\n        success_criteria = [\n            \"Policy generates without errors\",\n            \"Formal verification succeeds\",\n            \"Policy handles specified requirements\",\n            \"No constitutional violations detected\",\n        ]\n\n        if difficulty_config[\"require_verification\"]:\n            success_criteria.append(\"Passes all verification checks\")\n\n        challenge = SelfPlayChallenge(\n            challenge_id=\"\",\n            natural_language_spec=spec,\n            difficulty_level=target_difficulty,\n            category=category,\n            constraints=constraints,\n            success_criteria=success_criteria,\n        )\n\n        return challenge\n\n    def _select_improvement_category(self) -> str:\n        \"\"\"Select category that needs the most improvement.\"\"\"\n        # Analyze performance by category\n        category_performance = {}\n\n        for round_obj in self.completed_rounds[-10:]:  # Last 10 rounds\n            for challenge in round_obj.challenges:\n                cat = challenge.category\n                if cat not in category_performance:\n                    category_performance[cat] = {\"attempted\": 0, \"solved\": 0}\n\n                category_performance[cat][\"attempted\"] += 1\n                if challenge.success_achieved:\n                    category_performance[cat][\"solved\"] += 1\n\n        # Calculate success rates\n        success_rates = {}\n        for cat, stats in category_performance.items():\n            if stats[\"attempted\"] > 0:\n                success_rates[cat] = stats[\"solved\"] / stats[\"attempted\"]\n\n        # Select category with lowest success rate\n        if success_rates:\n            worst_category = min(success_rates, key=success_rates.get)\n            return worst_category\n\n        return random.choice(list(self.challenge_templates.keys()))\n\n    async def execute_self_play_round(self, round_obj: SelfPlayRound) -> JSONDict:\n        \"\"\"\n        Execute a complete self-play round.\n\n        Args:\n            round_obj: The round to execute\n\n        Returns:\n            Round execution results\n        \"\"\"\n        results = {\n            \"round_id\": round_obj.round_id,\n            \"challenges_attempted\": 0,\n            \"challenges_solved\": 0,\n            \"total_attempts\": 0,\n            \"average_difficulty\": 0.0,\n            \"improvement_metrics\": {},\n        }\n\n        for challenge in round_obj.challenges:\n            round_obj.challenges_attempted += 1\n\n            # Attempt to solve the challenge\n            solution_result = await self._attempt_challenge_solution(challenge)\n\n            challenge.attempted_solutions.append(solution_result)\n\n            if solution_result[\"success\"]:\n                challenge.success_achieved = True\n                challenge.best_solution = solution_result\n                round_obj.challenges_solved += 1\n\n            results[\"challenges_attempted\"] += 1\n            results[\"total_attempts\"] += solution_result.get(\"attempts\", 1)\n\n        # Calculate metrics\n        if round_obj.challenges_attempted > 0:\n            round_obj.average_difficulty = (\n                sum(c.difficulty_level.value for c in round_obj.challenges)\n                / round_obj.challenges_attempted\n            )\n\n            results[\"challenges_solved\"] = round_obj.challenges_solved\n            results[\"average_difficulty\"] = round_obj.average_difficulty\n\n        # Complete the round\n        round_obj.completed_at = time.time()\n        self.completed_rounds.append(round_obj)\n        self.active_round = None\n\n        # Update metrics\n        self._update_metrics(round_obj)\n\n        # Calculate improvement\n        improvement = await self._calculate_improvement()\n        results[\"improvement_metrics\"] = improvement\n\n        logger.info(\n            f\"Completed self-play round {round_obj.round_id}: \"\n            f\"{round_obj.challenges_solved}/{round_obj.challenges_attempted} challenges solved\"\n        )\n\n        return results\n\n    async def _attempt_challenge_solution(self, challenge: SelfPlayChallenge) -> JSONDict:\n        \"\"\"Attempt to solve a self-play challenge.\"\"\"\n        start_time = time.time()\n        attempts = 0\n        max_attempts = 3  # Maximum attempts per challenge\n\n        for attempt in range(max_attempts):\n            attempts += 1\n\n            try:\n                # Use the policy generator to solve the challenge\n                policy = await self.policy_generator.generate_verified_policy(\n                    challenge.natural_language_spec\n                )\n\n                # Validate against success criteria\n                validation_result = await self._validate_solution(policy, challenge)\n\n                if validation_result[\"valid\"]:\n                    execution_time = time.time() - start_time\n                    return {\n                        \"success\": True,\n                        \"policy\": policy.to_dict(),\n                        \"attempts\": attempts,\n                        \"execution_time\": execution_time,\n                        \"validation\": validation_result,\n                    }\n\n            except PolicyVerificationError as e:\n                logger.debug(f\"Challenge attempt {attempt + 1} failed: {str(e)}\")\n                continue\n\n            except Exception as e:\n                logger.error(f\"Unexpected error in challenge attempt: {str(e)}\")\n                continue\n\n        # All attempts failed\n        execution_time = time.time() - start_time\n        return {\n            \"success\": False,\n            \"attempts\": attempts,\n            \"execution_time\": execution_time,\n            \"error\": \"All solution attempts failed\",\n        }\n\n    async def _validate_solution(self, policy: VerifiedPolicy, challenge: SelfPlayChallenge) -> JSONDict:\n        \"\"\"Validate a solution against challenge criteria.\"\"\"\n        validation = {\"valid\": True, \"criteria_met\": [], \"criteria_failed\": [], \"score\": 0.0}\n\n        # Check each success criterion\n        for criterion in challenge.success_criteria:\n            if \"Policy generates without errors\" in criterion:\n                # Policy was generated successfully\n                validation[\"criteria_met\"].append(criterion)\n                validation[\"score\"] += 0.25\n\n            elif \"Formal verification succeeds\" in criterion:\n                # Check if verification succeeded\n                if hasattr(policy, \"proof\") and policy.proof:\n                    validation[\"criteria_met\"].append(criterion)\n                    validation[\"score\"] += 0.25\n                else:\n                    validation[\"criteria_failed\"].append(criterion)\n                    validation[\"valid\"] = False\n\n            elif \"Policy handles specified requirements\" in criterion:\n                # Basic check: ensure policy contains relevant keywords\n                spec_lower = challenge.natural_language_spec.lower()\n                rego_lower = policy.rego_code.lower()\n\n                relevant_keywords = []\n                if \"security\" in spec_lower or \"access\" in spec_lower:\n                    relevant_keywords.extend([\"allow\", \"deny\", \"access\"])\n                if \"data\" in spec_lower:\n                    relevant_keywords.extend([\"data\", \"integrity\"])\n                if \"compliance\" in spec_lower:\n                    relevant_keywords.extend([\"compliance\", \"audit\"])\n\n                keyword_matches = sum(1 for kw in relevant_keywords if kw in rego_lower)\n                if keyword_matches >= len(relevant_keywords) * 0.6:\n                    validation[\"criteria_met\"].append(criterion)\n                    validation[\"score\"] += 0.25\n                else:\n                    validation[\"criteria_failed\"].append(criterion)\n                    validation[\"valid\"] = False\n\n            elif \"No constitutional violations detected\" in criterion:\n                # Check constitutional hash\n                if (\n                    hasattr(policy, \"constitutional_hash\")\n                    and policy.constitutional_hash == CONSTITUTIONAL_HASH\n                ):\n                    validation[\"criteria_met\"].append(criterion)\n                    validation[\"score\"] += 0.25\n                else:\n                    validation[\"criteria_failed\"].append(criterion)\n                    validation[\"valid\"] = False\n\n        return validation\n\n    def _update_metrics(self, round_obj: SelfPlayRound) -> None:\n        \"\"\"Update system metrics after a round.\"\"\"\n        self._metrics[\"total_rounds\"] += 1\n        self._metrics[\"total_challenges\"] += round_obj.challenges_attempted\n        self._metrics[\"challenges_solved\"] += round_obj.challenges_solved\n\n        # Update success rate\n        if self._metrics[\"total_challenges\"] > 0:\n            self._metrics[\"average_success_rate\"] = (\n                self._metrics[\"challenges_solved\"] / self._metrics[\"total_challenges\"]\n            )\n\n        # Update difficulty progression\n        if round_obj.challenges_attempted > 0:\n            avg_difficulty = round_obj.average_difficulty\n            self._metrics[\"difficulty_progression\"] = avg_difficulty\n\n    async def _calculate_improvement(self) -> Dict[str, float]:\n        \"\"\"Calculate improvement metrics compared to baseline.\"\"\"\n        improvement = {\n            \"success_rate_improvement\": 0.0,\n            \"difficulty_improvement\": 0.0,\n            \"speed_improvement\": 0.0,\n            \"overall_improvement\": 0.0,\n        }\n\n        # Compare to recent performance\n        recent_rounds = self.completed_rounds[-5:]  # Last 5 rounds\n        if len(recent_rounds) >= 2:\n            current_success_rate = self._metrics[\"average_success_rate\"]\n            recent_avg_success = sum(\n                r.challenges_solved / max(r.challenges_attempted, 1) for r in recent_rounds\n            ) / len(recent_rounds)\n\n            improvement[\"success_rate_improvement\"] = current_success_rate - recent_avg_success\n\n            # Difficulty improvement\n            recent_avg_difficulty = sum(r.average_difficulty for r in recent_rounds) / len(\n                recent_rounds\n            )\n            improvement[\"difficulty_improvement\"] = (\n                self._metrics[\"difficulty_progression\"] - recent_avg_difficulty\n            )\n\n        # Calculate overall improvement score\n        improvement[\"overall_improvement\"] = (\n            improvement[\"success_rate_improvement\"] * 0.5\n            + improvement[\"difficulty_improvement\"] * 0.3\n            + improvement[\"speed_improvement\"] * 0.2\n        )\n\n        return improvement\n\n    async def run_self_improvement_session(\n        self, num_rounds: int = 5, adaptive_difficulty: bool = True\n    ) -> JSONDict:\n        \"\"\"\n        Run a complete self-improvement session.\n\n        Args:\n            num_rounds: Number of rounds to run\n            adaptive_difficulty: Whether to adapt difficulty based on performance\n\n        Returns:\n            Session results\n        \"\"\"\n        session_results = {\n            \"session_id\": hashlib.sha256(f\"session_{time.time()}\".encode()).hexdigest()[:12],\n            \"rounds_completed\": 0,\n            \"total_challenges\": 0,\n            \"challenges_solved\": 0,\n            \"improvement_achieved\": False,\n            \"final_metrics\": {},\n            \"round_summaries\": [],\n        }\n\n        starting_success_rate = self._metrics[\"average_success_rate\"]\n        current_difficulty = DifficultyLevel.INTERMEDIATE\n\n        for round_num in range(num_rounds):\n            # Adapt difficulty if enabled\n            if adaptive_difficulty and round_num > 0:\n                current_difficulty = await self._adapt_difficulty(current_difficulty)\n\n            # Run a round\n            round_obj = await self.start_self_play_round(\n                mode=SelfPlayMode.IMPROVEMENT, target_difficulty=current_difficulty\n            )\n\n            round_results = await self.execute_self_play_round(round_obj)\n\n            session_results[\"round_summaries\"].append(round_results)\n            session_results[\"rounds_completed\"] += 1\n            session_results[\"total_challenges\"] += round_results[\"challenges_attempted\"]\n            session_results[\"challenges_solved\"] += round_results[\"challenges_solved\"]\n\n            # Check for significant improvement\n            if (\n                round_results[\"improvement_metrics\"].get(\"overall_improvement\", 0)\n                > self.improvement_threshold\n            ):\n                session_results[\"improvement_achieved\"] = True\n                logger.info(f\"Significant improvement detected in round {round_num + 1}\")\n\n        # Final assessment\n        final_success_rate = self._metrics[\"average_success_rate\"]\n        session_results[\"final_metrics\"] = self.get_system_metrics()\n        session_results[\"success_rate_improvement\"] = final_success_rate - starting_success_rate\n\n        logger.info(\n            f\"Completed self-improvement session: \"\n            f\"{session_results['challenges_solved']}/{session_results['total_challenges']} \"\n            f\"challenges solved, success rate improvement: \"\n            f\"{session_results['success_rate_improvement']:.3f}\"\n        )\n\n        return session_results\n\n    async def _adapt_difficulty(self, current_difficulty: DifficultyLevel) -> DifficultyLevel:\n        \"\"\"Adapt difficulty based on recent performance.\"\"\"\n        recent_success_rate = self._metrics[\"average_success_rate\"]\n\n        if recent_success_rate > 0.8:\n            # Increase difficulty\n            if current_difficulty.value < max(d.value for d in DifficultyLevel):\n                new_difficulty = DifficultyLevel(current_difficulty.value + 1)\n                logger.info(f\"Increasing difficulty to {new_difficulty.name}\")\n                return new_difficulty\n\n        elif recent_success_rate < 0.4:\n            # Decrease difficulty\n            if current_difficulty.value > min(d.value for d in DifficultyLevel):\n                new_difficulty = DifficultyLevel(current_difficulty.value - 1)\n                logger.info(f\"Decreasing difficulty to {new_difficulty.name}\")\n                return new_difficulty\n\n        return current_difficulty\n\n    def get_system_metrics(self) -> JSONDict:\n        \"\"\"Get current system performance metrics.\"\"\"\n        return {\n            **self._metrics,\n            \"active_round\": self.active_round.round_id if self.active_round else None,\n            \"completed_rounds\": len(self.completed_rounds),\n            \"available_categories\": len(self.challenge_templates),\n            \"psv_agents\": len(self.psv_agents),\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n\n    async def analyze_learning_patterns(self) -> JSONDict:\n        \"\"\"Analyze patterns in the learning process.\"\"\"\n        analysis = {\n            \"category_performance\": {},\n            \"difficulty_progression\": {},\n            \"learning_trends\": {},\n            \"bottlenecks_identified\": [],\n        }\n\n        # Analyze performance by category\n        for round_obj in self.completed_rounds:\n            for challenge in round_obj.challenges:\n                cat = challenge.category\n                if cat not in analysis[\"category_performance\"]:\n                    analysis[\"category_performance\"][cat] = {\"attempted\": 0, \"solved\": 0}\n\n                analysis[\"category_performance\"][cat][\"attempted\"] += 1\n                if challenge.success_achieved:\n                    analysis[\"category_performance\"][cat][\"solved\"] += 1\n\n        # Calculate success rates\n        for _, stats in analysis[\"category_performance\"].items():\n            if stats[\"attempted\"] > 0:\n                stats[\"success_rate\"] = stats[\"solved\"] / stats[\"attempted\"]\n\n        # Identify bottlenecks (categories with low success rates)\n        for cat, stats in analysis[\"category_performance\"].items():\n            success_rate = stats.get(\"success_rate\", 0)\n            if success_rate < 0.5 and stats[\"attempted\"] >= 3:\n                analysis[\"bottlenecks_identified\"].append(\n                    {\n                        \"category\": cat,\n                        \"success_rate\": success_rate,\n                        \"recommendation\": \"Focus improvement efforts on this category\",\n                    }\n                )\n\n        return analysis\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.243137",
  "last_updated": "2026-01-04T05:35:59.126590"
}