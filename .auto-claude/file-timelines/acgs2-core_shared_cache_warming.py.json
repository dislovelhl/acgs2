{
  "file_path": "src/core/shared/cache_warming.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Cache Warming Module\nConstitutional Hash: cdd01ef066bc6cf2\n\nProvides cache pre-population at service startup to prevent cold start performance\ndegradation with rate limiting to avoid thundering herd.\n\nWarming Strategy:\n- Load top 100 most-accessed keys from L3 into L2 (Redis)\n- Load top 10 most-accessed keys into L1 (in-process)\n- Rate limited to 100 keys/second to avoid overwhelming the system\n- Staggered loading to prevent connection pool exhaustion\n\nUsage:\n    from shared.cache_warming import CacheWarmer, get_cache_warmer\n\n    # Direct usage\n    warmer = CacheWarmer(rate_limit=100)\n    await warmer.warm_cache(source_keys=['key1', 'key2'])\n\n    # FastAPI startup event\n    @app.on_event(\"startup\")\n    async def startup_event():\n        warmer = get_cache_warmer()\n        await warmer.warm_cache()\n\nExample:\n    warmer = CacheWarmer(\n        rate_limit=100,\n        l1_count=10,\n        l2_count=100,\n    )\n    result = await warmer.warm_cache()\n    print(f\"Warmed {result.keys_warmed} keys in {result.duration_seconds:.2f}s\")\n\"\"\"\n\nimport asyncio\nimport logging\nimport threading\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional\n\n# Constitutional Hash for governance validation\nCONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\n\nclass WarmingStatus(Enum):\n    \"\"\"Cache warming status.\"\"\"\n\n    IDLE = \"idle\"\n    WARMING = \"warming\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n\n@dataclass\nclass WarmingConfig:\n    \"\"\"Configuration for cache warming.\"\"\"\n\n    # Rate limiting\n    rate_limit: int = 100  # Keys per second\n    batch_size: int = 10  # Keys per batch for efficient loading\n\n    # Tier allocation\n    l1_count: int = 10  # Top N keys to warm into L1\n    l2_count: int = 100  # Top N keys to warm into L2\n\n    # Timeouts\n    key_timeout: float = 1.0  # Timeout per key load in seconds\n    total_timeout: float = 300.0  # Maximum warming duration in seconds\n\n    # Retry settings\n    max_retries: int = 3\n    retry_delay: float = 0.5\n\n    # Priority loading\n    priority_keys: List[str] = field(default_factory=list)\n\n\n@dataclass\nclass WarmingResult:\n    \"\"\"Result of cache warming operation.\"\"\"\n\n    status: WarmingStatus\n    keys_warmed: int = 0\n    keys_failed: int = 0\n    l1_keys: int = 0\n    l2_keys: int = 0\n    duration_seconds: float = 0.0\n    error_message: Optional[str] = None\n    details: Dict[str, Any] = field(default_factory=dict)\n\n    @property\n    def success(self) -> bool:\n        \"\"\"Check if warming was successful.\"\"\"\n        return self.status == WarmingStatus.COMPLETED\n\n    @property\n    def success_rate(self) -> float:\n        \"\"\"Calculate success rate.\"\"\"\n        total = self.keys_warmed + self.keys_failed\n        return self.keys_warmed / total if total > 0 else 0.0\n\n\n@dataclass\nclass WarmingProgress:\n    \"\"\"Progress tracking for cache warming.\"\"\"\n\n    total_keys: int = 0\n    processed_keys: int = 0\n    current_batch: int = 0\n    total_batches: int = 0\n    elapsed_seconds: float = 0.0\n    estimated_remaining: float = 0.0\n\n    @property\n    def percent_complete(self) -> float:\n        \"\"\"Calculate completion percentage.\"\"\"\n        return (self.processed_keys / self.total_keys * 100) if self.total_keys > 0 else 0.0\n\n\nclass RateLimiter:\n    \"\"\"\n    Token bucket rate limiter for controlling cache warming speed.\n\n    Thread-safe implementation that allows bursting up to max_tokens\n    while maintaining average rate of tokens_per_second.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokens_per_second: float = 100.0,\n        max_tokens: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize rate limiter.\n\n        Args:\n            tokens_per_second: Target rate (keys per second)\n            max_tokens: Maximum burst capacity (defaults to tokens_per_second)\n        \"\"\"\n        self.tokens_per_second = tokens_per_second\n        self.max_tokens = max_tokens or int(tokens_per_second)\n        self.tokens = float(self.max_tokens)\n        self.last_update = time.monotonic()\n        self._lock = threading.Lock()\n\n    def _refill(self) -> None:\n        \"\"\"Refill tokens based on elapsed time.\"\"\"\n        now = time.monotonic()\n        elapsed = now - self.last_update\n        self.tokens = min(\n            self.max_tokens,\n            self.tokens + elapsed * self.tokens_per_second,\n        )\n        self.last_update = now\n\n    def acquire(self, tokens: int = 1) -> float:\n        \"\"\"\n        Acquire tokens, returning wait time if needed.\n\n        Args:\n            tokens: Number of tokens to acquire\n\n        Returns:\n            Time to wait in seconds (0 if tokens available)\n        \"\"\"\n        with self._lock:\n            self._refill()\n\n            if self.tokens >= tokens:\n                self.tokens -= tokens\n                return 0.0\n\n            # Calculate wait time\n            needed = tokens - self.tokens\n            wait_time = needed / self.tokens_per_second\n            return wait_time\n\n    async def acquire_async(self, tokens: int = 1) -> None:\n        \"\"\"\n        Async version of acquire that waits if needed.\n\n        Args:\n            tokens: Number of tokens to acquire\n        \"\"\"\n        wait_time = self.acquire(tokens)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n            # After waiting, actually consume the tokens\n            with self._lock:\n                self._refill()\n                self.tokens -= tokens\n\n\nclass CacheWarmer:\n    \"\"\"\n    Cache warming manager for pre-populating caches at startup.\n\n    Implements rate-limited cache warming to prevent thundering herd\n    during cold starts. Prioritizes hot data for L1 and distributes\n    remaining data to L2.\n\n    Features:\n    - Rate limited warming (default 100 keys/second)\n    - Batch loading for efficiency\n    - Priority key support\n    - Progress tracking\n    - Graceful error handling\n\n    Thread Safety:\n    - Uses RateLimiter with internal locking\n    - Progress tracking is thread-safe\n    - Supports concurrent warming cancellation\n    \"\"\"\n\n    def __init__(\n        self,\n        rate_limit: int = 100,\n        config: Optional[WarmingConfig] = None,\n        cache_manager: Optional[Any] = None,\n    ):\n        \"\"\"\n        Initialize cache warmer.\n\n        Args:\n            rate_limit: Maximum keys per second (overrides config if provided)\n            config: Optional WarmingConfig for detailed configuration\n            cache_manager: Optional TieredCacheManager instance (lazy loaded if None)\n        \"\"\"\n        self.config = config or WarmingConfig(rate_limit=rate_limit)\n        if rate_limit != 100:  # Explicit override\n            self.config.rate_limit = rate_limit\n\n        self._rate_limiter = RateLimiter(\n            tokens_per_second=self.config.rate_limit,\n            max_tokens=self.config.batch_size * 2,  # Allow 2x batch burst\n        )\n\n        self._cache_manager = cache_manager\n        self._status = WarmingStatus.IDLE\n        self._progress = WarmingProgress()\n        self._lock = threading.Lock()\n        self._cancel_event = asyncio.Event()\n\n        # Callbacks for progress updates\n        self._progress_callbacks: List[Callable[[WarmingProgress], None]] = []\n\n        logger.debug(\n            f\"[{CONSTITUTIONAL_HASH}] CacheWarmer initialized: \"\n            f\"rate_limit={self.config.rate_limit}/s, \"\n            f\"l1_count={self.config.l1_count}, l2_count={self.config.l2_count}\"\n        )\n\n    @property\n    def status(self) -> WarmingStatus:\n        \"\"\"Get current warming status.\"\"\"\n        return self._status\n\n    @property\n    def progress(self) -> WarmingProgress:\n        \"\"\"Get current warming progress.\"\"\"\n        return self._progress\n\n    @property\n    def is_warming(self) -> bool:\n        \"\"\"Check if warming is in progress.\"\"\"\n        return self._status == WarmingStatus.WARMING\n\n    def _get_cache_manager(self) -> Any:\n        \"\"\"\n        Get or lazily initialize the cache manager.\n\n        Returns:\n            TieredCacheManager instance\n        \"\"\"\n        if self._cache_manager is None:\n            # Lazy import to avoid circular dependencies\n            from shared.tiered_cache import get_tiered_cache\n\n            self._cache_manager = get_tiered_cache()\n        return self._cache_manager\n\n    async def warm_cache(\n        self,\n        source_keys: Optional[List[str]] = None,\n        key_loader: Optional[Callable[[str], Any]] = None,\n    ) -> WarmingResult:\n        \"\"\"\n        Warm the cache with rate limiting.\n\n        Loads keys into L1 and L2 tiers based on priority (hot keys to L1,\n        remaining to L2).\n\n        Args:\n            source_keys: Optional list of keys to warm (uses L3 keys if not provided)\n            key_loader: Optional function to load key values (uses cache_manager if not provided)\n\n        Returns:\n            WarmingResult with statistics and status\n        \"\"\"\n        start_time = time.monotonic()\n\n        with self._lock:\n            if self._status == WarmingStatus.WARMING:\n                logger.warning(f\"[{CONSTITUTIONAL_HASH}] Cache warming already in progress\")\n                return WarmingResult(\n                    status=WarmingStatus.FAILED,\n                    error_message=\"Warming already in progress\",\n                )\n\n            self._status = WarmingStatus.WARMING\n            self._cancel_event.clear()\n\n        try:\n            # Get cache manager\n            cache_manager = self._get_cache_manager()\n\n            # Determine keys to warm\n            keys_to_warm = await self._get_keys_to_warm(source_keys)\n\n            if not keys_to_warm:\n                logger.info(f\"[{CONSTITUTIONAL_HASH}] No keys to warm\")\n                with self._lock:\n                    self._status = WarmingStatus.COMPLETED\n                return WarmingResult(\n                    status=WarmingStatus.COMPLETED,\n                    duration_seconds=time.monotonic() - start_time,\n                )\n\n            # Initialize progress tracking\n            total_keys = len(keys_to_warm)\n            self._progress = WarmingProgress(\n                total_keys=total_keys,\n                total_batches=(total_keys + self.config.batch_size - 1) // self.config.batch_size,\n            )\n\n            logger.info(\n                f\"[{CONSTITUTIONAL_HASH}] Starting cache warming: \"\n                f\"{total_keys} keys, rate={self.config.rate_limit}/s\"\n            )\n\n            # Warm cache in batches\n            result = await self._warm_in_batches(\n                keys_to_warm,\n                cache_manager,\n                key_loader,\n                start_time,\n            )\n\n            # Update final status\n            with self._lock:\n                self._status = result.status\n\n            result.duration_seconds = time.monotonic() - start_time\n\n            logger.info(\n                f\"[{CONSTITUTIONAL_HASH}] Cache warming completed: \"\n                f\"warmed={result.keys_warmed}, failed={result.keys_failed}, \"\n                f\"L1={result.l1_keys}, L2={result.l2_keys}, \"\n                f\"duration={result.duration_seconds:.2f}s\"\n            )\n\n            return result\n\n        except asyncio.CancelledError:\n            logger.warning(f\"[{CONSTITUTIONAL_HASH}] Cache warming cancelled\")\n            with self._lock:\n                self._status = WarmingStatus.CANCELLED\n            return WarmingResult(\n                status=WarmingStatus.CANCELLED,\n                duration_seconds=time.monotonic() - start_time,\n            )\n\n        except Exception as e:\n            logger.error(f\"[{CONSTITUTIONAL_HASH}] Cache warming failed: {e}\")\n            with self._lock:\n                self._status = WarmingStatus.FAILED\n            return WarmingResult(\n                status=WarmingStatus.FAILED,\n                error_message=str(e),\n                duration_seconds=time.monotonic() - start_time,\n            )\n\n    async def _get_keys_to_warm(\n        self,\n        source_keys: Optional[List[str]] = None,\n    ) -> List[str]:\n        \"\"\"\n        Get the list of keys to warm.\n\n        Priority order:\n        1. Explicit source_keys if provided\n        2. Priority keys from config\n        3. Keys from L3 cache (top accessed)\n\n        Args:\n            source_keys: Optional explicit list of keys\n\n        Returns:\n            List of keys to warm, ordered by priority\n        \"\"\"\n        if source_keys:\n            return source_keys[: self.config.l2_count]\n\n        keys = []\n\n        # Add priority keys first\n        if self.config.priority_keys:\n            keys.extend(self.config.priority_keys)\n\n        # Try to get keys from L3 cache\n        try:\n            cache_manager = self._get_cache_manager()\n\n            # Access L3 cache keys (in-memory simulation)\n            if hasattr(cache_manager, \"_l3_cache\"):\n                with cache_manager._l3_lock:\n                    l3_keys = list(cache_manager._l3_cache.keys())\n\n                # Sort by access frequency if available\n                if hasattr(cache_manager, \"_access_records\"):\n                    with cache_manager._access_lock:\n                        sorted_keys = sorted(\n                            l3_keys,\n                            key=lambda k: cache_manager._access_records.get(\n                                k, type(\"obj\", (object,), {\"accesses_per_minute\": 0})()\n                            ).accesses_per_minute,\n                            reverse=True,\n                        )\n                        keys.extend(sorted_keys)\n                else:\n                    keys.extend(l3_keys)\n\n        except Exception as e:\n            logger.warning(f\"[{CONSTITUTIONAL_HASH}] Failed to get L3 keys for warming: {e}\")\n\n        # Remove duplicates while preserving order\n        seen = set()\n        unique_keys = []\n        for key in keys:\n            if key not in seen:\n                seen.add(key)\n                unique_keys.append(key)\n\n        return unique_keys[: self.config.l2_count]\n\n    async def _warm_in_batches(\n        self,\n        keys: List[str],\n        cache_manager: Any,\n        key_loader: Optional[Callable[[str], Any]],\n        start_time: float,\n    ) -> WarmingResult:\n        \"\"\"\n        Warm cache keys in rate-limited batches.\n\n        Args:\n            keys: Keys to warm\n            cache_manager: TieredCacheManager instance\n            key_loader: Optional custom key loader\n            start_time: Warming start time\n\n        Returns:\n            WarmingResult with statistics\n        \"\"\"\n        # Import CacheTier for tier specification\n        from shared.tiered_cache import CacheTier\n\n        result = WarmingResult(status=WarmingStatus.COMPLETED)\n        processed = 0\n        batch_num = 0\n\n        # Identify hot keys for L1 (top N most accessed)\n        l1_keys = keys[: self.config.l1_count]\n\n        # Create batches\n        batches = [\n            keys[i : i + self.config.batch_size]\n            for i in range(0, len(keys), self.config.batch_size)\n        ]\n\n        for batch in batches:\n            # Check cancellation\n            if self._cancel_event.is_set():\n                result.status = WarmingStatus.CANCELLED\n                break\n\n            # Check timeout\n            elapsed = time.monotonic() - start_time\n            if elapsed >= self.config.total_timeout:\n                logger.warning(\n                    f\"[{CONSTITUTIONAL_HASH}] Cache warming timeout after \"\n                    f\"{elapsed:.1f}s (limit: {self.config.total_timeout}s)\"\n                )\n                result.status = WarmingStatus.COMPLETED\n                result.details[\"timeout\"] = True\n                break\n\n            batch_num += 1\n\n            # Rate limit: acquire tokens for this batch\n            await self._rate_limiter.acquire_async(len(batch))\n\n            # Process batch\n            for key in batch:\n                try:\n                    # Load value\n                    value = await self._load_key_value(key, cache_manager, key_loader)\n\n                    if value is not None:\n                        # Determine target tier\n                        if key in l1_keys:\n                            await cache_manager.set(key, value, tier=CacheTier.L1)\n                            result.l1_keys += 1\n                        else:\n                            await cache_manager.set(key, value, tier=CacheTier.L2)\n                            result.l2_keys += 1\n\n                        result.keys_warmed += 1\n                    else:\n                        result.keys_failed += 1\n\n                except Exception as e:\n                    logger.debug(f\"[{CONSTITUTIONAL_HASH}] Failed to warm key '{key}': {e}\")\n                    result.keys_failed += 1\n\n                processed += 1\n\n            # Update progress\n            elapsed = time.monotonic() - start_time\n            rate = processed / elapsed if elapsed > 0 else 0\n            remaining = len(keys) - processed\n\n            self._progress = WarmingProgress(\n                total_keys=len(keys),\n                processed_keys=processed,\n                current_batch=batch_num,\n                total_batches=len(batches),\n                elapsed_seconds=elapsed,\n                estimated_remaining=remaining / rate if rate > 0 else 0,\n            )\n\n            # Notify progress callbacks\n            for callback in self._progress_callbacks:\n                try:\n                    callback(self._progress)\n                except Exception as e:\n                    logger.warning(f\"Progress callback error: {e}\")\n\n            # Yield to event loop periodically\n            await asyncio.sleep(0)\n\n        return result\n\n    async def _load_key_value(\n        self,\n        key: str,\n        cache_manager: Any,\n        key_loader: Optional[Callable[[str], Any]],\n    ) -> Optional[Any]:\n        \"\"\"\n        Load value for a key during warming.\n\n        Args:\n            key: Key to load\n            cache_manager: TieredCacheManager instance\n            key_loader: Optional custom loader function\n\n        Returns:\n            Value or None if not found\n        \"\"\"\n        retries = 0\n\n        while retries < self.config.max_retries:\n            try:\n                if key_loader:\n                    # Use custom loader\n                    value = key_loader(key)\n                    if asyncio.iscoroutine(value):\n                        value = await asyncio.wait_for(\n                            value,\n                            timeout=self.config.key_timeout,\n                        )\n                    return value\n\n                # Try to get from L3 cache\n                if hasattr(cache_manager, \"_l3_cache\"):\n                    with cache_manager._l3_lock:\n                        if key in cache_manager._l3_cache:\n                            cached = cache_manager._l3_cache[key]\n                            return cached.get(\"data\")\n\n                # Try to get from existing tiers\n                value = cache_manager.get(key)\n                if value is not None:\n                    return value\n\n                return None\n\n            except asyncio.TimeoutError:\n                retries += 1\n                if retries < self.config.max_retries:\n                    await asyncio.sleep(self.config.retry_delay)\n\n            except Exception as e:\n                logger.debug(f\"Key load error for '{key}': {e}\")\n                retries += 1\n                if retries < self.config.max_retries:\n                    await asyncio.sleep(self.config.retry_delay)\n\n        return None\n\n    def cancel(self) -> None:\n        \"\"\"Cancel ongoing cache warming.\"\"\"\n        self._cancel_event.set()\n        logger.info(f\"[{CONSTITUTIONAL_HASH}] Cache warming cancellation requested\")\n\n    def on_progress(self, callback: Callable[[WarmingProgress], None]) -> None:\n        \"\"\"\n        Register a progress callback.\n\n        Args:\n            callback: Function called with WarmingProgress on each batch\n        \"\"\"\n        self._progress_callbacks.append(callback)\n\n    def remove_progress_callback(self, callback: Callable[[WarmingProgress], None]) -> bool:\n        \"\"\"\n        Remove a progress callback.\n\n        Args:\n            callback: Callback to remove\n\n        Returns:\n            True if callback was removed\n        \"\"\"\n        try:\n            self._progress_callbacks.remove(callback)\n            return True\n        except ValueError:\n            return False\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get cache warmer statistics.\n\n        Returns:\n            Dictionary with warmer configuration and status\n        \"\"\"\n        return {\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n            \"status\": self._status.value,\n            \"config\": {\n                \"rate_limit\": self.config.rate_limit,\n                \"batch_size\": self.config.batch_size,\n                \"l1_count\": self.config.l1_count,\n                \"l2_count\": self.config.l2_count,\n            },\n            \"progress\": {\n                \"total_keys\": self._progress.total_keys,\n                \"processed_keys\": self._progress.processed_keys,\n                \"percent_complete\": self._progress.percent_complete,\n                \"elapsed_seconds\": self._progress.elapsed_seconds,\n            },\n        }\n\n    def __repr__(self) -> str:\n        \"\"\"String representation.\"\"\"\n        return f\"CacheWarmer(rate_limit={self.config.rate_limit}, \" f\"status={self._status.value})\"\n\n\n# -----------------------------------------------------------------------------\n# Singleton pattern for shared usage\n# -----------------------------------------------------------------------------\n\n_default_warmer: Optional[CacheWarmer] = None\n_singleton_lock = threading.Lock()\n\n\ndef get_cache_warmer(\n    rate_limit: int = 100,\n    config: Optional[WarmingConfig] = None,\n) -> CacheWarmer:\n    \"\"\"\n    Get or create the singleton CacheWarmer instance.\n\n    Thread-safe singleton pattern for shared warmer access.\n\n    Args:\n        rate_limit: Maximum keys per second (used only on first call)\n        config: Optional WarmingConfig (used only on first call)\n\n    Returns:\n        CacheWarmer singleton instance\n    \"\"\"\n    global _default_warmer\n\n    if _default_warmer is None:\n        with _singleton_lock:\n            if _default_warmer is None:\n                _default_warmer = CacheWarmer(rate_limit=rate_limit, config=config)\n                logger.info(\n                    f\"[{CONSTITUTIONAL_HASH}] CacheWarmer singleton created: \"\n                    f\"rate_limit={rate_limit}\"\n                )\n\n    return _default_warmer\n\n\ndef reset_cache_warmer() -> None:\n    \"\"\"\n    Reset the singleton CacheWarmer instance.\n\n    Useful for testing or when configuration needs to change.\n    \"\"\"\n    global _default_warmer\n\n    with _singleton_lock:\n        if _default_warmer is not None:\n            # Cancel any ongoing warming\n            _default_warmer.cancel()\n            _default_warmer = None\n            logger.info(f\"[{CONSTITUTIONAL_HASH}] CacheWarmer singleton reset\")\n\n\nasync def warm_cache_on_startup(\n    source_keys: Optional[List[str]] = None,\n    priority_keys: Optional[List[str]] = None,\n    rate_limit: int = 100,\n) -> WarmingResult:\n    \"\"\"\n    Convenience function for FastAPI startup event.\n\n    Usage:\n        @app.on_event(\"startup\")\n        async def startup_event():\n            result = await warm_cache_on_startup()\n            if not result.success:\n                logger.warning(f\"Cache warming failed: {result.error_message}\")\n\n    Args:\n        source_keys: Optional list of keys to warm\n        priority_keys: Optional list of priority keys (warmed first)\n        rate_limit: Maximum keys per second\n\n    Returns:\n        WarmingResult with statistics\n    \"\"\"\n    config = WarmingConfig(\n        rate_limit=rate_limit,\n        priority_keys=priority_keys or [],\n    )\n\n    warmer = get_cache_warmer(rate_limit=rate_limit, config=config)\n    return await warmer.warm_cache(source_keys=source_keys)\n\n\n__all__ = [\n    # Constants\n    \"CONSTITUTIONAL_HASH\",\n    # Enums\n    \"WarmingStatus\",\n    # Classes\n    \"CacheWarmer\",\n    \"WarmingConfig\",\n    \"WarmingResult\",\n    \"WarmingProgress\",\n    \"RateLimiter\",\n    # Functions\n    \"get_cache_warmer\",\n    \"reset_cache_warmer\",\n    \"warm_cache_on_startup\",\n]\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.329154",
  "last_updated": "2026-01-04T05:35:59.035146"
}