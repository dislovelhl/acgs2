{
  "file_path": "integration-service/src/consumers/event_consumer.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nKafka Event Consumer for Governance Events\n\nProvides async Kafka consumer to ingest governance events from the Agent Bus\nand route them to enabled integrations.\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport os\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom typing import Any, Callable, Coroutine, Dict, List, Optional, Set\nfrom uuid import uuid4\n\nfrom aiokafka import AIOKafkaConsumer\nfrom aiokafka.errors import KafkaConnectionError, KafkaError, OffsetOutOfRangeError\nfrom aiokafka.structs import ConsumerRecord\nfrom pydantic import BaseModel, ConfigDict, Field, field_validator\n\nfrom ..integrations.base import EventSeverity, IntegrationEvent\nfrom ..types import ConfigDict as ConfigDictType, EventData, JSONDict\n\nlogger = logging.getLogger(__name__)\n\n\nclass GovernanceEventType(str, Enum):\n    \"\"\"Types of governance events consumed from Agent Bus.\"\"\"\n\n    # Policy events\n    POLICY_VIOLATION = \"policy.violation\"\n    POLICY_CREATED = \"policy.created\"\n    POLICY_UPDATED = \"policy.updated\"\n    POLICY_DELETED = \"policy.deleted\"\n    POLICY_EVALUATION = \"policy.evaluation\"\n\n    # Compliance events\n    COMPLIANCE_CHECK_STARTED = \"compliance.check.started\"\n    COMPLIANCE_CHECK_PASSED = \"compliance.check.passed\"\n    COMPLIANCE_CHECK_FAILED = \"compliance.check.failed\"\n\n    # Access review events\n    ACCESS_REVIEW_STARTED = \"access_review.started\"\n    ACCESS_REVIEW_COMPLETED = \"access_review.completed\"\n    ACCESS_REVIEW_EXPIRED = \"access_review.expired\"\n\n    # Approval workflow events\n    APPROVAL_REQUESTED = \"approval.requested\"\n    APPROVAL_GRANTED = \"approval.granted\"\n    APPROVAL_DENIED = \"approval.denied\"\n    APPROVAL_EXPIRED = \"approval.expired\"\n\n    # System events\n    SYSTEM_ALERT = \"system.alert\"\n    INTEGRATION_ERROR = \"integration.error\"\n    AUDIT_LOG = \"audit.log\"\n\n    # Custom events\n    CUSTOM = \"custom\"\n\n\nclass EventConsumerState(str, Enum):\n    \"\"\"State of the event consumer.\"\"\"\n\n    STOPPED = \"stopped\"\n    STARTING = \"starting\"\n    RUNNING = \"running\"\n    PAUSED = \"paused\"\n    STOPPING = \"stopping\"\n    ERROR = \"error\"\n\n\nclass GovernanceEvent(BaseModel):\n    \"\"\"\n    Governance event model from Agent Bus.\n\n    Represents an event published by the Agent Bus that needs to be\n    routed to configured integrations.\n    \"\"\"\n\n    event_id: str = Field(\n        default_factory=lambda: str(uuid4()),\n        description=\"Unique event identifier\",\n    )\n    event_type: str = Field(..., description=\"Type of governance event\")\n    timestamp: datetime = Field(\n        default_factory=lambda: datetime.now(timezone.utc),\n        description=\"Event timestamp in UTC\",\n    )\n    severity: str = Field(default=\"info\", description=\"Event severity level\")\n    source: str = Field(default=\"agent-bus\", description=\"Source system\")\n\n    # Event content\n    policy_id: Optional[str] = Field(None, description=\"Related policy ID\")\n    resource_id: Optional[str] = Field(None, description=\"Affected resource ID\")\n    resource_type: Optional[str] = Field(None, description=\"Type of affected resource\")\n    action: Optional[str] = Field(None, description=\"Action that triggered the event\")\n    outcome: Optional[str] = Field(None, description=\"Outcome of the action\")\n\n    # Event details\n    title: str = Field(..., description=\"Event title/summary\")\n    description: Optional[str] = Field(None, description=\"Detailed description\")\n    details: EventData = Field(\n        default_factory=dict, description=\"Additional event details\"\n    )\n\n    # Metadata\n    user_id: Optional[str] = Field(None, description=\"User who triggered the event\")\n    tenant_id: Optional[str] = Field(\n        None, description=\"Tenant ID for multi-tenant deployments\"\n    )\n    correlation_id: Optional[str] = Field(\n        None, description=\"Correlation ID for tracing\"\n    )\n    tags: List[str] = Field(default_factory=list, description=\"Event tags\")\n\n    # Kafka metadata\n    kafka_topic: Optional[str] = Field(None, description=\"Source Kafka topic\")\n    kafka_partition: Optional[int] = Field(None, description=\"Kafka partition\")\n    kafka_offset: Optional[int] = Field(None, description=\"Kafka offset\")\n    kafka_timestamp: Optional[int] = Field(\n        None, description=\"Kafka message timestamp (ms)\"\n    )\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        str_strip_whitespace=True,\n    )\n\n    @field_validator(\"severity\", mode=\"before\")\n    @classmethod\n    def normalize_severity(cls, v: str) -> str:\n        \"\"\"Normalize severity to lowercase.\"\"\"\n        if isinstance(v, str):\n            return v.lower().strip()\n        return str(v).lower()\n\n    @field_validator(\"event_type\", mode=\"before\")\n    @classmethod\n    def normalize_event_type(cls, v: str) -> str:\n        \"\"\"Normalize event type format.\"\"\"\n        if isinstance(v, str):\n            return v.lower().strip()\n        return str(v).lower()\n\n    def to_integration_event(self) -> IntegrationEvent:\n        \"\"\"\n        Convert to IntegrationEvent for adapter consumption.\n\n        Returns:\n            IntegrationEvent suitable for integration adapters\n        \"\"\"\n        # Map severity string to EventSeverity enum\n        severity_map = {\n            \"critical\": EventSeverity.CRITICAL,\n            \"high\": EventSeverity.HIGH,\n            \"medium\": EventSeverity.MEDIUM,\n            \"low\": EventSeverity.LOW,\n            \"info\": EventSeverity.INFO,\n        }\n        severity = severity_map.get(self.severity.lower(), EventSeverity.INFO)\n\n        return IntegrationEvent(\n            event_id=self.event_id,\n            event_type=self.event_type,\n            timestamp=self.timestamp,\n            severity=severity,\n            source=self.source,\n            policy_id=self.policy_id,\n            resource_id=self.resource_id,\n            resource_type=self.resource_type,\n            action=self.action,\n            outcome=self.outcome,\n            title=self.title,\n            description=self.description,\n            details=self.details,\n            user_id=self.user_id,\n            tenant_id=self.tenant_id,\n            correlation_id=self.correlation_id,\n            tags=self.tags,\n        )\n\n\nclass EventConsumerMetrics(BaseModel):\n    \"\"\"Metrics for event consumer operations.\"\"\"\n\n    events_received: int = Field(default=0, description=\"Total events received\")\n    events_processed: int = Field(default=0, description=\"Events successfully processed\")\n    events_failed: int = Field(default=0, description=\"Events that failed processing\")\n    events_skipped: int = Field(default=0, description=\"Events skipped due to filters\")\n    events_retried: int = Field(default=0, description=\"Events retried after failure\")\n\n    # Timing metrics\n    last_event_received_at: Optional[datetime] = Field(\n        None, description=\"Timestamp of last received event\"\n    )\n    last_event_processed_at: Optional[datetime] = Field(\n        None, description=\"Timestamp of last successfully processed event\"\n    )\n    last_error_at: Optional[datetime] = Field(\n        None, description=\"Timestamp of last error\"\n    )\n\n    # Lag metrics\n    current_lag: int = Field(default=0, description=\"Current consumer lag\")\n    max_lag_observed: int = Field(default=0, description=\"Maximum lag observed\")\n\n    # Processing metrics\n    avg_processing_time_ms: float = Field(\n        default=0.0, description=\"Average processing time in milliseconds\"\n    )\n    total_processing_time_ms: float = Field(\n        default=0.0, description=\"Total processing time in milliseconds\"\n    )\n\n    # Connection metrics\n    connection_errors: int = Field(default=0, description=\"Number of connection errors\")\n    reconnections: int = Field(default=0, description=\"Number of reconnections\")\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        validate_assignment=True,\n    )\n\n    def record_event_received(self) -> None:\n        \"\"\"Record that an event was received.\"\"\"\n        self.events_received += 1\n        self.last_event_received_at = datetime.now(timezone.utc)\n\n    def record_event_processed(self, processing_time_ms: float) -> None:\n        \"\"\"Record that an event was successfully processed.\"\"\"\n        self.events_processed += 1\n        self.last_event_processed_at = datetime.now(timezone.utc)\n        self.total_processing_time_ms += processing_time_ms\n\n        # Update average\n        if self.events_processed > 0:\n            self.avg_processing_time_ms = (\n                self.total_processing_time_ms / self.events_processed\n            )\n\n    def record_event_failed(self) -> None:\n        \"\"\"Record that an event failed processing.\"\"\"\n        self.events_failed += 1\n        self.last_error_at = datetime.now(timezone.utc)\n\n    def record_event_skipped(self) -> None:\n        \"\"\"Record that an event was skipped.\"\"\"\n        self.events_skipped += 1\n\n    def record_connection_error(self) -> None:\n        \"\"\"Record a connection error.\"\"\"\n        self.connection_errors += 1\n        self.last_error_at = datetime.now(timezone.utc)\n\n    def record_reconnection(self) -> None:\n        \"\"\"Record a successful reconnection.\"\"\"\n        self.reconnections += 1\n\n    def update_lag(self, lag: int) -> None:\n        \"\"\"Update current lag metrics.\"\"\"\n        self.current_lag = lag\n        if lag > self.max_lag_observed:\n            self.max_lag_observed = lag\n\n    def to_dict(self) -> JSONDict:\n        \"\"\"Convert metrics to dictionary.\"\"\"\n        return self.model_dump()\n\n\nclass EventConsumerConfig(BaseModel):\n    \"\"\"Configuration for the Kafka event consumer.\"\"\"\n\n    # Kafka connection settings\n    bootstrap_servers: str = Field(\n        default=\"localhost:9092\",\n        description=\"Comma-separated list of Kafka bootstrap servers\",\n    )\n    topics: List[str] = Field(\n        default_factory=lambda: [\"governance-events\"],\n        description=\"Kafka topics to subscribe to\",\n    )\n    group_id: str = Field(\n        default=\"integration-service-consumer\",\n        description=\"Kafka consumer group ID\",\n    )\n    client_id: str = Field(\n        default=\"integration-service\",\n        description=\"Kafka client ID\",\n    )\n\n    # Consumer behavior settings\n    auto_offset_reset: str = Field(\n        default=\"latest\",\n        description=\"Offset reset behavior (earliest, latest)\",\n    )\n    enable_auto_commit: bool = Field(\n        default=True,\n        description=\"Enable automatic offset commits\",\n    )\n    auto_commit_interval_ms: int = Field(\n        default=5000,\n        ge=1000,\n        description=\"Auto-commit interval in milliseconds\",\n    )\n    max_poll_records: int = Field(\n        default=100,\n        ge=1,\n        le=1000,\n        description=\"Maximum records to fetch per poll\",\n    )\n    max_poll_interval_ms: int = Field(\n        default=300000,\n        ge=60000,\n        description=\"Maximum interval between polls in milliseconds\",\n    )\n    session_timeout_ms: int = Field(\n        default=30000,\n        ge=10000,\n        description=\"Consumer session timeout in milliseconds\",\n    )\n    heartbeat_interval_ms: int = Field(\n        default=10000,\n        ge=1000,\n        description=\"Heartbeat interval in milliseconds\",\n    )\n\n    # Security settings\n    security_protocol: str = Field(\n        default=\"PLAINTEXT\",\n        description=\"Security protocol (PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL)\",\n    )\n    sasl_mechanism: Optional[str] = Field(\n        None,\n        description=\"SASL mechanism (PLAIN, SCRAM-SHA-256, SCRAM-SHA-512)\",\n    )\n    sasl_username: Optional[str] = Field(\n        None,\n        description=\"SASL username\",\n    )\n    sasl_password: Optional[str] = Field(\n        None,\n        description=\"SASL password\",\n    )\n    ssl_cafile: Optional[str] = Field(\n        None,\n        description=\"Path to CA certificate file\",\n    )\n    ssl_certfile: Optional[str] = Field(\n        None,\n        description=\"Path to client certificate file\",\n    )\n    ssl_keyfile: Optional[str] = Field(\n        None,\n        description=\"Path to client key file\",\n    )\n\n    # Processing settings\n    batch_size: int = Field(\n        default=10,\n        ge=1,\n        le=100,\n        description=\"Number of events to process in a batch\",\n    )\n    batch_timeout_seconds: float = Field(\n        default=5.0,\n        ge=0.1,\n        le=60.0,\n        description=\"Maximum time to wait for a batch to fill\",\n    )\n    max_retries: int = Field(\n        default=3,\n        ge=0,\n        le=10,\n        description=\"Maximum retries for failed event processing\",\n    )\n    retry_delay_seconds: float = Field(\n        default=1.0,\n        ge=0.1,\n        le=60.0,\n        description=\"Initial delay between retries\",\n    )\n    retry_exponential_base: float = Field(\n        default=2.0,\n        ge=1.5,\n        le=4.0,\n        description=\"Exponential backoff multiplier\",\n    )\n\n    # Filtering settings\n    event_type_filter: Optional[List[str]] = Field(\n        None,\n        description=\"Filter to only process specific event types (None = all)\",\n    )\n    severity_filter: Optional[List[str]] = Field(\n        None,\n        description=\"Filter to only process specific severities (None = all)\",\n    )\n    tenant_filter: Optional[List[str]] = Field(\n        None,\n        description=\"Filter to only process events from specific tenants (None = all)\",\n    )\n\n    # Health check settings\n    health_check_interval_seconds: int = Field(\n        default=30,\n        ge=5,\n        description=\"Interval for consumer health checks\",\n    )\n    max_consecutive_errors: int = Field(\n        default=10,\n        ge=1,\n        description=\"Maximum consecutive errors before pausing\",\n    )\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        str_strip_whitespace=True,\n    )\n\n    @field_validator(\"auto_offset_reset\", mode=\"before\")\n    @classmethod\n    def validate_offset_reset(cls, v: str) -> str:\n        \"\"\"Validate offset reset policy.\"\"\"\n        valid = {\"earliest\", \"latest\", \"none\"}\n        if v.lower() not in valid:\n            raise ValueError(f\"auto_offset_reset must be one of {valid}\")\n        return v.lower()\n\n    @field_validator(\"security_protocol\", mode=\"before\")\n    @classmethod\n    def validate_security_protocol(cls, v: str) -> str:\n        \"\"\"Validate security protocol.\"\"\"\n        valid = {\"PLAINTEXT\", \"SSL\", \"SASL_PLAINTEXT\", \"SASL_SSL\"}\n        v_upper = v.upper()\n        if v_upper not in valid:\n            raise ValueError(f\"security_protocol must be one of {valid}\")\n        return v_upper\n\n    @classmethod\n    def from_environment(cls) -> \"EventConsumerConfig\":\n        \"\"\"\n        Create configuration from environment variables.\n\n        Environment variable names:\n        - KAFKA_BOOTSTRAP_SERVERS\n        - KAFKA_TOPICS (comma-separated)\n        - KAFKA_GROUP_ID\n        - KAFKA_CLIENT_ID\n        - KAFKA_SECURITY_PROTOCOL\n        - KAFKA_SASL_MECHANISM\n        - KAFKA_SASL_USERNAME\n        - KAFKA_SASL_PASSWORD\n        - KAFKA_SSL_CAFILE\n        - KAFKA_SSL_CERTFILE\n        - KAFKA_SSL_KEYFILE\n        - KAFKA_AUTO_OFFSET_RESET\n        - KAFKA_MAX_POLL_RECORDS\n        \"\"\"\n        topics_str = os.getenv(\"KAFKA_TOPICS\", \"governance-events\")\n        topics = [t.strip() for t in topics_str.split(\",\") if t.strip()]\n\n        event_types_str = os.getenv(\"KAFKA_EVENT_TYPE_FILTER\")\n        event_type_filter = None\n        if event_types_str:\n            event_type_filter = [t.strip() for t in event_types_str.split(\",\")]\n\n        severity_str = os.getenv(\"KAFKA_SEVERITY_FILTER\")\n        severity_filter = None\n        if severity_str:\n            severity_filter = [s.strip() for s in severity_str.split(\",\")]\n\n        tenant_str = os.getenv(\"KAFKA_TENANT_FILTER\")\n        tenant_filter = None\n        if tenant_str:\n            tenant_filter = [t.strip() for t in tenant_str.split(\",\")]\n\n        return cls(\n            bootstrap_servers=os.getenv(\n                \"KAFKA_BOOTSTRAP_SERVERS\", \"localhost:9092\"\n            ),\n            topics=topics,\n            group_id=os.getenv(\n                \"KAFKA_GROUP_ID\", \"integration-service-consumer\"\n            ),\n            client_id=os.getenv(\"KAFKA_CLIENT_ID\", \"integration-service\"),\n            auto_offset_reset=os.getenv(\"KAFKA_AUTO_OFFSET_RESET\", \"latest\"),\n            max_poll_records=int(os.getenv(\"KAFKA_MAX_POLL_RECORDS\", \"100\")),\n            security_protocol=os.getenv(\"KAFKA_SECURITY_PROTOCOL\", \"PLAINTEXT\"),\n            sasl_mechanism=os.getenv(\"KAFKA_SASL_MECHANISM\"),\n            sasl_username=os.getenv(\"KAFKA_SASL_USERNAME\"),\n            sasl_password=os.getenv(\"KAFKA_SASL_PASSWORD\"),\n            ssl_cafile=os.getenv(\"KAFKA_SSL_CAFILE\"),\n            ssl_certfile=os.getenv(\"KAFKA_SSL_CERTFILE\"),\n            ssl_keyfile=os.getenv(\"KAFKA_SSL_KEYFILE\"),\n            event_type_filter=event_type_filter,\n            severity_filter=severity_filter,\n            tenant_filter=tenant_filter,\n        )\n\n\n# Type alias for event handler callback\nEventHandler = Callable[[GovernanceEvent], Coroutine[Any, Any, bool]]\n\n\nclass EventConsumer:\n    \"\"\"\n    Kafka consumer for governance events from Agent Bus.\n\n    Provides async consumption of governance events with support for:\n    - Configurable event filtering\n    - Batch processing\n    - Retry logic with exponential backoff\n    - Metrics and health monitoring\n    - Graceful shutdown\n    \"\"\"\n\n    def __init__(self, config: Optional[EventConsumerConfig] = None):\n        \"\"\"\n        Initialize the event consumer.\n\n        Args:\n            config: Consumer configuration. If None, loads from environment.\n        \"\"\"\n        self.config = config or EventConsumerConfig.from_environment()\n        self._consumer: Optional[AIOKafkaConsumer] = None\n        self._state = EventConsumerState.STOPPED\n        self._metrics = EventConsumerMetrics()\n        self._handlers: List[EventHandler] = []\n        self._stop_event = asyncio.Event()\n        self._consecutive_errors = 0\n        self._assigned_partitions: Set[str] = set()\n\n    @property\n    def state(self) -> EventConsumerState:\n        \"\"\"Get current consumer state.\"\"\"\n        return self._state\n\n    @property\n    def metrics(self) -> EventConsumerMetrics:\n        \"\"\"Get current metrics.\"\"\"\n        return self._metrics\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Check if consumer is running.\"\"\"\n        return self._state == EventConsumerState.RUNNING\n\n    @property\n    def is_healthy(self) -> bool:\n        \"\"\"Check if consumer is healthy.\"\"\"\n        return (\n            self._state in (EventConsumerState.RUNNING, EventConsumerState.PAUSED)\n            and self._consecutive_errors < self.config.max_consecutive_errors\n        )\n\n    def add_handler(self, handler: EventHandler) -> None:\n        \"\"\"\n        Add an event handler callback.\n\n        Args:\n            handler: Async function that processes events. Should return True on\n                    success, False on failure.\n        \"\"\"\n        self._handlers.append(handler)\n        logger.info(f\"Added event handler: {handler.__name__}\")\n\n    def remove_handler(self, handler: EventHandler) -> None:\n        \"\"\"\n        Remove an event handler callback.\n\n        Args:\n            handler: Handler to remove\n        \"\"\"\n        if handler in self._handlers:\n            self._handlers.remove(handler)\n            logger.info(f\"Removed event handler: {handler.__name__}\")\n\n    def clear_handlers(self) -> None:\n        \"\"\"Remove all event handlers.\"\"\"\n        self._handlers.clear()\n        logger.info(\"Cleared all event handlers\")\n\n    async def start(self) -> None:\n        \"\"\"\n        Start the consumer and begin consuming events.\n\n        Raises:\n            KafkaConnectionError: If connection to Kafka fails\n        \"\"\"\n        if self._state == EventConsumerState.RUNNING:\n            logger.warning(\"Consumer is already running\")\n            return\n\n        logger.info(\n            f\"Starting Kafka consumer for topics: {self.config.topics}\"\n        )\n        self._state = EventConsumerState.STARTING\n        self._stop_event.clear()\n\n        try:\n            # Build consumer configuration\n            consumer_config = self._build_consumer_config()\n\n            # Create and start consumer\n            self._consumer = AIOKafkaConsumer(\n                *self.config.topics,\n                **consumer_config,\n            )\n\n            await self._consumer.start()\n\n            # Get assigned partitions\n            assignment = self._consumer.assignment()\n            self._assigned_partitions = {\n                f\"{tp.topic}:{tp.partition}\" for tp in assignment\n            }\n            logger.info(f\"Assigned partitions: {self._assigned_partitions}\")\n\n            self._state = EventConsumerState.RUNNING\n            self._consecutive_errors = 0\n            logger.info(\"Kafka consumer started successfully\")\n\n        except KafkaConnectionError as e:\n            self._state = EventConsumerState.ERROR\n            self._metrics.record_connection_error()\n            logger.error(f\"Failed to connect to Kafka: {e}\")\n            raise\n\n        except Exception as e:\n            self._state = EventConsumerState.ERROR\n            logger.error(f\"Failed to start consumer: {e}\")\n            raise\n\n    async def stop(self) -> None:\n        \"\"\"\n        Stop the consumer gracefully.\n\n        Waits for current message processing to complete before stopping.\n        \"\"\"\n        if self._state == EventConsumerState.STOPPED:\n            logger.warning(\"Consumer is already stopped\")\n            return\n\n        logger.info(\"Stopping Kafka consumer...\")\n        self._state = EventConsumerState.STOPPING\n        self._stop_event.set()\n\n        if self._consumer:\n            try:\n                await self._consumer.stop()\n                logger.info(\"Kafka consumer stopped\")\n            except Exception as e:\n                logger.error(f\"Error stopping consumer: {e}\")\n            finally:\n                self._consumer = None\n\n        self._state = EventConsumerState.STOPPED\n        self._assigned_partitions.clear()\n\n    async def pause(self) -> None:\n        \"\"\"Pause event consumption.\"\"\"\n        if self._state != EventConsumerState.RUNNING:\n            logger.warning(f\"Cannot pause consumer in state: {self._state}\")\n            return\n\n        if self._consumer:\n            partitions = self._consumer.assignment()\n            self._consumer.pause(*partitions)\n            self._state = EventConsumerState.PAUSED\n            logger.info(\"Consumer paused\")\n\n    async def resume(self) -> None:\n        \"\"\"Resume event consumption.\"\"\"\n        if self._state != EventConsumerState.PAUSED:\n            logger.warning(f\"Cannot resume consumer in state: {self._state}\")\n            return\n\n        if self._consumer:\n            partitions = self._consumer.assignment()\n            self._consumer.resume(*partitions)\n            self._state = EventConsumerState.RUNNING\n            logger.info(\"Consumer resumed\")\n\n    async def consume(self) -> None:\n        \"\"\"\n        Main consumption loop.\n\n        Continuously polls for messages and processes them through registered handlers.\n        Call stop() to terminate the loop gracefully.\n        \"\"\"\n        if self._state != EventConsumerState.RUNNING:\n            raise RuntimeError(f\"Consumer not running (state: {self._state})\")\n\n        logger.info(\"Starting consumption loop\")\n\n        while not self._stop_event.is_set():\n            try:\n                # Poll for messages\n                messages = await self._poll_messages()\n\n                if not messages:\n                    continue\n\n                # Process messages\n                for message in messages:\n                    if self._stop_event.is_set():\n                        break\n\n                    await self._process_message(message)\n\n            except OffsetOutOfRangeError:\n                logger.warning(\"Offset out of range, seeking to beginning\")\n                if self._consumer:\n                    await self._consumer.seek_to_beginning()\n                self._metrics.record_connection_error()\n\n            except KafkaError as e:\n                self._consecutive_errors += 1\n                self._metrics.record_connection_error()\n                logger.error(f\"Kafka error: {e}\")\n\n                if self._consecutive_errors >= self.config.max_consecutive_errors:\n                    logger.error(\n                        f\"Max consecutive errors ({self.config.max_consecutive_errors}) \"\n                        \"reached, pausing consumer\"\n                    )\n                    await self.pause()\n                    # Wait before trying to resume\n                    await asyncio.sleep(self.config.retry_delay_seconds * 5)\n                    await self.resume()\n                else:\n                    await asyncio.sleep(self.config.retry_delay_seconds)\n\n            except asyncio.CancelledError:\n                logger.info(\"Consumption loop cancelled\")\n                break\n\n            except Exception as e:\n                self._consecutive_errors += 1\n                self._metrics.record_connection_error()\n                logger.exception(f\"Unexpected error in consumption loop: {e}\")\n                await asyncio.sleep(self.config.retry_delay_seconds)\n\n        logger.info(\"Consumption loop terminated\")\n\n    async def _poll_messages(self) -> List[ConsumerRecord]:\n        \"\"\"\n        Poll Kafka for messages.\n\n        Returns:\n            List of Kafka consumer records\n        \"\"\"\n        if not self._consumer:\n            return []\n\n        try:\n            # Use getmany for batch polling\n            batch = await asyncio.wait_for(\n                self._consumer.getmany(\n                    timeout_ms=int(self.config.batch_timeout_seconds * 1000),\n                    max_records=self.config.batch_size,\n                ),\n                timeout=self.config.batch_timeout_seconds + 5.0,\n            )\n\n            messages = []\n            for _tp, records in batch.items():\n                messages.extend(records)\n\n            return messages\n\n        except asyncio.TimeoutError:\n            return []\n\n    async def _process_message(self, message: ConsumerRecord) -> None:\n        \"\"\"\n        Process a single Kafka message.\n\n        Args:\n            message: Kafka consumer record\n        \"\"\"\n        start_time = datetime.now(timezone.utc)\n\n        try:\n            # Parse message\n            event = self._parse_message(message)\n\n            if event is None:\n                self._metrics.record_event_skipped()\n                return\n\n            self._metrics.record_event_received()\n\n            # Apply filters\n            if not self._should_process_event(event):\n                self._metrics.record_event_skipped()\n                logger.debug(f\"Event {event.event_id} filtered out\")\n                return\n\n            # Process through handlers\n            success = await self._dispatch_event(event)\n\n            # Record metrics\n            end_time = datetime.now(timezone.utc)\n            processing_time_ms = (end_time - start_time).total_seconds() * 1000\n\n            if success:\n                self._metrics.record_event_processed(processing_time_ms)\n                self._consecutive_errors = 0\n            else:\n                self._metrics.record_event_failed()\n\n        except Exception as e:\n            self._metrics.record_event_failed()\n            logger.exception(f\"Error processing message: {e}\")\n\n    def _parse_message(self, message: ConsumerRecord) -> Optional[GovernanceEvent]:\n        \"\"\"\n        Parse a Kafka message into a GovernanceEvent.\n\n        Args:\n            message: Kafka consumer record\n\n        Returns:\n            Parsed GovernanceEvent or None if parsing fails\n        \"\"\"\n        try:\n            # Decode message value\n            if message.value is None:\n                logger.warning(\"Received message with null value\")\n                return None\n\n            if isinstance(message.value, bytes):\n                value_str = message.value.decode(\"utf-8\")\n            else:\n                value_str = str(message.value)\n\n            # Parse JSON\n            data = json.loads(value_str)\n\n            # Ensure required fields\n            if \"title\" not in data:\n                data[\"title\"] = data.get(\"event_type\", \"Unknown Event\")\n\n            # Create event with Kafka metadata\n            event = GovernanceEvent(\n                kafka_topic=message.topic,\n                kafka_partition=message.partition,\n                kafka_offset=message.offset,\n                kafka_timestamp=message.timestamp,\n                **data,\n            )\n\n            return event\n\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to parse message JSON: {e}\")\n            return None\n\n        except Exception as e:\n            logger.error(f\"Failed to parse message: {e}\")\n            return None\n\n    def _should_process_event(self, event: GovernanceEvent) -> bool:\n        \"\"\"\n        Check if an event should be processed based on filters.\n\n        Args:\n            event: The governance event to check\n\n        Returns:\n            True if the event passes all filters\n        \"\"\"\n        # Check event type filter\n        if self.config.event_type_filter:\n            if event.event_type not in self.config.event_type_filter:\n                return False\n\n        # Check severity filter\n        if self.config.severity_filter:\n            if event.severity not in self.config.severity_filter:\n                return False\n\n        # Check tenant filter\n        if self.config.tenant_filter:\n            if event.tenant_id not in self.config.tenant_filter:\n                return False\n\n        return True\n\n    async def _dispatch_event(self, event: GovernanceEvent) -> bool:\n        \"\"\"\n        Dispatch an event to all registered handlers.\n\n        Args:\n            event: The governance event to dispatch\n\n        Returns:\n            True if all handlers succeeded, False otherwise\n        \"\"\"\n        if not self._handlers:\n            logger.warning(\"No event handlers registered\")\n            return True\n\n        all_success = True\n\n        for handler in self._handlers:\n            try:\n                success = await self._invoke_handler_with_retry(handler, event)\n                if not success:\n                    all_success = False\n            except Exception as e:\n                logger.exception(\n                    f\"Handler {handler.__name__} failed for event {event.event_id}: {e}\"\n                )\n                all_success = False\n\n        return all_success\n\n    async def _invoke_handler_with_retry(\n        self, handler: EventHandler, event: GovernanceEvent\n    ) -> bool:\n        \"\"\"\n        Invoke a handler with retry logic.\n\n        Args:\n            handler: The event handler to invoke\n            event: The governance event to process\n\n        Returns:\n            True if handler succeeded, False otherwise\n        \"\"\"\n        last_error: Optional[Exception] = None\n\n        for attempt in range(self.config.max_retries + 1):\n            try:\n                result = await handler(event)\n                return result\n\n            except Exception as e:\n                last_error = e\n                self._metrics.events_retried += 1\n\n                if attempt < self.config.max_retries:\n                    delay = self.config.retry_delay_seconds * (\n                        self.config.retry_exponential_base ** attempt\n                    )\n                    logger.warning(\n                        f\"Handler {handler.__name__} failed (attempt {attempt + 1}/\"\n                        f\"{self.config.max_retries + 1}), retrying in {delay:.2f}s: {e}\"\n                    )\n                    await asyncio.sleep(delay)\n                else:\n                    logger.error(\n                        f\"Handler {handler.__name__} failed after \"\n                        f\"{self.config.max_retries + 1} attempts: {last_error}\"\n                    )\n\n        return False\n\n    def _build_consumer_config(self) -> ConfigDictType:\n        \"\"\"\n        Build Kafka consumer configuration dictionary.\n\n        Returns:\n            Configuration dictionary for AIOKafkaConsumer\n        \"\"\"\n        config = {\n            \"bootstrap_servers\": self.config.bootstrap_servers,\n            \"group_id\": self.config.group_id,\n            \"client_id\": self.config.client_id,\n            \"auto_offset_reset\": self.config.auto_offset_reset,\n            \"enable_auto_commit\": self.config.enable_auto_commit,\n            \"auto_commit_interval_ms\": self.config.auto_commit_interval_ms,\n            \"max_poll_records\": self.config.max_poll_records,\n            \"max_poll_interval_ms\": self.config.max_poll_interval_ms,\n            \"session_timeout_ms\": self.config.session_timeout_ms,\n            \"heartbeat_interval_ms\": self.config.heartbeat_interval_ms,\n            \"security_protocol\": self.config.security_protocol,\n        }\n\n        # Add SASL configuration if using SASL\n        if self.config.security_protocol in (\"SASL_PLAINTEXT\", \"SASL_SSL\"):\n            if self.config.sasl_mechanism:\n                config[\"sasl_mechanism\"] = self.config.sasl_mechanism\n            if self.config.sasl_username:\n                config[\"sasl_plain_username\"] = self.config.sasl_username\n            if self.config.sasl_password:\n                config[\"sasl_plain_password\"] = self.config.sasl_password\n\n        # Add SSL configuration if using SSL\n        if self.config.security_protocol in (\"SSL\", \"SASL_SSL\"):\n            if self.config.ssl_cafile:\n                config[\"ssl_cafile\"] = self.config.ssl_cafile\n            if self.config.ssl_certfile:\n                config[\"ssl_certfile\"] = self.config.ssl_certfile\n            if self.config.ssl_keyfile:\n                config[\"ssl_keyfile\"] = self.config.ssl_keyfile\n\n        return config\n\n    async def get_lag(self) -> Dict[str, int]:\n        \"\"\"\n        Get current consumer lag per partition.\n\n        Returns:\n            Dictionary mapping partition to lag\n        \"\"\"\n        if not self._consumer:\n            return {}\n\n        lag = {}\n\n        try:\n            assignment = self._consumer.assignment()\n\n            for tp in assignment:\n                # Get current position\n                position = await self._consumer.position(tp)\n\n                # Get end offset\n                end_offsets = await self._consumer.end_offsets([tp])\n                end_offset = end_offsets.get(tp, 0)\n\n                # Calculate lag\n                partition_lag = max(0, end_offset - position)\n                lag[f\"{tp.topic}:{tp.partition}\"] = partition_lag\n\n            # Update metrics\n            total_lag = sum(lag.values())\n            self._metrics.update_lag(total_lag)\n\n        except Exception as e:\n            logger.error(f\"Error getting consumer lag: {e}\")\n\n        return lag\n\n    def get_health_status(self) -> JSONDict:\n        \"\"\"\n        Get consumer health status.\n\n        Returns:\n            Dictionary with health information\n        \"\"\"\n        return {\n            \"state\": self._state.value,\n            \"is_healthy\": self.is_healthy,\n            \"is_running\": self.is_running,\n            \"consecutive_errors\": self._consecutive_errors,\n            \"max_consecutive_errors\": self.config.max_consecutive_errors,\n            \"assigned_partitions\": list(self._assigned_partitions),\n            \"handlers_count\": len(self._handlers),\n            \"topics\": self.config.topics,\n            \"group_id\": self.config.group_id,\n            \"metrics\": self._metrics.to_dict(),\n        }\n\n    def __repr__(self) -> str:\n        return (\n            f\"<EventConsumer(topics={self.config.topics}, \"\n            f\"group_id={self.config.group_id}, state={self._state.value})>\"\n        )\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.275991",
  "last_updated": "2026-01-04T05:35:58.837558"
}