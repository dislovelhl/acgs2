{
  "file_path": "adaptive-learning-engine/tests/unit/test_monitoring.py",
  "main_branch_history": [],
  "task_views": {
    "049-cache-drift-detection-report-results": {
      "task_id": "049-cache-drift-detection-report-results",
      "branch_point": {
        "commit_hash": "4a99fe22e8e0087919301b1aa185d4e2c6da716c",
        "content": "\"\"\"\nUnit tests for the Adaptive Learning Engine monitoring module.\n\nTests cover:\n- DriftDetector: Evidently-based drift detection\n- MetricsRegistry: Prometheus metrics collection\n\nConstitutional Hash: cdd01ef066bc6cf2\n\"\"\"\n\nimport threading\nimport time\nfrom typing import Any, Dict, List\nfrom unittest.mock import MagicMock, patch\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom prometheus_client import CollectorRegistry\n\nfrom src.monitoring.drift_detector import (\n    DriftAlert,\n    DriftDetector,\n    DriftMetrics,\n    DriftResult,\n    DriftStatus,\n)\nfrom src.monitoring.metrics import (\n    MetricLabel,\n    MetricsRegistry,\n    MetricsSnapshot,\n    create_metrics_registry,\n    get_metrics_registry,\n)\n\n\n# =============================================================================\n# Test Fixtures\n# =============================================================================\n\n\n@pytest.fixture\ndef sample_features() -> Dict[str, Any]:\n    \"\"\"Sample feature dictionary for testing.\"\"\"\n    return {\n        \"feature_a\": 1.0,\n        \"feature_b\": 2.5,\n        \"feature_c\": 0.3,\n    }\n\n\n@pytest.fixture\ndef reference_data() -> List[Dict[str, Any]]:\n    \"\"\"Sample reference dataset for drift testing.\"\"\"\n    np.random.seed(42)\n    return [{\"f1\": np.random.normal(0, 1), \"f2\": np.random.normal(0, 1)} for _ in range(100)]\n\n\n@pytest.fixture\ndef drifted_data() -> List[Dict[str, Any]]:\n    \"\"\"Sample current data with drift (different distribution).\"\"\"\n    np.random.seed(123)\n    return [\n        {\"f1\": np.random.normal(5, 2), \"f2\": np.random.normal(-5, 2)}  # Shifted distribution\n        for _ in range(100)\n    ]\n\n\n@pytest.fixture\ndef similar_data() -> List[Dict[str, Any]]:\n    \"\"\"Sample current data similar to reference (no drift).\"\"\"\n    np.random.seed(456)\n    return [\n        {\"f1\": np.random.normal(0.1, 1.1), \"f2\": np.random.normal(-0.1, 0.9)}  # Similar\n        for _ in range(100)\n    ]\n\n\n@pytest.fixture\ndef drift_detector() -> DriftDetector:\n    \"\"\"Fresh DriftDetector for testing.\"\"\"\n    return DriftDetector(\n        drift_threshold=0.2,\n        reference_window_size=100,\n        current_window_size=50,\n        min_samples_for_drift=10,\n        check_interval_seconds=60,\n        drift_share_threshold=0.5,\n        enabled=True,\n    )\n\n\n@pytest.fixture\ndef metrics_registry() -> MetricsRegistry:\n    \"\"\"Fresh MetricsRegistry for testing with isolated registry.\"\"\"\n    registry = CollectorRegistry()\n    return MetricsRegistry(registry=registry, prefix=\"test_adaptive\")\n\n\n# =============================================================================\n# DriftDetector Tests - Initialization\n# =============================================================================\n\n\nclass TestDriftDetectorInit:\n    \"\"\"Tests for DriftDetector initialization.\"\"\"\n\n    def test_default_initialization(self):\n        \"\"\"Test default initialization creates valid detector.\"\"\"\n        detector = DriftDetector()\n\n        assert detector.drift_threshold == 0.2\n        assert detector.reference_window_size == 1000\n        assert detector.current_window_size == 100\n        assert detector.min_samples_for_drift == 10\n        assert detector.check_interval_seconds == 300\n        assert detector.drift_share_threshold == 0.5\n        assert detector._enabled is True\n        assert detector._current_status == DriftStatus.INSUFFICIENT_DATA\n\n    def test_custom_initialization(self):\n        \"\"\"Test initialization with custom parameters.\"\"\"\n        detector = DriftDetector(\n            drift_threshold=0.3,\n            reference_window_size=500,\n            current_window_size=25,\n            min_samples_for_drift=20,\n            check_interval_seconds=120,\n            drift_share_threshold=0.7,\n            enabled=False,\n        )\n\n        assert detector.drift_threshold == 0.3\n        assert detector.reference_window_size == 500\n        assert detector.current_window_size == 25\n        assert detector.min_samples_for_drift == 20\n        assert detector.check_interval_seconds == 120\n        assert detector.drift_share_threshold == 0.7\n        assert detector._enabled is False\n        assert detector._current_status == DriftStatus.DISABLED\n\n    def test_data_windows_initialized_empty(self, drift_detector):\n        \"\"\"Test that data windows start empty.\"\"\"\n        assert len(drift_detector._reference_data) == 0\n        assert len(drift_detector._current_data) == 0\n        assert len(drift_detector._all_data) == 0\n\n\n# =============================================================================\n# DriftDetector Tests - Data Management\n# =============================================================================\n\n\nclass TestDriftDetectorDataManagement:\n    \"\"\"Tests for DriftDetector data point management.\"\"\"\n\n    def test_add_data_point(self, drift_detector, sample_features):\n        \"\"\"Test adding a single data point.\"\"\"\n        drift_detector.add_data_point(\n            features=sample_features,\n            label=1,\n            prediction=1,\n        )\n\n        assert len(drift_detector._current_data) == 1\n        assert len(drift_detector._reference_data) == 1\n        assert \"feature_a\" in drift_detector._known_columns\n\n    def test_add_data_point_disabled(self, sample_features):\n        \"\"\"Test that data points are ignored when disabled.\"\"\"\n        detector = DriftDetector(enabled=False)\n        detector.add_data_point(features=sample_features, label=1)\n\n        assert len(detector._current_data) == 0\n\n    def test_add_data_point_updates_current_window(self, drift_detector):\n        \"\"\"Test that data points are added to current window.\"\"\"\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        assert len(drift_detector._current_data) == 5\n        assert len(drift_detector._reference_data) == 5\n\n    def test_add_data_point_with_timestamp(self, drift_detector, sample_features):\n        \"\"\"Test adding data point with custom timestamp.\"\"\"\n        custom_time = 1234567890.0\n        drift_detector.add_data_point(\n            features=sample_features,\n            timestamp=custom_time,\n        )\n\n        record = list(drift_detector._current_data)[0]\n        assert record[\"_timestamp\"] == custom_time\n\n    def test_add_batch(self, drift_detector):\n        \"\"\"Test adding multiple data points at once.\"\"\"\n        data_points = [{\"x\": float(i)} for i in range(10)]\n        labels = [i % 2 for i in range(10)]\n        predictions = [i % 2 for i in range(10)]\n\n        count = drift_detector.add_batch(data_points, labels, predictions)\n\n        assert count == 10\n        assert len(drift_detector._current_data) == 10\n\n    def test_add_batch_partial_labels(self, drift_detector):\n        \"\"\"Test batch add with fewer labels than data points.\"\"\"\n        data_points = [{\"x\": float(i)} for i in range(10)]\n        labels = [1, 0, 1]  # Only 3 labels\n\n        count = drift_detector.add_batch(data_points, labels)\n\n        assert count == 10\n        # First 3 should have labels, rest should not\n        records = list(drift_detector._current_data)\n        assert \"_label\" in records[0]\n        assert \"_label\" not in records[5]\n\n\n# =============================================================================\n# DriftDetector Tests - Reference Data Management\n# =============================================================================\n\n\nclass TestDriftDetectorReferenceManagement:\n    \"\"\"Tests for reference data management.\"\"\"\n\n    def test_lock_reference_data(self, drift_detector, sample_features):\n        \"\"\"Test locking reference data.\"\"\"\n        drift_detector.add_data_point(sample_features)\n        drift_detector.lock_reference_data()\n\n        assert drift_detector._reference_locked is True\n\n    def test_locked_reference_not_updated(self, drift_detector, sample_features):\n        \"\"\"Test that locked reference is not updated with new data.\"\"\"\n        drift_detector.add_data_point(sample_features)\n        drift_detector.lock_reference_data()\n\n        # Add more data\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        # Reference should still have only 1 entry\n        assert len(drift_detector._reference_data) == 1\n        # Current should have all data\n        assert len(drift_detector._current_data) == 6\n\n    def test_unlock_reference_data(self, drift_detector, sample_features):\n        \"\"\"Test unlocking reference data.\"\"\"\n        drift_detector.lock_reference_data()\n        drift_detector.unlock_reference_data()\n\n        assert drift_detector._reference_locked is False\n\n    def test_update_reference_from_current(self, drift_detector):\n        \"\"\"Test updating reference from current window.\"\"\"\n        # Add initial data\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        drift_detector.lock_reference_data()\n\n        # Add more to current\n        for i in range(10, 15):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        # Current has 10 entries, reference has 5\n        assert len(drift_detector._current_data) == 10\n        assert len(drift_detector._reference_data) == 5\n\n        # Update reference from current\n        count = drift_detector.update_reference_from_current()\n\n        # Now reference should have 10 entries\n        assert count == 10\n        assert len(drift_detector._reference_data) == 10\n\n    def test_set_reference_data_from_dataframe(self, drift_detector):\n        \"\"\"Test setting reference data from DataFrame.\"\"\"\n        df = pd.DataFrame(\n            {\n                \"f1\": [1.0, 2.0, 3.0, 4.0, 5.0],\n                \"f2\": [0.1, 0.2, 0.3, 0.4, 0.5],\n            }\n        )\n\n        drift_detector.set_reference_data(df)\n\n        assert len(drift_detector._reference_data) == 5\n        assert drift_detector._reference_locked is True\n        assert \"f1\" in drift_detector._known_columns\n        assert \"f2\" in drift_detector._known_columns\n\n\n# =============================================================================\n# DriftDetector Tests - Drift Detection\n# =============================================================================\n\n\nclass TestDriftDetectorDriftCheck:\n    \"\"\"Tests for drift detection functionality.\"\"\"\n\n    def test_check_drift_disabled(self):\n        \"\"\"Test drift check when disabled.\"\"\"\n        detector = DriftDetector(enabled=False)\n        result = detector.check_drift()\n\n        assert result.status == DriftStatus.DISABLED\n        assert result.drift_detected is False\n        assert result.message == \"Drift detection is disabled\"\n\n    def test_check_drift_insufficient_reference_data(self, drift_detector):\n        \"\"\"Test drift check with insufficient reference data.\"\"\"\n        # Add only a few samples (less than min_samples_for_drift=10)\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        result = drift_detector.check_drift()\n\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n        assert result.drift_detected is False\n        assert \"Insufficient reference data\" in result.message\n\n    def test_check_drift_insufficient_current_data(self, drift_detector, reference_data):\n        \"\"\"Test drift check with insufficient current data.\"\"\"\n        # Add enough reference data\n        drift_detector.add_batch(reference_data[:20])\n        drift_detector.lock_reference_data()\n\n        # Clear current and add minimal data\n        drift_detector._current_data.clear()\n        for i in range(5):  # Less than min_samples_for_drift\n            drift_detector.add_data_point({\"f1\": float(i), \"f2\": float(i)})\n\n        result = drift_detector.check_drift()\n\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n        assert \"Insufficient current data\" in result.message\n\n    def test_check_drift_no_drift(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test drift check with no significant drift.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Add similar data to current\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data)\n\n        result = drift_detector.check_drift()\n\n        # With similar distributions, should not detect drift\n        # Note: The actual result depends on the data, but we can check structure\n        assert isinstance(result, DriftResult)\n        assert result.reference_size == 100\n        assert result.drift_threshold == drift_detector.drift_threshold\n\n    def test_check_drift_detected(self, drift_detector, reference_data, drifted_data):\n        \"\"\"Test drift detection when distribution shifts significantly.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Add clearly drifted data to current\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(drifted_data)\n\n        result = drift_detector.check_drift()\n\n        # With significantly different distributions, should detect drift\n        assert isinstance(result, DriftResult)\n        assert result.reference_size == 100\n        assert result.current_size >= len(drifted_data)\n        # Note: Actual drift detection depends on Evidently's algorithms\n\n    def test_check_drift_increments_counter(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test that drift check increments check counter.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        assert drift_detector._total_checks == 0\n\n        drift_detector.check_drift()\n        assert drift_detector._total_checks == 1\n\n        drift_detector.check_drift()\n        assert drift_detector._total_checks == 2\n\n    def test_check_drift_updates_last_check_time(self, drift_detector, reference_data):\n        \"\"\"Test that drift check updates last check time.\"\"\"\n        drift_detector.add_batch(reference_data)\n\n        assert drift_detector._last_check_time is None\n\n        before = time.time()\n        drift_detector.check_drift()\n        after = time.time()\n\n        assert drift_detector._last_check_time is not None\n        assert before <= drift_detector._last_check_time <= after\n\n    def test_check_drift_no_common_columns(self, drift_detector):\n        \"\"\"Test drift check with no common feature columns.\"\"\"\n        # Add reference with features a, b\n        drift_detector.add_batch([{\"a\": 1.0, \"b\": 2.0}] * 20)\n        drift_detector.lock_reference_data()\n\n        # Clear and add current with features c, d (different columns)\n        drift_detector._current_data.clear()\n        drift_detector.add_batch([{\"c\": 1.0, \"d\": 2.0}] * 20)\n\n        result = drift_detector.check_drift()\n\n        assert result.status == DriftStatus.ERROR\n        assert \"No common feature columns\" in result.message\n\n    @pytest.mark.asyncio\n    async def test_check_drift_async(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test async drift check.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        result = await drift_detector.check_drift_async()\n\n        assert isinstance(result, DriftResult)\n\n\n# =============================================================================\n# DriftDetector Tests - Status and Metrics\n# =============================================================================\n\n\nclass TestDriftDetectorStatusMetrics:\n    \"\"\"Tests for status and metrics retrieval.\"\"\"\n\n    def test_get_status_no_check_performed(self, drift_detector):\n        \"\"\"Test get_status when no check has been performed.\"\"\"\n        result = drift_detector.get_status()\n\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n        assert result.drift_detected is False\n        assert result.message == \"No drift check performed yet\"\n\n    def test_get_status_after_check(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test get_status after performing a check.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        drift_detector.check_drift()\n\n        status = drift_detector.get_status()\n        assert isinstance(status, DriftResult)\n        assert status.timestamp is not None\n\n    def test_get_metrics(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test get_metrics returns valid DriftMetrics.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        drift_detector.check_drift()\n\n        metrics = drift_detector.get_metrics()\n\n        assert isinstance(metrics, DriftMetrics)\n        assert metrics.total_checks == 1\n        assert metrics.status in list(DriftStatus)\n        assert metrics.data_points_collected > 0\n\n    def test_get_reference_data(self, drift_detector, reference_data):\n        \"\"\"Test get_reference_data returns DataFrame.\"\"\"\n        drift_detector.add_batch(reference_data[:10])\n\n        df = drift_detector.get_reference_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) > 0\n\n    def test_get_current_data(self, drift_detector, reference_data):\n        \"\"\"Test get_current_data returns DataFrame.\"\"\"\n        drift_detector.add_batch(reference_data[:10])\n\n        df = drift_detector.get_current_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) > 0\n\n    def test_get_reference_data_empty(self, drift_detector):\n        \"\"\"Test get_reference_data returns empty DataFrame when no data.\"\"\"\n        df = drift_detector.get_reference_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) == 0\n\n    def test_get_current_data_empty(self, drift_detector):\n        \"\"\"Test get_current_data returns empty DataFrame when no data.\"\"\"\n        df = drift_detector.get_current_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) == 0\n\n\n# =============================================================================\n# DriftDetector Tests - Alert Management\n# =============================================================================\n\n\nclass TestDriftDetectorAlerts:\n    \"\"\"Tests for drift alert management.\"\"\"\n\n    def test_register_alert_callback(self, drift_detector):\n        \"\"\"Test registering alert callback.\"\"\"\n        callback_results = []\n\n        def on_alert(alert: DriftAlert):\n            callback_results.append(alert)\n\n        drift_detector.register_alert_callback(on_alert)\n\n        assert len(drift_detector._alert_callbacks) == 1\n\n    def test_get_pending_alerts_empty(self, drift_detector):\n        \"\"\"Test get_pending_alerts when no alerts exist.\"\"\"\n        alerts = drift_detector.get_pending_alerts()\n        assert alerts == []\n\n    def test_acknowledge_alert(self, drift_detector):\n        \"\"\"Test acknowledging a drift alert.\"\"\"\n        # Manually create an alert for testing\n        alert = DriftAlert(\n            drift_result=DriftResult(\n                status=DriftStatus.DRIFT_DETECTED,\n                drift_detected=True,\n                drift_score=0.5,\n                drift_threshold=0.2,\n                columns_drifted={\"f1\": True},\n                column_drift_scores={\"f1\": 0.5},\n                reference_size=100,\n                current_size=50,\n            ),\n            severity=\"warning\",\n        )\n        drift_detector._pending_alerts.append(alert)\n\n        # Acknowledge it\n        result = drift_detector.acknowledge_alert(alert.alert_id)\n\n        assert result is True\n        assert alert.acknowledged is True\n\n    def test_acknowledge_alert_not_found(self, drift_detector):\n        \"\"\"Test acknowledging non-existent alert.\"\"\"\n        result = drift_detector.acknowledge_alert(\"nonexistent_alert_id\")\n        assert result is False\n\n    def test_alert_callback_triggered_on_drift(self, drift_detector, reference_data, drifted_data):\n        \"\"\"Test that alert callback is triggered when drift is detected.\"\"\"\n        callback_results = []\n\n        def on_alert(alert: DriftAlert):\n            callback_results.append(alert)\n\n        drift_detector.register_alert_callback(on_alert)\n\n        # Set up data for drift\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(drifted_data)\n\n        result = drift_detector.check_drift()\n\n        # If drift was detected, callback should have been triggered\n        if result.drift_detected:\n            assert len(callback_results) > 0\n            assert callback_results[0].severity in [\"warning\", \"critical\"]\n\n\n# =============================================================================\n# DriftDetector Tests - Enable/Disable\n# =============================================================================\n\n\nclass TestDriftDetectorEnableDisable:\n    \"\"\"Tests for enable/disable functionality.\"\"\"\n\n    def test_enable(self):\n        \"\"\"Test enabling drift detection.\"\"\"\n        detector = DriftDetector(enabled=False)\n        assert detector._enabled is False\n\n        detector.enable()\n\n        assert detector._enabled is True\n\n    def test_disable(self, drift_detector):\n        \"\"\"Test disabling drift detection.\"\"\"\n        assert drift_detector._enabled is True\n\n        drift_detector.disable()\n\n        assert drift_detector._enabled is False\n        assert drift_detector._current_status == DriftStatus.DISABLED\n\n    def test_is_enabled(self, drift_detector):\n        \"\"\"Test is_enabled method.\"\"\"\n        assert drift_detector.is_enabled() is True\n\n        drift_detector.disable()\n        assert drift_detector.is_enabled() is False\n\n\n# =============================================================================\n# DriftDetector Tests - Reset\n# =============================================================================\n\n\nclass TestDriftDetectorReset:\n    \"\"\"Tests for reset functionality.\"\"\"\n\n    def test_reset_clears_data(self, drift_detector, reference_data):\n        \"\"\"Test that reset clears all data.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.check_drift()\n\n        assert len(drift_detector._reference_data) > 0\n        assert drift_detector._total_checks > 0\n\n        drift_detector.reset()\n\n        assert len(drift_detector._reference_data) == 0\n        assert len(drift_detector._current_data) == 0\n        assert len(drift_detector._all_data) == 0\n        assert drift_detector._total_checks == 0\n        assert drift_detector._drift_detections == 0\n        assert drift_detector._consecutive_drift_count == 0\n        assert len(drift_detector._pending_alerts) == 0\n\n    def test_reset_unlocks_reference(self, drift_detector, reference_data):\n        \"\"\"Test that reset unlocks reference data.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        assert drift_detector._reference_locked is True\n\n        drift_detector.reset()\n\n        assert drift_detector._reference_locked is False\n\n\n# =============================================================================\n# DriftDetector Tests - Thread Safety\n# =============================================================================\n\n\nclass TestDriftDetectorThreadSafety:\n    \"\"\"Tests for DriftDetector thread safety.\"\"\"\n\n    def test_concurrent_data_additions(self, drift_detector):\n        \"\"\"Test concurrent data additions don't cause race conditions.\"\"\"\n        errors = []\n\n        def add_data():\n            try:\n                for i in range(100):\n                    drift_detector.add_data_point({\"x\": float(i), \"y\": float(i * 2)})\n            except Exception as e:\n                errors.append(e)\n\n        threads = [threading.Thread(target=add_data) for _ in range(5)]\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        assert len(errors) == 0\n        # All data should be added (500 total)\n        assert len(drift_detector._all_data) == 500\n\n\n# =============================================================================\n# DriftDetector Tests - Repr\n# =============================================================================\n\n\nclass TestDriftDetectorRepr:\n    \"\"\"Tests for DriftDetector string representation.\"\"\"\n\n    def test_repr(self, drift_detector):\n        \"\"\"Test __repr__ returns informative string.\"\"\"\n        repr_str = repr(drift_detector)\n\n        assert \"DriftDetector\" in repr_str\n        assert \"status=\" in repr_str\n        assert \"ref_size=\" in repr_str\n        assert \"cur_size=\" in repr_str\n\n\n# =============================================================================\n# MetricsRegistry Tests - Initialization\n# =============================================================================\n\n\nclass TestMetricsRegistryInit:\n    \"\"\"Tests for MetricsRegistry initialization.\"\"\"\n\n    def test_default_initialization(self):\n        \"\"\"Test default initialization creates valid registry.\"\"\"\n        registry = CollectorRegistry()\n        metrics = MetricsRegistry(registry=registry)\n\n        assert metrics._prefix == \"adaptive_learning\"\n        assert metrics._current_model_version == \"unknown\"\n        assert metrics._cold_start_active is True\n\n    def test_custom_initialization(self):\n        \"\"\"Test initialization with custom parameters.\"\"\"\n        registry = CollectorRegistry()\n        custom_buckets = (0.01, 0.1, 1.0)\n        metrics = MetricsRegistry(\n            registry=registry,\n            prefix=\"custom_prefix\",\n            latency_buckets=custom_buckets,\n        )\n\n        assert metrics._prefix == \"custom_prefix\"\n        assert metrics._latency_buckets == custom_buckets\n\n    def test_counters_initialized(self, metrics_registry):\n        \"\"\"Test that all counters are initialized.\"\"\"\n        assert metrics_registry.predictions_total is not None\n        assert metrics_registry.training_samples_total is not None\n        assert metrics_registry.errors_total is not None\n        assert metrics_registry.safety_violations_total is not None\n        assert metrics_registry.drift_checks_total is not None\n        assert metrics_registry.model_swaps_total is not None\n        assert metrics_registry.rollbacks_total is not None\n\n    def test_gauges_initialized(self, metrics_registry):\n        \"\"\"Test that all gauges are initialized.\"\"\"\n        assert metrics_registry.model_accuracy is not None\n        assert metrics_registry.drift_score is not None\n        assert metrics_registry.drift_threshold is not None\n        assert metrics_registry.training_queue_size is not None\n        assert metrics_registry.model_is_cold_start is not None\n        assert metrics_registry.uptime_seconds is not None\n\n    def test_histograms_initialized(self, metrics_registry):\n        \"\"\"Test that all histograms are initialized.\"\"\"\n        assert metrics_registry.prediction_latency is not None\n        assert metrics_registry.training_latency is not None\n        assert metrics_registry.drift_check_latency is not None\n        assert metrics_registry.model_swap_latency is not None\n\n\n# =============================================================================\n# MetricsRegistry Tests - Recording Methods\n# =============================================================================\n\n\nclass TestMetricsRegistryRecording:\n    \"\"\"Tests for metrics recording methods.\"\"\"\n\n    def test_record_prediction_success(self, metrics_registry):\n        \"\"\"Test recording successful prediction.\"\"\"\n        metrics_registry.record_prediction(\n            latency_seconds=0.012,\n            model_version=\"v1\",\n            success=True,\n        )\n\n        # Verify counter was incremented\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"success\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_prediction_failure(self, metrics_registry):\n        \"\"\"Test recording failed prediction.\"\"\"\n        metrics_registry.record_prediction(\n            latency_seconds=0.05,\n            model_version=\"v1\",\n            success=False,\n        )\n\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"error\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_training(self, metrics_registry):\n        \"\"\"Test recording training event.\"\"\"\n        metrics_registry.record_training(\n            latency_seconds=0.003,\n            model_version=\"v1\",\n            batch_size=10,\n        )\n\n        count = metrics_registry.training_samples_total.labels(model_version=\"v1\")._value.get()\n        assert count == 10\n\n    def test_record_error(self, metrics_registry):\n        \"\"\"Test recording error event.\"\"\"\n        metrics_registry.record_error(\n            error_type=\"validation_error\",\n            endpoint=\"/predict\",\n        )\n\n        count = metrics_registry.errors_total.labels(\n            error_type=\"validation_error\",\n            endpoint=\"/predict\",\n        )._value.get()\n        assert count == 1\n\n    def test_record_safety_violation(self, metrics_registry):\n        \"\"\"Test recording safety violation.\"\"\"\n        metrics_registry.record_safety_violation(result=\"rejected\")\n\n        count = metrics_registry.safety_violations_total.labels(\n            safety_result=\"rejected\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_drift_check(self, metrics_registry):\n        \"\"\"Test recording drift check.\"\"\"\n        metrics_registry.record_drift_check(\n            latency_seconds=0.5,\n            drift_detected=True,\n            drift_score=0.3,\n        )\n\n        count = metrics_registry.drift_checks_total.labels(\n            drift_status=\"drift_detected\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_model_swap_success(self, metrics_registry):\n        \"\"\"Test recording successful model swap.\"\"\"\n        metrics_registry.record_model_swap(latency_seconds=0.1, success=True)\n\n        count = metrics_registry.model_swaps_total.labels(status=\"success\")._value.get()\n        assert count == 1\n\n    def test_record_model_swap_failure(self, metrics_registry):\n        \"\"\"Test recording failed model swap.\"\"\"\n        metrics_registry.record_model_swap(latency_seconds=0.2, success=False)\n\n        count = metrics_registry.model_swaps_total.labels(status=\"error\")._value.get()\n        assert count == 1\n\n    def test_record_rollback(self, metrics_registry):\n        \"\"\"Test recording rollback event.\"\"\"\n        metrics_registry.record_rollback()\n        metrics_registry.record_rollback()\n\n        count = metrics_registry.rollbacks_total._value.get()\n        assert count == 2\n\n\n# =============================================================================\n# MetricsRegistry Tests - Gauge Setters\n# =============================================================================\n\n\nclass TestMetricsRegistryGauges:\n    \"\"\"Tests for gauge setter methods.\"\"\"\n\n    def test_set_model_accuracy(self, metrics_registry):\n        \"\"\"Test setting model accuracy gauge.\"\"\"\n        metrics_registry.set_model_accuracy(0.92, model_version=\"v1\")\n\n        value = metrics_registry.model_accuracy.labels(model_version=\"v1\")._value.get()\n        assert value == 0.92\n\n    def test_set_model_f1(self, metrics_registry):\n        \"\"\"Test setting model F1 score gauge.\"\"\"\n        metrics_registry.set_model_f1(0.88, model_version=\"v1\")\n\n        value = metrics_registry.model_f1_score.labels(model_version=\"v1\")._value.get()\n        assert value == 0.88\n\n    def test_set_drift_score(self, metrics_registry):\n        \"\"\"Test setting drift score gauge.\"\"\"\n        metrics_registry.set_drift_score(0.25)\n\n        value = metrics_registry.drift_score._value.get()\n        assert value == 0.25\n\n    def test_set_drift_threshold(self, metrics_registry):\n        \"\"\"Test setting drift threshold gauge.\"\"\"\n        metrics_registry.set_drift_threshold(0.2)\n\n        value = metrics_registry.drift_threshold._value.get()\n        assert value == 0.2\n\n    def test_set_training_queue_size(self, metrics_registry):\n        \"\"\"Test setting training queue size gauge.\"\"\"\n        metrics_registry.set_training_queue_size(100)\n\n        value = metrics_registry.training_queue_size._value.get()\n        assert value == 100\n\n    def test_set_reference_data_size(self, metrics_registry):\n        \"\"\"Test setting reference data size gauge.\"\"\"\n        metrics_registry.set_reference_data_size(1000)\n\n        value = metrics_registry.reference_data_size._value.get()\n        assert value == 1000\n\n    def test_set_current_data_size(self, metrics_registry):\n        \"\"\"Test setting current data size gauge.\"\"\"\n        metrics_registry.set_current_data_size(100)\n\n        value = metrics_registry.current_data_size._value.get()\n        assert value == 100\n\n    def test_set_model_samples_trained(self, metrics_registry):\n        \"\"\"Test setting model samples trained gauge.\"\"\"\n        metrics_registry.set_model_samples_trained(5000, model_version=\"v1\")\n\n        value = metrics_registry.model_samples_trained.labels(model_version=\"v1\")._value.get()\n        assert value == 5000\n\n    def test_set_cold_start(self, metrics_registry):\n        \"\"\"Test setting cold start gauge.\"\"\"\n        metrics_registry.set_cold_start(True)\n        assert metrics_registry.model_is_cold_start._value.get() == 1\n\n        metrics_registry.set_cold_start(False)\n        assert metrics_registry.model_is_cold_start._value.get() == 0\n\n    def test_set_consecutive_safety_failures(self, metrics_registry):\n        \"\"\"Test setting consecutive safety failures gauge.\"\"\"\n        metrics_registry.set_consecutive_safety_failures(3)\n\n        value = metrics_registry.consecutive_safety_failures._value.get()\n        assert value == 3\n\n    def test_set_safety_threshold(self, metrics_registry):\n        \"\"\"Test setting safety threshold gauge.\"\"\"\n        metrics_registry.set_safety_threshold(0.85)\n\n        value = metrics_registry.safety_threshold._value.get()\n        assert value == 0.85\n\n\n# =============================================================================\n# MetricsRegistry Tests - Info Setters\n# =============================================================================\n\n\nclass TestMetricsRegistryInfo:\n    \"\"\"Tests for info metric setters.\"\"\"\n\n    def test_set_service_info(self, metrics_registry):\n        \"\"\"Test setting service info.\"\"\"\n        metrics_registry.set_service_info(\n            version=\"1.0.0\",\n            environment=\"production\",\n            constitutional_hash=\"abc123\",\n        )\n\n        # Info metrics don't have a simple _value getter, just verify no error\n        assert True\n\n    def test_set_model_info(self, metrics_registry):\n        \"\"\"Test setting model info.\"\"\"\n        metrics_registry.set_model_info(\n            model_type=\"LogisticRegression\",\n            model_version=\"v2\",\n            algorithm=\"river.linear_model.LogisticRegression\",\n            mlflow_run_id=\"run_123\",\n        )\n\n        assert metrics_registry._current_model_version == \"v2\"\n\n\n# =============================================================================\n# MetricsRegistry Tests - Context Managers\n# =============================================================================\n\n\nclass TestMetricsRegistryContextManagers:\n    \"\"\"Tests for timing context managers.\"\"\"\n\n    def test_prediction_timer(self, metrics_registry):\n        \"\"\"Test prediction timer context manager.\"\"\"\n        with metrics_registry.prediction_timer(model_version=\"v1\"):\n            time.sleep(0.01)\n\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"success\"\n        )._value.get()\n        assert count == 1\n\n    def test_prediction_timer_with_error(self, metrics_registry):\n        \"\"\"Test prediction timer records error on exception.\"\"\"\n        with pytest.raises(ValueError):\n            with metrics_registry.prediction_timer(model_version=\"v1\"):\n                raise ValueError(\"Test error\")\n\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"error\"\n        )._value.get()\n        assert count == 1\n\n    def test_training_timer(self, metrics_registry):\n        \"\"\"Test training timer context manager.\"\"\"\n        with metrics_registry.training_timer(model_version=\"v1\", batch_size=5):\n            time.sleep(0.01)\n\n        count = metrics_registry.training_samples_total.labels(model_version=\"v1\")._value.get()\n        assert count == 5\n\n    def test_drift_check_timer(self, metrics_registry):\n        \"\"\"Test drift check timer context manager.\"\"\"\n        with metrics_registry.drift_check_timer():\n            time.sleep(0.01)\n\n        # Verify histogram observed a value\n        assert True  # Histogram observation doesn't have simple value getter\n\n    def test_model_swap_timer(self, metrics_registry):\n        \"\"\"Test model swap timer context manager.\"\"\"\n        with metrics_registry.model_swap_timer():\n            time.sleep(0.01)\n\n        count = metrics_registry.model_swaps_total.labels(status=\"success\")._value.get()\n        assert count == 1\n\n\n# =============================================================================\n# MetricsRegistry Tests - Metrics Export\n# =============================================================================\n\n\nclass TestMetricsRegistryExport:\n    \"\"\"Tests for metrics export functionality.\"\"\"\n\n    def test_generate_metrics(self, metrics_registry):\n        \"\"\"Test generating Prometheus metrics output.\"\"\"\n        metrics_registry.record_prediction(0.01, \"v1\", True)\n        metrics_registry.set_model_accuracy(0.92, \"v1\")\n\n        output = metrics_registry.generate_metrics()\n\n        assert isinstance(output, bytes)\n        assert b\"test_adaptive_predictions_total\" in output\n        assert b\"test_adaptive_model_accuracy\" in output\n\n    def test_generate_metrics_updates_uptime(self, metrics_registry):\n        \"\"\"Test that generate_metrics updates uptime.\"\"\"\n        time.sleep(0.1)\n        metrics_registry.generate_metrics()\n\n        uptime = metrics_registry.uptime_seconds._value.get()\n        assert uptime > 0\n\n    def test_get_content_type(self, metrics_registry):\n        \"\"\"Test getting Prometheus content type.\"\"\"\n        content_type = metrics_registry.get_content_type()\n\n        assert \"text/plain\" in content_type or \"openmetrics\" in content_type\n\n\n# =============================================================================\n# MetricsRegistry Tests - Callback Registration\n# =============================================================================\n\n\nclass TestMetricsRegistryCallbacks:\n    \"\"\"Tests for callback registration.\"\"\"\n\n    def test_register_update_callback(self, metrics_registry):\n        \"\"\"Test registering update callback.\"\"\"\n        callback_called = [False]\n\n        def my_callback():\n            callback_called[0] = True\n\n        metrics_registry.register_update_callback(my_callback)\n        metrics_registry.generate_metrics()\n\n        assert callback_called[0] is True\n\n    def test_unregister_update_callback(self, metrics_registry):\n        \"\"\"Test unregistering update callback.\"\"\"\n\n        def my_callback():\n            pass\n\n        metrics_registry.register_update_callback(my_callback)\n        assert len(metrics_registry._update_callbacks) == 1\n\n        result = metrics_registry.unregister_update_callback(my_callback)\n        assert result is True\n        assert len(metrics_registry._update_callbacks) == 0\n\n    def test_unregister_nonexistent_callback(self, metrics_registry):\n        \"\"\"Test unregistering callback that doesn't exist.\"\"\"\n\n        def my_callback():\n            pass\n\n        result = metrics_registry.unregister_update_callback(my_callback)\n        assert result is False\n\n\n# =============================================================================\n# MetricsRegistry Tests - Snapshot\n# =============================================================================\n\n\nclass TestMetricsRegistrySnapshot:\n    \"\"\"Tests for metrics snapshot functionality.\"\"\"\n\n    def test_get_snapshot(self, metrics_registry):\n        \"\"\"Test getting metrics snapshot.\"\"\"\n        metrics_registry.record_prediction(0.01, \"v1\", True)\n        metrics_registry.record_training(0.001, \"v1\", 10)\n        metrics_registry.set_drift_score(0.15)\n\n        snapshot = metrics_registry.get_snapshot()\n\n        assert isinstance(snapshot, MetricsSnapshot)\n        assert snapshot.predictions_total >= 1\n        assert snapshot.training_samples_total >= 10\n        assert snapshot.drift_score == 0.15\n        assert snapshot.timestamp > 0\n\n    def test_snapshot_contains_all_fields(self, metrics_registry):\n        \"\"\"Test that snapshot contains all required fields.\"\"\"\n        snapshot = metrics_registry.get_snapshot()\n\n        assert hasattr(snapshot, \"predictions_total\")\n        assert hasattr(snapshot, \"training_samples_total\")\n        assert hasattr(snapshot, \"model_accuracy\")\n        assert hasattr(snapshot, \"drift_score\")\n        assert hasattr(snapshot, \"prediction_latency_p50\")\n        assert hasattr(snapshot, \"prediction_latency_p95\")\n        assert hasattr(snapshot, \"prediction_latency_p99\")\n        assert hasattr(snapshot, \"model_version\")\n        assert hasattr(snapshot, \"cold_start_active\")\n        assert hasattr(snapshot, \"safety_violations\")\n\n\n# =============================================================================\n# MetricsRegistry Tests - Reset\n# =============================================================================\n\n\nclass TestMetricsRegistryReset:\n    \"\"\"Tests for reset functionality.\"\"\"\n\n    def test_reset_clears_state(self, metrics_registry):\n        \"\"\"Test that reset clears all state.\"\"\"\n        metrics_registry.record_prediction(0.01, \"v1\", True)\n        metrics_registry._current_model_version = \"v2\"\n\n        def my_callback():\n            pass\n\n        metrics_registry.register_update_callback(my_callback)\n\n        metrics_registry.reset()\n\n        assert metrics_registry._current_model_version == \"unknown\"\n        assert metrics_registry._cold_start_active is True\n        assert len(metrics_registry._update_callbacks) == 0\n\n\n# =============================================================================\n# MetricsRegistry Tests - Thread Safety\n# =============================================================================\n\n\nclass TestMetricsRegistryThreadSafety:\n    \"\"\"Tests for thread safety.\"\"\"\n\n    def test_concurrent_recording(self, metrics_registry):\n        \"\"\"Test concurrent metric recording doesn't cause race conditions.\"\"\"\n        errors = []\n\n        def record_predictions():\n            try:\n                for _ in range(100):\n                    metrics_registry.record_prediction(0.01, \"v1\", True)\n            except Exception as e:\n                errors.append(e)\n\n        def record_training():\n            try:\n                for _ in range(100):\n                    metrics_registry.record_training(0.001, \"v1\", 1)\n            except Exception as e:\n                errors.append(e)\n\n        threads = [\n            threading.Thread(target=record_predictions),\n            threading.Thread(target=record_training),\n            threading.Thread(target=record_predictions),\n            threading.Thread(target=record_training),\n        ]\n\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        assert len(errors) == 0\n\n\n# =============================================================================\n# MetricsRegistry Tests - Repr and Properties\n# =============================================================================\n\n\nclass TestMetricsRegistryRepr:\n    \"\"\"Tests for string representation.\"\"\"\n\n    def test_repr(self, metrics_registry):\n        \"\"\"Test __repr__ returns informative string.\"\"\"\n        repr_str = repr(metrics_registry)\n\n        assert \"MetricsRegistry\" in repr_str\n        assert \"prefix=\" in repr_str\n        assert \"model_version=\" in repr_str\n\n    def test_registry_property(self, metrics_registry):\n        \"\"\"Test registry property returns CollectorRegistry.\"\"\"\n        registry = metrics_registry.registry\n\n        assert isinstance(registry, CollectorRegistry)\n\n\n# =============================================================================\n# MetricsRegistry Tests - Global Registry\n# =============================================================================\n\n\nclass TestGlobalMetricsRegistry:\n    \"\"\"Tests for global metrics registry functions.\"\"\"\n\n    def test_get_metrics_registry(self):\n        \"\"\"Test get_metrics_registry returns global instance.\"\"\"\n        registry = get_metrics_registry()\n\n        assert isinstance(registry, MetricsRegistry)\n\n    def test_create_metrics_registry(self):\n        \"\"\"Test create_metrics_registry creates new instance.\"\"\"\n        registry1 = create_metrics_registry(prefix=\"test1\")\n        registry2 = create_metrics_registry(prefix=\"test2\")\n\n        assert registry1 is not registry2\n        assert registry1._prefix == \"test1\"\n        assert registry2._prefix == \"test2\"\n\n\n# =============================================================================\n# MetricLabel Enum Tests\n# =============================================================================\n\n\nclass TestMetricLabel:\n    \"\"\"Tests for MetricLabel enum.\"\"\"\n\n    def test_metric_label_values(self):\n        \"\"\"Test MetricLabel enum values.\"\"\"\n        assert MetricLabel.MODEL_TYPE == \"model_type\"\n        assert MetricLabel.MODEL_VERSION == \"model_version\"\n        assert MetricLabel.ENDPOINT == \"endpoint\"\n        assert MetricLabel.STATUS == \"status\"\n        assert MetricLabel.ERROR_TYPE == \"error_type\"\n        assert MetricLabel.DRIFT_STATUS == \"drift_status\"\n        assert MetricLabel.SAFETY_RESULT == \"safety_result\"\n\n\n# =============================================================================\n# Integration Tests - Drift Detection with Metrics\n# =============================================================================\n\n\nclass TestDriftDetectorMetricsIntegration:\n    \"\"\"Tests for integration between DriftDetector and MetricsRegistry.\"\"\"\n\n    def test_drift_check_updates_metrics(self, metrics_registry, reference_data, similar_data):\n        \"\"\"Test that drift check can update metrics registry.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            min_samples_for_drift=10,\n        )\n\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:20])\n\n        # Simulate what the API would do\n        start_time = time.time()\n        result = detector.check_drift()\n        latency = time.time() - start_time\n\n        # Record in metrics\n        metrics_registry.record_drift_check(\n            latency_seconds=latency,\n            drift_detected=result.drift_detected,\n            drift_score=result.drift_score,\n        )\n        metrics_registry.set_reference_data_size(len(detector._reference_data))\n        metrics_registry.set_current_data_size(len(detector._current_data))\n\n        # Verify metrics were updated\n        ref_size = metrics_registry.reference_data_size._value.get()\n        cur_size = metrics_registry.current_data_size._value.get()\n\n        assert ref_size == 100\n        assert cur_size >= 20\n",
        "timestamp": "2026-01-03T19:07:33.686648"
      },
      "worktree_state": {
        "content": "\"\"\"\nUnit tests for the Adaptive Learning Engine monitoring module.\n\nTests cover:\n- DriftDetector: Evidently-based drift detection\n- MetricsRegistry: Prometheus metrics collection\n\nConstitutional Hash: cdd01ef066bc6cf2\n\"\"\"\n\nimport threading\nimport time\nfrom typing import Any, Dict, List\nfrom unittest.mock import MagicMock, patch\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom prometheus_client import CollectorRegistry\n\nfrom src.monitoring.drift_detector import (\n    DriftAlert,\n    DriftDetector,\n    DriftMetrics,\n    DriftResult,\n    DriftStatus,\n)\nfrom src.monitoring.metrics import (\n    MetricLabel,\n    MetricsRegistry,\n    MetricsSnapshot,\n    create_metrics_registry,\n    get_metrics_registry,\n)\n\n\n# =============================================================================\n# Test Fixtures\n# =============================================================================\n\n\n@pytest.fixture\ndef sample_features() -> Dict[str, Any]:\n    \"\"\"Sample feature dictionary for testing.\"\"\"\n    return {\n        \"feature_a\": 1.0,\n        \"feature_b\": 2.5,\n        \"feature_c\": 0.3,\n    }\n\n\n@pytest.fixture\ndef reference_data() -> List[Dict[str, Any]]:\n    \"\"\"Sample reference dataset for drift testing.\"\"\"\n    np.random.seed(42)\n    return [{\"f1\": np.random.normal(0, 1), \"f2\": np.random.normal(0, 1)} for _ in range(100)]\n\n\n@pytest.fixture\ndef drifted_data() -> List[Dict[str, Any]]:\n    \"\"\"Sample current data with drift (different distribution).\"\"\"\n    np.random.seed(123)\n    return [\n        {\"f1\": np.random.normal(5, 2), \"f2\": np.random.normal(-5, 2)}  # Shifted distribution\n        for _ in range(100)\n    ]\n\n\n@pytest.fixture\ndef similar_data() -> List[Dict[str, Any]]:\n    \"\"\"Sample current data similar to reference (no drift).\"\"\"\n    np.random.seed(456)\n    return [\n        {\"f1\": np.random.normal(0.1, 1.1), \"f2\": np.random.normal(-0.1, 0.9)}  # Similar\n        for _ in range(100)\n    ]\n\n\n@pytest.fixture\ndef drift_detector() -> DriftDetector:\n    \"\"\"Fresh DriftDetector for testing.\"\"\"\n    return DriftDetector(\n        drift_threshold=0.2,\n        reference_window_size=100,\n        current_window_size=50,\n        min_samples_for_drift=10,\n        check_interval_seconds=60,\n        drift_share_threshold=0.5,\n        enabled=True,\n    )\n\n\n@pytest.fixture\ndef metrics_registry() -> MetricsRegistry:\n    \"\"\"Fresh MetricsRegistry for testing with isolated registry.\"\"\"\n    registry = CollectorRegistry()\n    return MetricsRegistry(registry=registry, prefix=\"test_adaptive\")\n\n\n# =============================================================================\n# DriftDetector Tests - Initialization\n# =============================================================================\n\n\nclass TestDriftDetectorInit:\n    \"\"\"Tests for DriftDetector initialization.\"\"\"\n\n    def test_default_initialization(self):\n        \"\"\"Test default initialization creates valid detector.\"\"\"\n        detector = DriftDetector()\n\n        assert detector.drift_threshold == 0.2\n        assert detector.reference_window_size == 1000\n        assert detector.current_window_size == 100\n        assert detector.min_samples_for_drift == 10\n        assert detector.check_interval_seconds == 300\n        assert detector.drift_share_threshold == 0.5\n        assert detector._enabled is True\n        assert detector._current_status == DriftStatus.INSUFFICIENT_DATA\n\n    def test_custom_initialization(self):\n        \"\"\"Test initialization with custom parameters.\"\"\"\n        detector = DriftDetector(\n            drift_threshold=0.3,\n            reference_window_size=500,\n            current_window_size=25,\n            min_samples_for_drift=20,\n            check_interval_seconds=120,\n            drift_share_threshold=0.7,\n            enabled=False,\n        )\n\n        assert detector.drift_threshold == 0.3\n        assert detector.reference_window_size == 500\n        assert detector.current_window_size == 25\n        assert detector.min_samples_for_drift == 20\n        assert detector.check_interval_seconds == 120\n        assert detector.drift_share_threshold == 0.7\n        assert detector._enabled is False\n        assert detector._current_status == DriftStatus.DISABLED\n\n    def test_data_windows_initialized_empty(self, drift_detector):\n        \"\"\"Test that data windows start empty.\"\"\"\n        assert len(drift_detector._reference_data) == 0\n        assert len(drift_detector._current_data) == 0\n        assert len(drift_detector._all_data) == 0\n\n\n# =============================================================================\n# DriftDetector Tests - Data Management\n# =============================================================================\n\n\nclass TestDriftDetectorDataManagement:\n    \"\"\"Tests for DriftDetector data point management.\"\"\"\n\n    def test_add_data_point(self, drift_detector, sample_features):\n        \"\"\"Test adding a single data point.\"\"\"\n        drift_detector.add_data_point(\n            features=sample_features,\n            label=1,\n            prediction=1,\n        )\n\n        assert len(drift_detector._current_data) == 1\n        assert len(drift_detector._reference_data) == 1\n        assert \"feature_a\" in drift_detector._known_columns\n\n    def test_add_data_point_disabled(self, sample_features):\n        \"\"\"Test that data points are ignored when disabled.\"\"\"\n        detector = DriftDetector(enabled=False)\n        detector.add_data_point(features=sample_features, label=1)\n\n        assert len(detector._current_data) == 0\n\n    def test_add_data_point_updates_current_window(self, drift_detector):\n        \"\"\"Test that data points are added to current window.\"\"\"\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        assert len(drift_detector._current_data) == 5\n        assert len(drift_detector._reference_data) == 5\n\n    def test_add_data_point_with_timestamp(self, drift_detector, sample_features):\n        \"\"\"Test adding data point with custom timestamp.\"\"\"\n        custom_time = 1234567890.0\n        drift_detector.add_data_point(\n            features=sample_features,\n            timestamp=custom_time,\n        )\n\n        record = list(drift_detector._current_data)[0]\n        assert record[\"_timestamp\"] == custom_time\n\n    def test_add_batch(self, drift_detector):\n        \"\"\"Test adding multiple data points at once.\"\"\"\n        data_points = [{\"x\": float(i)} for i in range(10)]\n        labels = [i % 2 for i in range(10)]\n        predictions = [i % 2 for i in range(10)]\n\n        count = drift_detector.add_batch(data_points, labels, predictions)\n\n        assert count == 10\n        assert len(drift_detector._current_data) == 10\n\n    def test_add_batch_partial_labels(self, drift_detector):\n        \"\"\"Test batch add with fewer labels than data points.\"\"\"\n        data_points = [{\"x\": float(i)} for i in range(10)]\n        labels = [1, 0, 1]  # Only 3 labels\n\n        count = drift_detector.add_batch(data_points, labels)\n\n        assert count == 10\n        # First 3 should have labels, rest should not\n        records = list(drift_detector._current_data)\n        assert \"_label\" in records[0]\n        assert \"_label\" not in records[5]\n\n\n# =============================================================================\n# DriftDetector Tests - Reference Data Management\n# =============================================================================\n\n\nclass TestDriftDetectorReferenceManagement:\n    \"\"\"Tests for reference data management.\"\"\"\n\n    def test_lock_reference_data(self, drift_detector, sample_features):\n        \"\"\"Test locking reference data.\"\"\"\n        drift_detector.add_data_point(sample_features)\n        drift_detector.lock_reference_data()\n\n        assert drift_detector._reference_locked is True\n\n    def test_locked_reference_not_updated(self, drift_detector, sample_features):\n        \"\"\"Test that locked reference is not updated with new data.\"\"\"\n        drift_detector.add_data_point(sample_features)\n        drift_detector.lock_reference_data()\n\n        # Add more data\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        # Reference should still have only 1 entry\n        assert len(drift_detector._reference_data) == 1\n        # Current should have all data\n        assert len(drift_detector._current_data) == 6\n\n    def test_unlock_reference_data(self, drift_detector, sample_features):\n        \"\"\"Test unlocking reference data.\"\"\"\n        drift_detector.lock_reference_data()\n        drift_detector.unlock_reference_data()\n\n        assert drift_detector._reference_locked is False\n\n    def test_update_reference_from_current(self, drift_detector):\n        \"\"\"Test updating reference from current window.\"\"\"\n        # Add initial data\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        drift_detector.lock_reference_data()\n\n        # Add more to current\n        for i in range(10, 15):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        # Current has 10 entries, reference has 5\n        assert len(drift_detector._current_data) == 10\n        assert len(drift_detector._reference_data) == 5\n\n        # Update reference from current\n        count = drift_detector.update_reference_from_current()\n\n        # Now reference should have 10 entries\n        assert count == 10\n        assert len(drift_detector._reference_data) == 10\n\n    def test_set_reference_data_from_dataframe(self, drift_detector):\n        \"\"\"Test setting reference data from DataFrame.\"\"\"\n        df = pd.DataFrame(\n            {\n                \"f1\": [1.0, 2.0, 3.0, 4.0, 5.0],\n                \"f2\": [0.1, 0.2, 0.3, 0.4, 0.5],\n            }\n        )\n\n        drift_detector.set_reference_data(df)\n\n        assert len(drift_detector._reference_data) == 5\n        assert drift_detector._reference_locked is True\n        assert \"f1\" in drift_detector._known_columns\n        assert \"f2\" in drift_detector._known_columns\n\n\n# =============================================================================\n# DriftDetector Tests - Drift Detection\n# =============================================================================\n\n\nclass TestDriftDetectorDriftCheck:\n    \"\"\"Tests for drift detection functionality.\"\"\"\n\n    def test_check_drift_disabled(self):\n        \"\"\"Test drift check when disabled.\"\"\"\n        detector = DriftDetector(enabled=False)\n        result = detector.check_drift()\n\n        assert result.status == DriftStatus.DISABLED\n        assert result.drift_detected is False\n        assert result.message == \"Drift detection is disabled\"\n\n    def test_check_drift_insufficient_reference_data(self, drift_detector):\n        \"\"\"Test drift check with insufficient reference data.\"\"\"\n        # Add only a few samples (less than min_samples_for_drift=10)\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        result = drift_detector.check_drift()\n\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n        assert result.drift_detected is False\n        assert \"Insufficient reference data\" in result.message\n\n    def test_check_drift_insufficient_current_data(self, drift_detector, reference_data):\n        \"\"\"Test drift check with insufficient current data.\"\"\"\n        # Add enough reference data\n        drift_detector.add_batch(reference_data[:20])\n        drift_detector.lock_reference_data()\n\n        # Clear current and add minimal data\n        drift_detector._current_data.clear()\n        for i in range(5):  # Less than min_samples_for_drift\n            drift_detector.add_data_point({\"f1\": float(i), \"f2\": float(i)})\n\n        result = drift_detector.check_drift()\n\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n        assert \"Insufficient current data\" in result.message\n\n    def test_check_drift_no_drift(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test drift check with no significant drift.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Add similar data to current\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data)\n\n        result = drift_detector.check_drift()\n\n        # With similar distributions, should not detect drift\n        # Note: The actual result depends on the data, but we can check structure\n        assert isinstance(result, DriftResult)\n        assert result.reference_size == 100\n        assert result.drift_threshold == drift_detector.drift_threshold\n\n    def test_check_drift_detected(self, drift_detector, reference_data, drifted_data):\n        \"\"\"Test drift detection when distribution shifts significantly.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Add clearly drifted data to current\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(drifted_data)\n\n        result = drift_detector.check_drift()\n\n        # With significantly different distributions, should detect drift\n        assert isinstance(result, DriftResult)\n        assert result.reference_size == 100\n        assert result.current_size >= len(drifted_data)\n        # Note: Actual drift detection depends on Evidently's algorithms\n\n    def test_check_drift_increments_counter(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test that drift check increments check counter.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        assert drift_detector._total_checks == 0\n\n        drift_detector.check_drift()\n        assert drift_detector._total_checks == 1\n\n        drift_detector.check_drift()\n        assert drift_detector._total_checks == 2\n\n    def test_check_drift_updates_last_check_time(self, drift_detector, reference_data):\n        \"\"\"Test that drift check updates last check time.\"\"\"\n        drift_detector.add_batch(reference_data)\n\n        assert drift_detector._last_check_time is None\n\n        before = time.time()\n        drift_detector.check_drift()\n        after = time.time()\n\n        assert drift_detector._last_check_time is not None\n        assert before <= drift_detector._last_check_time <= after\n\n    def test_check_drift_no_common_columns(self, drift_detector):\n        \"\"\"Test drift check with no common feature columns.\"\"\"\n        # Add reference with features a, b\n        drift_detector.add_batch([{\"a\": 1.0, \"b\": 2.0}] * 20)\n        drift_detector.lock_reference_data()\n\n        # Clear and add current with features c, d (different columns)\n        drift_detector._current_data.clear()\n        drift_detector.add_batch([{\"c\": 1.0, \"d\": 2.0}] * 20)\n\n        result = drift_detector.check_drift()\n\n        assert result.status == DriftStatus.ERROR\n        assert \"No common feature columns\" in result.message\n\n    @pytest.mark.asyncio\n    async def test_check_drift_async(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test async drift check.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        result = await drift_detector.check_drift_async()\n\n        assert isinstance(result, DriftResult)\n\n\n# =============================================================================\n# DriftDetector Tests - Status and Metrics\n# =============================================================================\n\n\nclass TestDriftDetectorStatusMetrics:\n    \"\"\"Tests for status and metrics retrieval.\"\"\"\n\n    def test_get_status_no_check_performed(self, drift_detector):\n        \"\"\"Test get_status when no check has been performed.\"\"\"\n        result = drift_detector.get_status()\n\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n        assert result.drift_detected is False\n        assert result.message == \"No drift check performed yet\"\n\n    def test_get_status_after_check(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test get_status after performing a check.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        drift_detector.check_drift()\n\n        status = drift_detector.get_status()\n        assert isinstance(status, DriftResult)\n        assert status.timestamp is not None\n\n    def test_get_metrics(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test get_metrics returns valid DriftMetrics.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:20])\n\n        drift_detector.check_drift()\n\n        metrics = drift_detector.get_metrics()\n\n        assert isinstance(metrics, DriftMetrics)\n        assert metrics.total_checks == 1\n        assert metrics.status in list(DriftStatus)\n        assert metrics.data_points_collected > 0\n\n    def test_get_reference_data(self, drift_detector, reference_data):\n        \"\"\"Test get_reference_data returns DataFrame.\"\"\"\n        drift_detector.add_batch(reference_data[:10])\n\n        df = drift_detector.get_reference_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) > 0\n\n    def test_get_current_data(self, drift_detector, reference_data):\n        \"\"\"Test get_current_data returns DataFrame.\"\"\"\n        drift_detector.add_batch(reference_data[:10])\n\n        df = drift_detector.get_current_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) > 0\n\n    def test_get_reference_data_empty(self, drift_detector):\n        \"\"\"Test get_reference_data returns empty DataFrame when no data.\"\"\"\n        df = drift_detector.get_reference_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) == 0\n\n    def test_get_current_data_empty(self, drift_detector):\n        \"\"\"Test get_current_data returns empty DataFrame when no data.\"\"\"\n        df = drift_detector.get_current_data()\n\n        assert isinstance(df, pd.DataFrame)\n        assert len(df) == 0\n\n\n# =============================================================================\n# DriftDetector Tests - Alert Management\n# =============================================================================\n\n\nclass TestDriftDetectorAlerts:\n    \"\"\"Tests for drift alert management.\"\"\"\n\n    def test_register_alert_callback(self, drift_detector):\n        \"\"\"Test registering alert callback.\"\"\"\n        callback_results = []\n\n        def on_alert(alert: DriftAlert):\n            callback_results.append(alert)\n\n        drift_detector.register_alert_callback(on_alert)\n\n        assert len(drift_detector._alert_callbacks) == 1\n\n    def test_get_pending_alerts_empty(self, drift_detector):\n        \"\"\"Test get_pending_alerts when no alerts exist.\"\"\"\n        alerts = drift_detector.get_pending_alerts()\n        assert alerts == []\n\n    def test_acknowledge_alert(self, drift_detector):\n        \"\"\"Test acknowledging a drift alert.\"\"\"\n        # Manually create an alert for testing\n        alert = DriftAlert(\n            drift_result=DriftResult(\n                status=DriftStatus.DRIFT_DETECTED,\n                drift_detected=True,\n                drift_score=0.5,\n                drift_threshold=0.2,\n                columns_drifted={\"f1\": True},\n                column_drift_scores={\"f1\": 0.5},\n                reference_size=100,\n                current_size=50,\n            ),\n            severity=\"warning\",\n        )\n        drift_detector._pending_alerts.append(alert)\n\n        # Acknowledge it\n        result = drift_detector.acknowledge_alert(alert.alert_id)\n\n        assert result is True\n        assert alert.acknowledged is True\n\n    def test_acknowledge_alert_not_found(self, drift_detector):\n        \"\"\"Test acknowledging non-existent alert.\"\"\"\n        result = drift_detector.acknowledge_alert(\"nonexistent_alert_id\")\n        assert result is False\n\n    def test_alert_callback_triggered_on_drift(self, drift_detector, reference_data, drifted_data):\n        \"\"\"Test that alert callback is triggered when drift is detected.\"\"\"\n        callback_results = []\n\n        def on_alert(alert: DriftAlert):\n            callback_results.append(alert)\n\n        drift_detector.register_alert_callback(on_alert)\n\n        # Set up data for drift\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(drifted_data)\n\n        result = drift_detector.check_drift()\n\n        # If drift was detected, callback should have been triggered\n        if result.drift_detected:\n            assert len(callback_results) > 0\n            assert callback_results[0].severity in [\"warning\", \"critical\"]\n\n\n# =============================================================================\n# DriftDetector Tests - Enable/Disable\n# =============================================================================\n\n\nclass TestDriftDetectorEnableDisable:\n    \"\"\"Tests for enable/disable functionality.\"\"\"\n\n    def test_enable(self):\n        \"\"\"Test enabling drift detection.\"\"\"\n        detector = DriftDetector(enabled=False)\n        assert detector._enabled is False\n\n        detector.enable()\n\n        assert detector._enabled is True\n\n    def test_disable(self, drift_detector):\n        \"\"\"Test disabling drift detection.\"\"\"\n        assert drift_detector._enabled is True\n\n        drift_detector.disable()\n\n        assert drift_detector._enabled is False\n        assert drift_detector._current_status == DriftStatus.DISABLED\n\n    def test_is_enabled(self, drift_detector):\n        \"\"\"Test is_enabled method.\"\"\"\n        assert drift_detector.is_enabled() is True\n\n        drift_detector.disable()\n        assert drift_detector.is_enabled() is False\n\n\n# =============================================================================\n# DriftDetector Tests - Reset\n# =============================================================================\n\n\nclass TestDriftDetectorReset:\n    \"\"\"Tests for reset functionality.\"\"\"\n\n    def test_reset_clears_data(self, drift_detector, reference_data):\n        \"\"\"Test that reset clears all data.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.check_drift()\n\n        assert len(drift_detector._reference_data) > 0\n        assert drift_detector._total_checks > 0\n\n        drift_detector.reset()\n\n        assert len(drift_detector._reference_data) == 0\n        assert len(drift_detector._current_data) == 0\n        assert len(drift_detector._all_data) == 0\n        assert drift_detector._total_checks == 0\n        assert drift_detector._drift_detections == 0\n        assert drift_detector._consecutive_drift_count == 0\n        assert len(drift_detector._pending_alerts) == 0\n\n    def test_reset_unlocks_reference(self, drift_detector, reference_data):\n        \"\"\"Test that reset unlocks reference data.\"\"\"\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        assert drift_detector._reference_locked is True\n\n        drift_detector.reset()\n\n        assert drift_detector._reference_locked is False\n\n\n# =============================================================================\n# DriftDetector Tests - Thread Safety\n# =============================================================================\n\n\nclass TestDriftDetectorThreadSafety:\n    \"\"\"Tests for DriftDetector thread safety.\"\"\"\n\n    def test_concurrent_data_additions(self, drift_detector):\n        \"\"\"Test concurrent data additions don't cause race conditions.\"\"\"\n        errors = []\n\n        def add_data():\n            try:\n                for i in range(100):\n                    drift_detector.add_data_point({\"x\": float(i), \"y\": float(i * 2)})\n            except Exception as e:\n                errors.append(e)\n\n        threads = [threading.Thread(target=add_data) for _ in range(5)]\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        assert len(errors) == 0\n        # All data should be added (500 total)\n        assert len(drift_detector._all_data) == 500\n\n\n# =============================================================================\n# DriftDetector Tests - DataFrame Caching\n# =============================================================================\n\n\nclass TestDriftDetectorDataFrameCaching:\n    \"\"\"Tests for DataFrame caching functionality.\"\"\"\n\n    def test_dataframes_cached_on_first_conversion(self, drift_detector, reference_data):\n        \"\"\"Test that DataFrames are cached on first conversion.\"\"\"\n        # Add data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # First call should populate cache\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._reference_checksum is None\n\n        ref_df = drift_detector.get_reference_data()\n\n        # Cache should now be populated\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._reference_checksum is not None\n        assert isinstance(drift_detector._reference_df_cache, pd.DataFrame)\n\n    def test_cached_dataframes_reused_when_data_unchanged(self, drift_detector, reference_data):\n        \"\"\"Test that cached DataFrames are reused when data hasn't changed.\"\"\"\n        # Add data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # First call\n        ref_df_1 = drift_detector.get_reference_data()\n        checksum_1 = drift_detector._reference_checksum\n        cached_df_1 = drift_detector._reference_df_cache\n\n        # Second call should reuse cache\n        ref_df_2 = drift_detector.get_reference_data()\n        checksum_2 = drift_detector._reference_checksum\n        cached_df_2 = drift_detector._reference_df_cache\n\n        # Checksums should be identical\n        assert checksum_1 == checksum_2\n        # Cache objects should be the same (identity check)\n        assert cached_df_1 is cached_df_2\n        # DataFrames should be equal\n        pd.testing.assert_frame_equal(ref_df_1, ref_df_2)\n\n    def test_cache_invalidated_when_reference_data_changes(self, drift_detector, reference_data):\n        \"\"\"Test that cache is invalidated when reference data changes.\"\"\"\n        # Add initial data\n        drift_detector.add_batch(reference_data[:50])\n\n        # Get reference data to populate cache\n        ref_df_1 = drift_detector.get_reference_data()\n        checksum_1 = drift_detector._reference_checksum\n\n        assert drift_detector._reference_df_cache is not None\n\n        # Add more data (reference not locked)\n        drift_detector.add_batch(reference_data[50:])\n\n        # Cache should be invalidated\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._reference_checksum is None\n\n        # Get reference data again to repopulate cache\n        ref_df_2 = drift_detector.get_reference_data()\n        checksum_2 = drift_detector._reference_checksum\n\n        # New checksum should be different\n        assert checksum_1 != checksum_2\n        # DataFrames should be different sizes\n        assert len(ref_df_1) != len(ref_df_2)\n\n    def test_cache_invalidated_when_current_data_changes(self, drift_detector, reference_data):\n        \"\"\"Test that cache is invalidated when current data changes.\"\"\"\n        # Add initial data\n        drift_detector.add_batch(reference_data[:50])\n        drift_detector.lock_reference_data()\n\n        # Clear current data and add some\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(reference_data[50:70])\n\n        # Get current data to populate cache\n        cur_df_1 = drift_detector.get_current_data()\n        checksum_1 = drift_detector._current_checksum\n\n        assert drift_detector._current_df_cache is not None\n\n        # Add more data to current\n        drift_detector.add_batch(reference_data[70:90])\n\n        # Cache should be invalidated\n        assert drift_detector._current_df_cache is None\n        assert drift_detector._current_checksum is None\n\n        # Get current data again to repopulate cache\n        cur_df_2 = drift_detector.get_current_data()\n        checksum_2 = drift_detector._current_checksum\n\n        # New checksum should be different\n        assert checksum_1 != checksum_2\n        # DataFrames should be different sizes\n        assert len(cur_df_1) != len(cur_df_2)\n\n    def test_report_cache_invalidated_when_data_changes(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that report cache is invalidated when data changes.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Clear current and add data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check should populate report cache\n        result_1 = drift_detector.check_drift()\n        assert drift_detector._last_report_cache is not None\n        assert drift_detector._report_cache_checksum is not None\n        cache_checksum_1 = drift_detector._report_cache_checksum\n\n        # Add more data to current (this should invalidate report cache)\n        drift_detector.add_batch(similar_data[30:50])\n\n        # Report cache should be invalidated\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n        # Second drift check should create new cache\n        result_2 = drift_detector.check_drift()\n        cache_checksum_2 = drift_detector._report_cache_checksum\n\n        # Checksums should be different\n        assert cache_checksum_1 != cache_checksum_2\n\n    def test_cached_report_reused_when_data_unchanged(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that cached drift report is reused when data hasn't changed.\"\"\"\n        # Add data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check\n        result_1 = drift_detector.check_drift()\n        cache_checksum_1 = drift_detector._report_cache_checksum\n        cached_report_1 = drift_detector._last_report_cache\n\n        # Second drift check without changing data\n        result_2 = drift_detector.check_drift()\n        cache_checksum_2 = drift_detector._report_cache_checksum\n        cached_report_2 = drift_detector._last_report_cache\n\n        # Checksums should be identical\n        assert cache_checksum_1 == cache_checksum_2\n        # Cached objects should be the same\n        assert cached_report_1 is cached_report_2\n        # Results should have same drift detection outcome\n        assert result_1.drift_detected == result_2.drift_detected\n        assert result_1.drift_score == result_2.drift_score\n\n    def test_cache_disabled_when_enable_caching_false(self, reference_data):\n        \"\"\"Test that caching is disabled when enable_caching=False.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data)\n\n        # Get reference data\n        ref_df = detector.get_reference_data()\n\n        # Cache should not be populated\n        assert detector._reference_df_cache is None\n        assert detector._reference_checksum is None\n\n    def test_cache_cleared_on_reset(self, drift_detector, reference_data):\n        \"\"\"Test that cache is cleared when detector is reset.\"\"\"\n        # Add data and populate caches\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Populate all caches\n        drift_detector.get_reference_data()\n        drift_detector.get_current_data()\n        drift_detector.check_drift()\n\n        # Verify caches are populated\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._current_df_cache is not None\n\n        # Reset detector\n        drift_detector.reset()\n\n        # All caches should be cleared\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._current_df_cache is None\n        assert drift_detector._reference_checksum is None\n        assert drift_detector._current_checksum is None\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n\n# =============================================================================\n# DriftDetector Tests - Report Caching\n# =============================================================================\n\n\nclass TestDriftDetectorReportCaching:\n    \"\"\"Tests for drift report result caching functionality.\"\"\"\n\n    def test_report_results_are_cached(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test that drift report results are cached after first check_drift call.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Clear current and add data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # Before first check, cache should be empty\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n        # First drift check should populate report cache\n        result = drift_detector.check_drift()\n\n        # After first check, cache should be populated\n        assert drift_detector._last_report_cache is not None\n        assert drift_detector._report_cache_checksum is not None\n        assert isinstance(drift_detector._last_report_cache, DriftResult)\n        # Cached result should match returned result\n        assert drift_detector._last_report_cache.drift_detected == result.drift_detected\n        assert drift_detector._last_report_cache.drift_score == result.drift_score\n\n    def test_cached_reports_returned_when_data_unchanged(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that cached drift reports are returned when data hasn't changed.\"\"\"\n        # Add data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check - creates cache\n        result_1 = drift_detector.check_drift()\n        cached_report_1 = drift_detector._last_report_cache\n        cache_checksum_1 = drift_detector._report_cache_checksum\n\n        # Second drift check without changing data - should use cache\n        result_2 = drift_detector.check_drift()\n        cached_report_2 = drift_detector._last_report_cache\n        cache_checksum_2 = drift_detector._report_cache_checksum\n\n        # Cache checksum should be identical\n        assert cache_checksum_1 == cache_checksum_2\n        # Cached report object should be the same instance\n        assert cached_report_1 is cached_report_2\n        # Results should have identical drift metrics\n        assert result_1.drift_detected == result_2.drift_detected\n        assert result_1.drift_score == result_2.drift_score\n        assert result_1.reference_size == result_2.reference_size\n        assert result_1.current_size == result_2.current_size\n\n    def test_new_reports_generated_when_reference_data_changes(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that new drift reports are generated when reference data changes.\"\"\"\n        # Add initial reference data\n        drift_detector.add_batch(reference_data[:50])\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check\n        result_1 = drift_detector.check_drift()\n        cache_checksum_1 = drift_detector._report_cache_checksum\n        cached_report_1 = drift_detector._last_report_cache\n\n        # Add more reference data (not locked, so it will update)\n        drift_detector.add_batch(reference_data[50:])\n\n        # Cache should be invalidated\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n        # Second drift check should generate new report\n        result_2 = drift_detector.check_drift()\n        cache_checksum_2 = drift_detector._report_cache_checksum\n        cached_report_2 = drift_detector._last_report_cache\n\n        # Checksums should be different\n        assert cache_checksum_1 != cache_checksum_2\n        # Cached reports should be different instances\n        assert cached_report_1 is not cached_report_2\n        # Results should have different reference sizes\n        assert result_1.reference_size != result_2.reference_size\n\n    def test_new_reports_generated_when_current_data_changes(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that new drift reports are generated when current data changes.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Add initial current data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check\n        result_1 = drift_detector.check_drift()\n        cache_checksum_1 = drift_detector._report_cache_checksum\n        cached_report_1 = drift_detector._last_report_cache\n\n        # Add more current data\n        drift_detector.add_batch(similar_data[30:60])\n\n        # Cache should be invalidated\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n        # Second drift check should generate new report\n        result_2 = drift_detector.check_drift()\n        cache_checksum_2 = drift_detector._report_cache_checksum\n        cached_report_2 = drift_detector._last_report_cache\n\n        # Checksums should be different\n        assert cache_checksum_1 != cache_checksum_2\n        # Cached reports should be different instances\n        assert cached_report_1 is not cached_report_2\n        # Results should have different current sizes\n        assert result_1.current_size != result_2.current_size\n\n    def test_timestamps_updated_on_cached_results(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that timestamps are updated when returning cached drift results.\"\"\"\n        # Add data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check\n        result_1 = drift_detector.check_drift()\n        timestamp_1 = result_1.timestamp\n\n        # Wait a short time to ensure timestamps differ\n        time.sleep(0.1)\n\n        # Second drift check without changing data - should use cache\n        result_2 = drift_detector.check_drift()\n        timestamp_2 = result_2.timestamp\n\n        # Timestamps should be different (second one should be later)\n        assert timestamp_2 > timestamp_1\n        # But other fields should be identical\n        assert result_1.drift_detected == result_2.drift_detected\n        assert result_1.drift_score == result_2.drift_score\n        assert result_1.reference_size == result_2.reference_size\n        assert result_1.current_size == result_2.current_size\n\n    def test_cached_report_preserves_all_fields_except_timestamp(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that cached reports preserve all fields except timestamp.\"\"\"\n        # Add data\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # First drift check\n        result_1 = drift_detector.check_drift()\n\n        # Wait and check again\n        time.sleep(0.1)\n        result_2 = drift_detector.check_drift()\n\n        # All fields except timestamp should be identical\n        assert result_1.status == result_2.status\n        assert result_1.drift_detected == result_2.drift_detected\n        assert result_1.drift_score == result_2.drift_score\n        assert result_1.drift_threshold == result_2.drift_threshold\n        assert result_1.reference_size == result_2.reference_size\n        assert result_1.current_size == result_2.current_size\n        assert result_1.message == result_2.message\n        # Timestamp should be updated\n        assert result_2.timestamp > result_1.timestamp\n\n    def test_report_cache_with_insufficient_data(self, drift_detector):\n        \"\"\"Test that reports with insufficient data are not cached.\"\"\"\n        # Add only a few samples (less than min_samples_for_drift=10)\n        for i in range(5):\n            drift_detector.add_data_point({\"x\": float(i)})\n\n        # Check drift\n        result = drift_detector.check_drift()\n\n        # Result should indicate insufficient data\n        assert result.status == DriftStatus.INSUFFICIENT_DATA\n\n        # Report cache should remain empty (we don't cache error states)\n        # Note: Implementation may vary - this tests current behavior\n        # If cache behavior changes, adjust this test accordingly\n\n    def test_report_cache_disabled_when_caching_disabled(\n        self, reference_data, similar_data\n    ):\n        \"\"\"Test that report caching is disabled when enable_caching=False.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:30])\n\n        # Drift check\n        result = detector.check_drift()\n\n        # Report cache should not be populated\n        assert detector._last_report_cache is None\n        assert detector._report_cache_checksum is None\n\n\n# =============================================================================\n# DriftDetector Tests - Cache Invalidation\n# =============================================================================\n\n\nclass TestDriftDetectorCacheInvalidation:\n    \"\"\"Tests for cache invalidation in data modification methods.\"\"\"\n\n    def test_add_data_point_invalidates_current_cache(self, drift_detector, reference_data):\n        \"\"\"Test that add_data_point invalidates current data cache.\"\"\"\n        # Add initial data and lock reference\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Clear current and add some data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(reference_data[:30])\n\n        # Populate current cache\n        drift_detector.get_current_data()\n        assert drift_detector._current_df_cache is not None\n        assert drift_detector._current_checksum is not None\n\n        # Add a single data point\n        drift_detector.add_data_point({\"f1\": 1.0, \"f2\": 2.0})\n\n        # Current cache should be invalidated\n        assert drift_detector._current_df_cache is None\n        assert drift_detector._current_checksum is None\n\n    def test_add_data_point_invalidates_reference_cache_when_not_locked(\n        self, drift_detector, reference_data\n    ):\n        \"\"\"Test that add_data_point invalidates reference cache when reference is not locked.\"\"\"\n        # Add initial data (reference NOT locked)\n        drift_detector.add_batch(reference_data[:50])\n\n        # Populate reference cache\n        drift_detector.get_reference_data()\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._reference_checksum is not None\n\n        # Add a data point (should invalidate reference cache since not locked)\n        drift_detector.add_data_point({\"f1\": 1.0, \"f2\": 2.0})\n\n        # Reference cache should be invalidated\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._reference_checksum is None\n\n    def test_add_data_point_does_not_invalidate_reference_cache_when_locked(\n        self, drift_detector, reference_data\n    ):\n        \"\"\"Test that add_data_point does not invalidate reference cache when locked.\"\"\"\n        # Add initial data and lock reference\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Populate reference cache\n        ref_df_1 = drift_detector.get_reference_data()\n        checksum_1 = drift_detector._reference_checksum\n        assert drift_detector._reference_df_cache is not None\n\n        # Add a data point (should NOT invalidate reference cache since locked)\n        drift_detector.add_data_point({\"f1\": 1.0, \"f2\": 2.0})\n\n        # Reference cache should still be valid\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._reference_checksum == checksum_1\n\n    def test_add_data_point_invalidates_report_cache(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that add_data_point invalidates report cache.\"\"\"\n        # Add reference data and lock\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n\n        # Clear current and add data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # Populate report cache\n        drift_detector.check_drift()\n        assert drift_detector._last_report_cache is not None\n        assert drift_detector._report_cache_checksum is not None\n\n        # Add a data point (should invalidate report cache)\n        drift_detector.add_data_point({\"f1\": 1.0, \"f2\": 2.0})\n\n        # Report cache should be invalidated\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n    def test_set_reference_data_invalidates_reference_cache(\n        self, drift_detector, reference_data\n    ):\n        \"\"\"Test that set_reference_data invalidates reference cache.\"\"\"\n        # Add initial data\n        drift_detector.add_batch(reference_data[:50])\n\n        # Populate reference cache\n        drift_detector.get_reference_data()\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._reference_checksum is not None\n\n        # Set new reference data from DataFrame\n        new_ref_df = pd.DataFrame(reference_data[50:])\n        drift_detector.set_reference_data(new_ref_df)\n\n        # Reference cache should be invalidated\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._reference_checksum is None\n\n    def test_set_reference_data_invalidates_report_cache(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that set_reference_data invalidates report cache.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data[:50])\n        drift_detector.lock_reference_data()\n\n        # Clear current and add data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # Populate report cache\n        drift_detector.check_drift()\n        assert drift_detector._last_report_cache is not None\n        assert drift_detector._report_cache_checksum is not None\n\n        # Set new reference data (should invalidate report cache)\n        new_ref_df = pd.DataFrame(reference_data[50:])\n        drift_detector.set_reference_data(new_ref_df)\n\n        # Report cache should be invalidated\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n    def test_update_reference_from_current_invalidates_reference_cache(\n        self, drift_detector, reference_data\n    ):\n        \"\"\"Test that update_reference_from_current invalidates reference cache.\"\"\"\n        # Add initial data\n        drift_detector.add_batch(reference_data[:50])\n\n        # Populate reference cache\n        drift_detector.get_reference_data()\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._reference_checksum is not None\n\n        # Update reference from current\n        drift_detector.update_reference_from_current()\n\n        # Reference cache should be invalidated\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._reference_checksum is None\n\n    def test_update_reference_from_current_invalidates_report_cache(\n        self, drift_detector, reference_data, similar_data\n    ):\n        \"\"\"Test that update_reference_from_current invalidates report cache.\"\"\"\n        # Add reference data\n        drift_detector.add_batch(reference_data[:50])\n        drift_detector.lock_reference_data()\n\n        # Clear current and add data\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # Populate report cache\n        drift_detector.check_drift()\n        assert drift_detector._last_report_cache is not None\n        assert drift_detector._report_cache_checksum is not None\n\n        # Update reference from current (should invalidate report cache)\n        drift_detector.update_reference_from_current()\n\n        # Report cache should be invalidated\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n    def test_reset_clears_all_caches(self, drift_detector, reference_data, similar_data):\n        \"\"\"Test that reset clears all cache fields.\"\"\"\n        # Add data and populate all caches\n        drift_detector.add_batch(reference_data)\n        drift_detector.lock_reference_data()\n        drift_detector._current_data.clear()\n        drift_detector.add_batch(similar_data[:30])\n\n        # Populate all caches\n        drift_detector.get_reference_data()\n        drift_detector.get_current_data()\n        drift_detector.check_drift()\n\n        # Verify caches are populated\n        assert drift_detector._reference_df_cache is not None\n        assert drift_detector._current_df_cache is not None\n        assert drift_detector._reference_checksum is not None\n        assert drift_detector._current_checksum is not None\n        assert drift_detector._last_report_cache is not None\n        assert drift_detector._report_cache_checksum is not None\n\n        # Reset detector\n        drift_detector.reset()\n\n        # All caches should be cleared\n        assert drift_detector._reference_df_cache is None\n        assert drift_detector._current_df_cache is None\n        assert drift_detector._reference_checksum is None\n        assert drift_detector._current_checksum is None\n        assert drift_detector._last_report_cache is None\n        assert drift_detector._report_cache_checksum is None\n\n\n# =============================================================================\n# DriftDetector Tests - Cache Enable/Disable\n# =============================================================================\n\n\nclass TestDriftDetectorCacheDisabled:\n    \"\"\"Tests for verifying caching can be disabled and falls back to non-cached behavior.\"\"\"\n\n    def test_check_drift_works_without_caching(self, reference_data, similar_data):\n        \"\"\"Test that check_drift works correctly when caching is disabled.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:30])\n\n        # Check drift should work correctly\n        result = detector.check_drift()\n\n        # Result should be valid\n        assert isinstance(result, DriftResult)\n        assert result.status in [DriftStatus.NO_DRIFT, DriftStatus.DRIFT_DETECTED]\n        assert result.reference_size == len(reference_data)\n        assert result.current_size == 30\n\n        # Caches should remain empty\n        assert detector._last_report_cache is None\n        assert detector._report_cache_checksum is None\n\n    def test_multiple_check_drift_calls_recompute_without_cache(\n        self, reference_data, similar_data\n    ):\n        \"\"\"Test that multiple check_drift calls with same data recompute without caching.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:30])\n\n        # First check\n        result_1 = detector.check_drift()\n        cache_after_first = detector._last_report_cache\n\n        # Second check with same data\n        result_2 = detector.check_drift()\n        cache_after_second = detector._last_report_cache\n\n        # Results should have same values (same data)\n        assert result_1.drift_detected == result_2.drift_detected\n        assert result_1.drift_score == result_2.drift_score\n        assert result_1.reference_size == result_2.reference_size\n        assert result_1.current_size == result_2.current_size\n\n        # But cache should remain empty (no caching occurred)\n        assert cache_after_first is None\n        assert cache_after_second is None\n\n    def test_get_reference_data_returns_fresh_dataframe_without_cache(self, reference_data):\n        \"\"\"Test that get_reference_data returns fresh DataFrame when caching is disabled.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data)\n\n        # Get reference data twice\n        ref_df_1 = detector.get_reference_data()\n        ref_df_2 = detector.get_reference_data()\n\n        # DataFrames should have same content\n        pd.testing.assert_frame_equal(ref_df_1, ref_df_2)\n\n        # But should be different objects (no caching)\n        assert ref_df_1 is not ref_df_2\n\n        # Cache should remain empty\n        assert detector._reference_df_cache is None\n        assert detector._reference_checksum is None\n\n    def test_get_current_data_returns_fresh_dataframe_without_cache(self, reference_data):\n        \"\"\"Test that get_current_data returns fresh DataFrame when caching is disabled.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data[:50])\n\n        # Get current data twice\n        cur_df_1 = detector.get_current_data()\n        cur_df_2 = detector.get_current_data()\n\n        # DataFrames should have same content\n        pd.testing.assert_frame_equal(cur_df_1, cur_df_2)\n\n        # But should be different objects (no caching)\n        assert cur_df_1 is not cur_df_2\n\n        # Cache should remain empty\n        assert detector._current_df_cache is None\n        assert detector._current_checksum is None\n\n    def test_generate_html_report_works_without_caching(\n        self, reference_data, similar_data, tmp_path\n    ):\n        \"\"\"Test that generate_html_report works correctly when caching is disabled.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Add data\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:30])\n\n        # Generate HTML report\n        output_path = tmp_path / \"drift_report.html\"\n        result = detector.generate_html_report(str(output_path))\n\n        # Should succeed\n        assert result is True\n        assert output_path.exists()\n\n        # Caches should remain empty\n        assert detector._reference_df_cache is None\n        assert detector._current_df_cache is None\n\n    def test_cache_flag_is_respected(self):\n        \"\"\"Test that _cache_enabled flag reflects enable_caching parameter.\"\"\"\n        detector_with_cache = DriftDetector(enable_caching=True)\n        detector_without_cache = DriftDetector(enable_caching=False)\n\n        assert detector_with_cache._cache_enabled is True\n        assert detector_without_cache._cache_enabled is False\n\n    def test_all_operations_work_correctly_without_caching(\n        self, reference_data, similar_data, drifted_data\n    ):\n        \"\"\"Test comprehensive workflow without caching to verify fallback behavior.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=False,\n        )\n\n        # Initial setup\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n\n        # Get reference data (should work)\n        ref_df = detector.get_reference_data()\n        assert len(ref_df) == len(reference_data)\n\n        # Check drift with similar data (should detect no drift)\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:30])\n\n        result_no_drift = detector.check_drift()\n        assert result_no_drift.status == DriftStatus.NO_DRIFT\n        assert result_no_drift.drift_detected is False\n\n        # Check drift with drifted data (should detect drift)\n        detector._current_data.clear()\n        detector.add_batch(drifted_data[:30])\n\n        result_drift = detector.check_drift()\n        assert result_drift.status == DriftStatus.DRIFT_DETECTED\n        assert result_drift.drift_detected is True\n\n        # Get current data (should work)\n        cur_df = detector.get_current_data()\n        assert len(cur_df) == 30\n\n        # All caches should remain empty throughout\n        assert detector._reference_df_cache is None\n        assert detector._current_df_cache is None\n        assert detector._reference_checksum is None\n        assert detector._current_checksum is None\n        assert detector._last_report_cache is None\n        assert detector._report_cache_checksum is None\n\n\n# =============================================================================\n# DriftDetector Tests - Performance Benchmark\n# =============================================================================\n\n\nclass TestDriftDetectorPerformanceBenchmark:\n    \"\"\"Tests for performance improvement from caching.\"\"\"\n\n    def test_cache_provides_significant_performance_improvement(self, reference_data, similar_data):\n        \"\"\"Test that cached check_drift() calls are significantly faster than uncached calls.\n\n        This test verifies the core value proposition of the caching feature:\n        repeated check_drift() calls with unchanged data should be >50% faster\n        due to cached DataFrame conversions and report results.\n        \"\"\"\n        # Create detector with caching enabled\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            enable_caching=True,\n        )\n\n        # Add sufficient data for meaningful drift detection\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:30])\n\n        # First call: should populate all caches (DataFrame + report)\n        start_time_1 = time.time()\n        result_1 = detector.check_drift()\n        elapsed_1 = time.time() - start_time_1\n\n        # Verify caches were populated\n        assert detector._reference_df_cache is not None\n        assert detector._current_df_cache is not None\n        assert detector._last_report_cache is not None\n        assert detector._report_cache_checksum is not None\n\n        # Second call: should hit all caches (same data, no changes)\n        start_time_2 = time.time()\n        result_2 = detector.check_drift()\n        elapsed_2 = time.time() - start_time_2\n\n        # Verify results are consistent (using cached data)\n        assert result_1.status == result_2.status\n        assert result_1.drift_detected == result_2.drift_detected\n        assert result_1.drift_score == result_2.drift_score\n\n        # Performance assertion: second call should be >50% faster\n        # This means elapsed_2 should be < 50% of elapsed_1\n        # i.e., elapsed_2 / elapsed_1 < 0.5\n        # Add small epsilon for timing measurement noise\n        improvement_ratio = elapsed_2 / elapsed_1 if elapsed_1 > 0 else 1.0\n\n        # The cached call should be significantly faster\n        # We expect >50% improvement (ratio < 0.5), but allow some margin for variability\n        # In practice, cache hits are often 90%+ faster (ratio < 0.1)\n        assert improvement_ratio < 0.5, (\n            f\"Cached call not significantly faster: \"\n            f\"first call took {elapsed_1:.6f}s, \"\n            f\"second call took {elapsed_2:.6f}s \"\n            f\"(ratio: {improvement_ratio:.2%}, expected < 50%)\"\n        )\n\n\n# =============================================================================\n# DriftDetector Tests - Repr\n# =============================================================================\n\n\nclass TestDriftDetectorRepr:\n    \"\"\"Tests for DriftDetector string representation.\"\"\"\n\n    def test_repr(self, drift_detector):\n        \"\"\"Test __repr__ returns informative string.\"\"\"\n        repr_str = repr(drift_detector)\n\n        assert \"DriftDetector\" in repr_str\n        assert \"status=\" in repr_str\n        assert \"ref_size=\" in repr_str\n        assert \"cur_size=\" in repr_str\n\n\n# =============================================================================\n# MetricsRegistry Tests - Initialization\n# =============================================================================\n\n\nclass TestMetricsRegistryInit:\n    \"\"\"Tests for MetricsRegistry initialization.\"\"\"\n\n    def test_default_initialization(self):\n        \"\"\"Test default initialization creates valid registry.\"\"\"\n        registry = CollectorRegistry()\n        metrics = MetricsRegistry(registry=registry)\n\n        assert metrics._prefix == \"adaptive_learning\"\n        assert metrics._current_model_version == \"unknown\"\n        assert metrics._cold_start_active is True\n\n    def test_custom_initialization(self):\n        \"\"\"Test initialization with custom parameters.\"\"\"\n        registry = CollectorRegistry()\n        custom_buckets = (0.01, 0.1, 1.0)\n        metrics = MetricsRegistry(\n            registry=registry,\n            prefix=\"custom_prefix\",\n            latency_buckets=custom_buckets,\n        )\n\n        assert metrics._prefix == \"custom_prefix\"\n        assert metrics._latency_buckets == custom_buckets\n\n    def test_counters_initialized(self, metrics_registry):\n        \"\"\"Test that all counters are initialized.\"\"\"\n        assert metrics_registry.predictions_total is not None\n        assert metrics_registry.training_samples_total is not None\n        assert metrics_registry.errors_total is not None\n        assert metrics_registry.safety_violations_total is not None\n        assert metrics_registry.drift_checks_total is not None\n        assert metrics_registry.model_swaps_total is not None\n        assert metrics_registry.rollbacks_total is not None\n\n    def test_gauges_initialized(self, metrics_registry):\n        \"\"\"Test that all gauges are initialized.\"\"\"\n        assert metrics_registry.model_accuracy is not None\n        assert metrics_registry.drift_score is not None\n        assert metrics_registry.drift_threshold is not None\n        assert metrics_registry.training_queue_size is not None\n        assert metrics_registry.model_is_cold_start is not None\n        assert metrics_registry.uptime_seconds is not None\n\n    def test_histograms_initialized(self, metrics_registry):\n        \"\"\"Test that all histograms are initialized.\"\"\"\n        assert metrics_registry.prediction_latency is not None\n        assert metrics_registry.training_latency is not None\n        assert metrics_registry.drift_check_latency is not None\n        assert metrics_registry.model_swap_latency is not None\n\n\n# =============================================================================\n# MetricsRegistry Tests - Recording Methods\n# =============================================================================\n\n\nclass TestMetricsRegistryRecording:\n    \"\"\"Tests for metrics recording methods.\"\"\"\n\n    def test_record_prediction_success(self, metrics_registry):\n        \"\"\"Test recording successful prediction.\"\"\"\n        metrics_registry.record_prediction(\n            latency_seconds=0.012,\n            model_version=\"v1\",\n            success=True,\n        )\n\n        # Verify counter was incremented\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"success\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_prediction_failure(self, metrics_registry):\n        \"\"\"Test recording failed prediction.\"\"\"\n        metrics_registry.record_prediction(\n            latency_seconds=0.05,\n            model_version=\"v1\",\n            success=False,\n        )\n\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"error\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_training(self, metrics_registry):\n        \"\"\"Test recording training event.\"\"\"\n        metrics_registry.record_training(\n            latency_seconds=0.003,\n            model_version=\"v1\",\n            batch_size=10,\n        )\n\n        count = metrics_registry.training_samples_total.labels(model_version=\"v1\")._value.get()\n        assert count == 10\n\n    def test_record_error(self, metrics_registry):\n        \"\"\"Test recording error event.\"\"\"\n        metrics_registry.record_error(\n            error_type=\"validation_error\",\n            endpoint=\"/predict\",\n        )\n\n        count = metrics_registry.errors_total.labels(\n            error_type=\"validation_error\",\n            endpoint=\"/predict\",\n        )._value.get()\n        assert count == 1\n\n    def test_record_safety_violation(self, metrics_registry):\n        \"\"\"Test recording safety violation.\"\"\"\n        metrics_registry.record_safety_violation(result=\"rejected\")\n\n        count = metrics_registry.safety_violations_total.labels(\n            safety_result=\"rejected\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_drift_check(self, metrics_registry):\n        \"\"\"Test recording drift check.\"\"\"\n        metrics_registry.record_drift_check(\n            latency_seconds=0.5,\n            drift_detected=True,\n            drift_score=0.3,\n        )\n\n        count = metrics_registry.drift_checks_total.labels(\n            drift_status=\"drift_detected\"\n        )._value.get()\n        assert count == 1\n\n    def test_record_model_swap_success(self, metrics_registry):\n        \"\"\"Test recording successful model swap.\"\"\"\n        metrics_registry.record_model_swap(latency_seconds=0.1, success=True)\n\n        count = metrics_registry.model_swaps_total.labels(status=\"success\")._value.get()\n        assert count == 1\n\n    def test_record_model_swap_failure(self, metrics_registry):\n        \"\"\"Test recording failed model swap.\"\"\"\n        metrics_registry.record_model_swap(latency_seconds=0.2, success=False)\n\n        count = metrics_registry.model_swaps_total.labels(status=\"error\")._value.get()\n        assert count == 1\n\n    def test_record_rollback(self, metrics_registry):\n        \"\"\"Test recording rollback event.\"\"\"\n        metrics_registry.record_rollback()\n        metrics_registry.record_rollback()\n\n        count = metrics_registry.rollbacks_total._value.get()\n        assert count == 2\n\n\n# =============================================================================\n# MetricsRegistry Tests - Gauge Setters\n# =============================================================================\n\n\nclass TestMetricsRegistryGauges:\n    \"\"\"Tests for gauge setter methods.\"\"\"\n\n    def test_set_model_accuracy(self, metrics_registry):\n        \"\"\"Test setting model accuracy gauge.\"\"\"\n        metrics_registry.set_model_accuracy(0.92, model_version=\"v1\")\n\n        value = metrics_registry.model_accuracy.labels(model_version=\"v1\")._value.get()\n        assert value == 0.92\n\n    def test_set_model_f1(self, metrics_registry):\n        \"\"\"Test setting model F1 score gauge.\"\"\"\n        metrics_registry.set_model_f1(0.88, model_version=\"v1\")\n\n        value = metrics_registry.model_f1_score.labels(model_version=\"v1\")._value.get()\n        assert value == 0.88\n\n    def test_set_drift_score(self, metrics_registry):\n        \"\"\"Test setting drift score gauge.\"\"\"\n        metrics_registry.set_drift_score(0.25)\n\n        value = metrics_registry.drift_score._value.get()\n        assert value == 0.25\n\n    def test_set_drift_threshold(self, metrics_registry):\n        \"\"\"Test setting drift threshold gauge.\"\"\"\n        metrics_registry.set_drift_threshold(0.2)\n\n        value = metrics_registry.drift_threshold._value.get()\n        assert value == 0.2\n\n    def test_set_training_queue_size(self, metrics_registry):\n        \"\"\"Test setting training queue size gauge.\"\"\"\n        metrics_registry.set_training_queue_size(100)\n\n        value = metrics_registry.training_queue_size._value.get()\n        assert value == 100\n\n    def test_set_reference_data_size(self, metrics_registry):\n        \"\"\"Test setting reference data size gauge.\"\"\"\n        metrics_registry.set_reference_data_size(1000)\n\n        value = metrics_registry.reference_data_size._value.get()\n        assert value == 1000\n\n    def test_set_current_data_size(self, metrics_registry):\n        \"\"\"Test setting current data size gauge.\"\"\"\n        metrics_registry.set_current_data_size(100)\n\n        value = metrics_registry.current_data_size._value.get()\n        assert value == 100\n\n    def test_set_model_samples_trained(self, metrics_registry):\n        \"\"\"Test setting model samples trained gauge.\"\"\"\n        metrics_registry.set_model_samples_trained(5000, model_version=\"v1\")\n\n        value = metrics_registry.model_samples_trained.labels(model_version=\"v1\")._value.get()\n        assert value == 5000\n\n    def test_set_cold_start(self, metrics_registry):\n        \"\"\"Test setting cold start gauge.\"\"\"\n        metrics_registry.set_cold_start(True)\n        assert metrics_registry.model_is_cold_start._value.get() == 1\n\n        metrics_registry.set_cold_start(False)\n        assert metrics_registry.model_is_cold_start._value.get() == 0\n\n    def test_set_consecutive_safety_failures(self, metrics_registry):\n        \"\"\"Test setting consecutive safety failures gauge.\"\"\"\n        metrics_registry.set_consecutive_safety_failures(3)\n\n        value = metrics_registry.consecutive_safety_failures._value.get()\n        assert value == 3\n\n    def test_set_safety_threshold(self, metrics_registry):\n        \"\"\"Test setting safety threshold gauge.\"\"\"\n        metrics_registry.set_safety_threshold(0.85)\n\n        value = metrics_registry.safety_threshold._value.get()\n        assert value == 0.85\n\n\n# =============================================================================\n# MetricsRegistry Tests - Info Setters\n# =============================================================================\n\n\nclass TestMetricsRegistryInfo:\n    \"\"\"Tests for info metric setters.\"\"\"\n\n    def test_set_service_info(self, metrics_registry):\n        \"\"\"Test setting service info.\"\"\"\n        metrics_registry.set_service_info(\n            version=\"1.0.0\",\n            environment=\"production\",\n            constitutional_hash=\"abc123\",\n        )\n\n        # Info metrics don't have a simple _value getter, just verify no error\n        assert True\n\n    def test_set_model_info(self, metrics_registry):\n        \"\"\"Test setting model info.\"\"\"\n        metrics_registry.set_model_info(\n            model_type=\"LogisticRegression\",\n            model_version=\"v2\",\n            algorithm=\"river.linear_model.LogisticRegression\",\n            mlflow_run_id=\"run_123\",\n        )\n\n        assert metrics_registry._current_model_version == \"v2\"\n\n\n# =============================================================================\n# MetricsRegistry Tests - Context Managers\n# =============================================================================\n\n\nclass TestMetricsRegistryContextManagers:\n    \"\"\"Tests for timing context managers.\"\"\"\n\n    def test_prediction_timer(self, metrics_registry):\n        \"\"\"Test prediction timer context manager.\"\"\"\n        with metrics_registry.prediction_timer(model_version=\"v1\"):\n            time.sleep(0.01)\n\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"success\"\n        )._value.get()\n        assert count == 1\n\n    def test_prediction_timer_with_error(self, metrics_registry):\n        \"\"\"Test prediction timer records error on exception.\"\"\"\n        with pytest.raises(ValueError):\n            with metrics_registry.prediction_timer(model_version=\"v1\"):\n                raise ValueError(\"Test error\")\n\n        count = metrics_registry.predictions_total.labels(\n            model_version=\"v1\", status=\"error\"\n        )._value.get()\n        assert count == 1\n\n    def test_training_timer(self, metrics_registry):\n        \"\"\"Test training timer context manager.\"\"\"\n        with metrics_registry.training_timer(model_version=\"v1\", batch_size=5):\n            time.sleep(0.01)\n\n        count = metrics_registry.training_samples_total.labels(model_version=\"v1\")._value.get()\n        assert count == 5\n\n    def test_drift_check_timer(self, metrics_registry):\n        \"\"\"Test drift check timer context manager.\"\"\"\n        with metrics_registry.drift_check_timer():\n            time.sleep(0.01)\n\n        # Verify histogram observed a value\n        assert True  # Histogram observation doesn't have simple value getter\n\n    def test_model_swap_timer(self, metrics_registry):\n        \"\"\"Test model swap timer context manager.\"\"\"\n        with metrics_registry.model_swap_timer():\n            time.sleep(0.01)\n\n        count = metrics_registry.model_swaps_total.labels(status=\"success\")._value.get()\n        assert count == 1\n\n\n# =============================================================================\n# MetricsRegistry Tests - Metrics Export\n# =============================================================================\n\n\nclass TestMetricsRegistryExport:\n    \"\"\"Tests for metrics export functionality.\"\"\"\n\n    def test_generate_metrics(self, metrics_registry):\n        \"\"\"Test generating Prometheus metrics output.\"\"\"\n        metrics_registry.record_prediction(0.01, \"v1\", True)\n        metrics_registry.set_model_accuracy(0.92, \"v1\")\n\n        output = metrics_registry.generate_metrics()\n\n        assert isinstance(output, bytes)\n        assert b\"test_adaptive_predictions_total\" in output\n        assert b\"test_adaptive_model_accuracy\" in output\n\n    def test_generate_metrics_updates_uptime(self, metrics_registry):\n        \"\"\"Test that generate_metrics updates uptime.\"\"\"\n        time.sleep(0.1)\n        metrics_registry.generate_metrics()\n\n        uptime = metrics_registry.uptime_seconds._value.get()\n        assert uptime > 0\n\n    def test_get_content_type(self, metrics_registry):\n        \"\"\"Test getting Prometheus content type.\"\"\"\n        content_type = metrics_registry.get_content_type()\n\n        assert \"text/plain\" in content_type or \"openmetrics\" in content_type\n\n\n# =============================================================================\n# MetricsRegistry Tests - Callback Registration\n# =============================================================================\n\n\nclass TestMetricsRegistryCallbacks:\n    \"\"\"Tests for callback registration.\"\"\"\n\n    def test_register_update_callback(self, metrics_registry):\n        \"\"\"Test registering update callback.\"\"\"\n        callback_called = [False]\n\n        def my_callback():\n            callback_called[0] = True\n\n        metrics_registry.register_update_callback(my_callback)\n        metrics_registry.generate_metrics()\n\n        assert callback_called[0] is True\n\n    def test_unregister_update_callback(self, metrics_registry):\n        \"\"\"Test unregistering update callback.\"\"\"\n\n        def my_callback():\n            pass\n\n        metrics_registry.register_update_callback(my_callback)\n        assert len(metrics_registry._update_callbacks) == 1\n\n        result = metrics_registry.unregister_update_callback(my_callback)\n        assert result is True\n        assert len(metrics_registry._update_callbacks) == 0\n\n    def test_unregister_nonexistent_callback(self, metrics_registry):\n        \"\"\"Test unregistering callback that doesn't exist.\"\"\"\n\n        def my_callback():\n            pass\n\n        result = metrics_registry.unregister_update_callback(my_callback)\n        assert result is False\n\n\n# =============================================================================\n# MetricsRegistry Tests - Snapshot\n# =============================================================================\n\n\nclass TestMetricsRegistrySnapshot:\n    \"\"\"Tests for metrics snapshot functionality.\"\"\"\n\n    def test_get_snapshot(self, metrics_registry):\n        \"\"\"Test getting metrics snapshot.\"\"\"\n        metrics_registry.record_prediction(0.01, \"v1\", True)\n        metrics_registry.record_training(0.001, \"v1\", 10)\n        metrics_registry.set_drift_score(0.15)\n\n        snapshot = metrics_registry.get_snapshot()\n\n        assert isinstance(snapshot, MetricsSnapshot)\n        assert snapshot.predictions_total >= 1\n        assert snapshot.training_samples_total >= 10\n        assert snapshot.drift_score == 0.15\n        assert snapshot.timestamp > 0\n\n    def test_snapshot_contains_all_fields(self, metrics_registry):\n        \"\"\"Test that snapshot contains all required fields.\"\"\"\n        snapshot = metrics_registry.get_snapshot()\n\n        assert hasattr(snapshot, \"predictions_total\")\n        assert hasattr(snapshot, \"training_samples_total\")\n        assert hasattr(snapshot, \"model_accuracy\")\n        assert hasattr(snapshot, \"drift_score\")\n        assert hasattr(snapshot, \"prediction_latency_p50\")\n        assert hasattr(snapshot, \"prediction_latency_p95\")\n        assert hasattr(snapshot, \"prediction_latency_p99\")\n        assert hasattr(snapshot, \"model_version\")\n        assert hasattr(snapshot, \"cold_start_active\")\n        assert hasattr(snapshot, \"safety_violations\")\n\n\n# =============================================================================\n# MetricsRegistry Tests - Reset\n# =============================================================================\n\n\nclass TestMetricsRegistryReset:\n    \"\"\"Tests for reset functionality.\"\"\"\n\n    def test_reset_clears_state(self, metrics_registry):\n        \"\"\"Test that reset clears all state.\"\"\"\n        metrics_registry.record_prediction(0.01, \"v1\", True)\n        metrics_registry._current_model_version = \"v2\"\n\n        def my_callback():\n            pass\n\n        metrics_registry.register_update_callback(my_callback)\n\n        metrics_registry.reset()\n\n        assert metrics_registry._current_model_version == \"unknown\"\n        assert metrics_registry._cold_start_active is True\n        assert len(metrics_registry._update_callbacks) == 0\n\n\n# =============================================================================\n# MetricsRegistry Tests - Thread Safety\n# =============================================================================\n\n\nclass TestMetricsRegistryThreadSafety:\n    \"\"\"Tests for thread safety.\"\"\"\n\n    def test_concurrent_recording(self, metrics_registry):\n        \"\"\"Test concurrent metric recording doesn't cause race conditions.\"\"\"\n        errors = []\n\n        def record_predictions():\n            try:\n                for _ in range(100):\n                    metrics_registry.record_prediction(0.01, \"v1\", True)\n            except Exception as e:\n                errors.append(e)\n\n        def record_training():\n            try:\n                for _ in range(100):\n                    metrics_registry.record_training(0.001, \"v1\", 1)\n            except Exception as e:\n                errors.append(e)\n\n        threads = [\n            threading.Thread(target=record_predictions),\n            threading.Thread(target=record_training),\n            threading.Thread(target=record_predictions),\n            threading.Thread(target=record_training),\n        ]\n\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        assert len(errors) == 0\n\n\n# =============================================================================\n# MetricsRegistry Tests - Repr and Properties\n# =============================================================================\n\n\nclass TestMetricsRegistryRepr:\n    \"\"\"Tests for string representation.\"\"\"\n\n    def test_repr(self, metrics_registry):\n        \"\"\"Test __repr__ returns informative string.\"\"\"\n        repr_str = repr(metrics_registry)\n\n        assert \"MetricsRegistry\" in repr_str\n        assert \"prefix=\" in repr_str\n        assert \"model_version=\" in repr_str\n\n    def test_registry_property(self, metrics_registry):\n        \"\"\"Test registry property returns CollectorRegistry.\"\"\"\n        registry = metrics_registry.registry\n\n        assert isinstance(registry, CollectorRegistry)\n\n\n# =============================================================================\n# MetricsRegistry Tests - Global Registry\n# =============================================================================\n\n\nclass TestGlobalMetricsRegistry:\n    \"\"\"Tests for global metrics registry functions.\"\"\"\n\n    def test_get_metrics_registry(self):\n        \"\"\"Test get_metrics_registry returns global instance.\"\"\"\n        registry = get_metrics_registry()\n\n        assert isinstance(registry, MetricsRegistry)\n\n    def test_create_metrics_registry(self):\n        \"\"\"Test create_metrics_registry creates new instance.\"\"\"\n        registry1 = create_metrics_registry(prefix=\"test1\")\n        registry2 = create_metrics_registry(prefix=\"test2\")\n\n        assert registry1 is not registry2\n        assert registry1._prefix == \"test1\"\n        assert registry2._prefix == \"test2\"\n\n\n# =============================================================================\n# MetricLabel Enum Tests\n# =============================================================================\n\n\nclass TestMetricLabel:\n    \"\"\"Tests for MetricLabel enum.\"\"\"\n\n    def test_metric_label_values(self):\n        \"\"\"Test MetricLabel enum values.\"\"\"\n        assert MetricLabel.MODEL_TYPE == \"model_type\"\n        assert MetricLabel.MODEL_VERSION == \"model_version\"\n        assert MetricLabel.ENDPOINT == \"endpoint\"\n        assert MetricLabel.STATUS == \"status\"\n        assert MetricLabel.ERROR_TYPE == \"error_type\"\n        assert MetricLabel.DRIFT_STATUS == \"drift_status\"\n        assert MetricLabel.SAFETY_RESULT == \"safety_result\"\n\n\n# =============================================================================\n# Integration Tests - Drift Detection with Metrics\n# =============================================================================\n\n\nclass TestDriftDetectorMetricsIntegration:\n    \"\"\"Tests for integration between DriftDetector and MetricsRegistry.\"\"\"\n\n    def test_drift_check_updates_metrics(self, metrics_registry, reference_data, similar_data):\n        \"\"\"Test that drift check can update metrics registry.\"\"\"\n        detector = DriftDetector(\n            reference_window_size=100,\n            current_window_size=50,\n            min_samples_for_drift=10,\n        )\n\n        detector.add_batch(reference_data)\n        detector.lock_reference_data()\n        detector._current_data.clear()\n        detector.add_batch(similar_data[:20])\n\n        # Simulate what the API would do\n        start_time = time.time()\n        result = detector.check_drift()\n        latency = time.time() - start_time\n\n        # Record in metrics\n        metrics_registry.record_drift_check(\n            latency_seconds=latency,\n            drift_detected=result.drift_detected,\n            drift_score=result.drift_score,\n        )\n        metrics_registry.set_reference_data_size(len(detector._reference_data))\n        metrics_registry.set_current_data_size(len(detector._current_data))\n\n        # Verify metrics were updated\n        ref_size = metrics_registry.reference_data_size._value.get()\n        cur_size = metrics_registry.current_data_size._value.get()\n\n        assert ref_size == 100\n        assert cur_size >= 20\n",
        "last_modified": "2026-01-03T19:08:21.370891"
      },
      "task_intent": {
        "title": "049-cache-drift-detection-report-results",
        "description": "The DriftDetector.check_drift() method creates a new Evidently Report object and runs full statistical analysis on every call. The _to_dataframe method also performs DataFrame conversion repeatedly from the same deque data without caching. This optimization adds intelligent caching to reduce CPU overhead in high-traffic scenarios (10,000+ RPS).",
        "from_plan": true
      },
      "commits_behind_main": 11,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:07:33.699575",
  "last_updated": "2026-01-03T19:07:33.702383"
}