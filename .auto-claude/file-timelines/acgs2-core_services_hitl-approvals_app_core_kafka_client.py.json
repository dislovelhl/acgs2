{
  "file_path": "acgs2-core/services/hitl-approvals/app/core/kafka_client.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"Constitutional Hash: cdd01ef066bc6cf2\nKafka Client for HITL Approvals Event Streaming\n\nProvides async Kafka producer and consumer for approval workflow events.\nTopics:\n- hitl.approvals.pending: New approval requests awaiting review\n- hitl.approvals.escalated: Approvals that have been escalated to next level\n- hitl.approvals.completed: Approvals that have been approved/rejected\n\nDesign Decisions:\n- Uses aiokafka for async operations (consistent with enhanced_agent_bus)\n- Singleton pattern for global client access\n- Topic auto-creation on startup (development mode)\n- JSON serialization for message payloads\n- Error sanitization for secure logging\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport re\nimport uuid\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom typing import Callable, Coroutine, Dict, List, Optional, Set\n\nfrom acgs2_core.shared.types import JSONDict, KwargsType\n\nfrom app.config import settings\n\nlogger = logging.getLogger(__name__)\n\n# Check if aiokafka is available\ntry:\n    from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n    from aiokafka.admin import AIOKafkaAdminClient, NewTopic\n\n    KAFKA_AVAILABLE = True\nexcept ImportError:\n    KAFKA_AVAILABLE = False\n    logger.warning(\n        \"aiokafka not installed. Kafka event streaming unavailable. \"\n        \"Install with: pip install aiokafka\"\n    )\n\n\n# =============================================================================\n# Topic Definitions\n# =============================================================================\n\n\nclass HITLTopic(str, Enum):\n    \"\"\"HITL approval event topics.\"\"\"\n\n    PENDING = \"hitl.approvals.pending\"\n    ESCALATED = \"hitl.approvals.escalated\"\n    COMPLETED = \"hitl.approvals.completed\"\n\n    @classmethod\n    def all_topics(cls) -> List[str]:\n        \"\"\"Get all topic names.\"\"\"\n        return [t.value for t in cls]\n\n\n# =============================================================================\n# Event Types\n# =============================================================================\n\n\nclass HITLEventType(str, Enum):\n    \"\"\"HITL approval event types.\"\"\"\n\n    # Approval lifecycle events\n    APPROVAL_REQUESTED = \"approval_requested\"\n    APPROVAL_SUBMITTED = \"approval_submitted\"\n    APPROVAL_APPROVED = \"approval_approved\"\n    APPROVAL_REJECTED = \"approval_rejected\"\n    APPROVAL_CANCELLED = \"approval_cancelled\"\n\n    # Escalation events\n    ESCALATION_TRIGGERED = \"escalation_triggered\"\n    ESCALATION_TIMEOUT = \"escalation_timeout\"\n    SLA_WARNING = \"sla_warning\"\n    SLA_BREACH = \"sla_breach\"\n\n    # Notification events\n    NOTIFICATION_SENT = \"notification_sent\"\n    NOTIFICATION_FAILED = \"notification_failed\"\n\n\n# =============================================================================\n# Event Data Structures\n# =============================================================================\n\n\n@dataclass\nclass HITLEvent:\n    \"\"\"\n    Represents an HITL approval event.\n\n    All events contain common metadata plus event-specific payload.\n    \"\"\"\n\n    event_id: str\n    event_type: HITLEventType\n    topic: HITLTopic\n    request_id: str\n    timestamp: float  # Unix timestamp\n    payload: JSONDict = field(default_factory=dict)\n\n    # Optional context\n    user_id: Optional[str] = None\n    chain_id: Optional[str] = None\n    priority: Optional[str] = None\n    escalation_level: int = 1\n\n    def to_dict(self) -> JSONDict:\n        \"\"\"Convert to dictionary for Kafka serialization.\"\"\"\n        return {\n            \"event_id\": self.event_id,\n            \"event_type\": self.event_type.value,\n            \"topic\": self.topic.value,\n            \"request_id\": self.request_id,\n            \"timestamp\": self.timestamp,\n            \"timestamp_iso\": datetime.fromtimestamp(self.timestamp, tz=timezone.utc).isoformat(),\n            \"payload\": self.payload,\n            \"user_id\": self.user_id,\n            \"chain_id\": self.chain_id,\n            \"priority\": self.priority,\n            \"escalation_level\": self.escalation_level,\n        }\n\n    @classmethod\n    def from_dict(cls, data: JSONDict) -> \"HITLEvent\":\n        \"\"\"Create from dictionary (Kafka deserialization).\"\"\"\n        return cls(\n            event_id=data[\"event_id\"],\n            event_type=HITLEventType(data[\"event_type\"]),\n            topic=HITLTopic(data[\"topic\"]),\n            request_id=data[\"request_id\"],\n            timestamp=float(data[\"timestamp\"]),\n            payload=data.get(\"payload\", {}),\n            user_id=data.get(\"user_id\"),\n            chain_id=data.get(\"chain_id\"),\n            priority=data.get(\"priority\"),\n            escalation_level=int(data.get(\"escalation_level\", 1)),\n        )\n\n    @classmethod\n    def create(\n        cls,\n        event_type: HITLEventType,\n        request_id: str,\n        payload: Optional[JSONDict] = None,\n        **kwargs: KwargsType,\n    ) -> \"HITLEvent\":\n        \"\"\"\n        Create a new HITL event with auto-generated ID and timestamp.\n\n        The topic is automatically determined based on event type.\n        \"\"\"\n        # Determine topic based on event type\n        if event_type in (\n            HITLEventType.ESCALATION_TRIGGERED,\n            HITLEventType.ESCALATION_TIMEOUT,\n            HITLEventType.SLA_WARNING,\n            HITLEventType.SLA_BREACH,\n        ):\n            topic = HITLTopic.ESCALATED\n        elif event_type in (\n            HITLEventType.APPROVAL_APPROVED,\n            HITLEventType.APPROVAL_REJECTED,\n            HITLEventType.APPROVAL_CANCELLED,\n        ):\n            topic = HITLTopic.COMPLETED\n        else:\n            topic = HITLTopic.PENDING\n\n        import time\n\n        return cls(\n            event_id=str(uuid.uuid4()),\n            event_type=event_type,\n            topic=topic,\n            request_id=request_id,\n            timestamp=time.time(),\n            payload=payload or {},\n            **kwargs,\n        )\n\n\n# Type alias for event handlers\nfrom typing import Any\n\nEventHandler = Callable[[HITLEvent], Coroutine[Any, Any, None]]\n\n\n# =============================================================================\n# Kafka Client Exceptions\n# =============================================================================\n\n\nclass KafkaClientError(Exception):\n    \"\"\"Base exception for Kafka client errors.\"\"\"\n\n    pass\n\n\nclass KafkaConnectionError(KafkaClientError):\n    \"\"\"Raised when Kafka connection fails.\"\"\"\n\n    pass\n\n\nclass KafkaNotAvailableError(KafkaClientError):\n    \"\"\"Raised when aiokafka is not installed.\"\"\"\n\n    pass\n\n\nclass KafkaPublishError(KafkaClientError):\n    \"\"\"Raised when message publishing fails.\"\"\"\n\n    pass\n\n\n# =============================================================================\n# Kafka Client\n# =============================================================================\n\n\nclass HITLKafkaClient:\n    \"\"\"\n    Kafka client for HITL approval event streaming.\n\n    Features:\n    - Async producer for publishing events\n    - Consumer support with handler registration\n    - Topic auto-creation in development mode\n    - JSON serialization with proper error handling\n    - Error message sanitization for secure logging\n    - Singleton pattern support\n    \"\"\"\n\n    def __init__(\n        self,\n        bootstrap_servers: Optional[str] = None,\n        client_id: str = \"hitl-approvals\",\n        auto_create_topics: bool = True,\n    ):\n        \"\"\"\n        Initialize the Kafka client.\n\n        Args:\n            bootstrap_servers: Kafka bootstrap servers (uses settings if None)\n            client_id: Client identifier for Kafka\n            auto_create_topics: Whether to create topics on startup\n        \"\"\"\n        if not KAFKA_AVAILABLE:\n            raise KafkaNotAvailableError(\n                \"aiokafka not installed. Install with: pip install aiokafka\"\n            )\n\n        self._bootstrap_servers = bootstrap_servers or settings.kafka_bootstrap_servers\n        self._client_id = client_id\n        self._auto_create_topics = auto_create_topics\n\n        self._producer: Optional[AIOKafkaProducer] = None\n        self._consumers: Dict[str, AIOKafkaConsumer] = {}\n        self._handlers: Dict[HITLTopic, List[EventHandler]] = {topic: [] for topic in HITLTopic}\n        self._running = False\n        self._consumer_tasks: List[asyncio.Task] = []\n        self._lock = asyncio.Lock()\n\n        logger.info(\n            f\"HITLKafkaClient initialized (bootstrap={self._sanitize_url(self._bootstrap_servers)})\"\n        )\n\n    # =========================================================================\n    # Connection Management\n    # =========================================================================\n\n    async def start(self) -> None:\n        \"\"\"\n        Start the Kafka client.\n\n        Creates topics if needed and starts the producer.\n        \"\"\"\n        if self._running:\n            logger.warning(\"HITLKafkaClient already running\")\n            return\n\n        try:\n            # Create topics if enabled\n            if self._auto_create_topics:\n                await self._ensure_topics_exist()\n\n            # Start producer\n            self._producer = AIOKafkaProducer(\n                bootstrap_servers=self._bootstrap_servers,\n                client_id=self._client_id,\n                value_serializer=self._serialize_event,\n                key_serializer=lambda k: k.encode(\"utf-8\") if k else None,\n                acks=\"all\",  # Ensure durability\n                retry_backoff_ms=500,\n                request_timeout_ms=30000,\n                max_batch_size=16384,\n            )\n            await self._producer.start()\n\n            self._running = True\n            logger.info(f\"HITLKafkaClient started on {self._sanitize_url(self._bootstrap_servers)}\")\n\n        except Exception as e:\n            raise KafkaConnectionError(\n                f\"Failed to start Kafka client: {self._sanitize_error(e)}\"\n            ) from e\n\n    async def stop(self) -> None:\n        \"\"\"Stop the Kafka client and cleanup resources.\"\"\"\n        self._running = False\n\n        # Cancel consumer tasks\n        for task in self._consumer_tasks:\n            task.cancel()\n            try:\n                await task\n            except asyncio.CancelledError:\n                pass\n\n        self._consumer_tasks = []\n\n        # Stop consumers\n        for consumer in self._consumers.values():\n            try:\n                await consumer.stop()\n            except Exception as e:\n                logger.error(f\"Error stopping consumer: {self._sanitize_error(e)}\")\n\n        self._consumers = {}\n\n        # Stop producer\n        if self._producer:\n            try:\n                await self._producer.flush()\n                await self._producer.stop()\n            except Exception as e:\n                logger.error(f\"Error stopping producer: {self._sanitize_error(e)}\")\n            self._producer = None\n\n        logger.info(\"HITLKafkaClient stopped\")\n\n    async def health_check(self) -> bool:\n        \"\"\"\n        Check Kafka connectivity.\n\n        Returns:\n            True if Kafka is reachable\n        \"\"\"\n        if not self._running or not self._producer:\n            return False\n\n        try:\n            # Try to get metadata from broker\n            metadata = await self._producer.partitions_for(HITLTopic.PENDING.value)\n            return metadata is not None\n        except Exception as e:\n            logger.error(f\"Kafka health check failed: {self._sanitize_error(e)}\")\n            return False\n\n    async def _ensure_topics_exist(self) -> None:\n        \"\"\"Create HITL topics if they don't exist.\"\"\"\n        try:\n            admin = AIOKafkaAdminClient(\n                bootstrap_servers=self._bootstrap_servers,\n                client_id=f\"{self._client_id}-admin\",\n            )\n            await admin.start()\n\n            try:\n                # Get existing topics\n                existing_topics: Set[str] = set()\n                try:\n                    # describe_cluster doesn't return topics directly\n                    # We'll just try to create and handle the error\n                    await admin.describe_cluster()\n                except Exception:  # nosec B110 - intentionally ignoring errors from describe_cluster\n                    pass\n\n                # Create new topics\n                new_topics = [\n                    NewTopic(\n                        name=topic,\n                        num_partitions=3,  # Support parallelism\n                        replication_factor=1,  # Adjust for production\n                    )\n                    for topic in HITLTopic.all_topics()\n                    if topic not in existing_topics\n                ]\n\n                if new_topics:\n                    try:\n                        await admin.create_topics(new_topics)\n                        logger.info(f\"Created Kafka topics: {[t.name for t in new_topics]}\")\n                    except Exception as e:\n                        # Topic may already exist\n                        if \"TopicAlreadyExistsError\" not in str(e):\n                            logger.warning(f\"Topic creation warning: {self._sanitize_error(e)}\")\n\n            finally:\n                await admin.close()\n\n        except Exception as e:\n            logger.warning(\n                f\"Could not ensure topics exist (will be created on first use): \"\n                f\"{self._sanitize_error(e)}\"\n            )\n\n    # =========================================================================\n    # Publishing\n    # =========================================================================\n\n    async def publish_event(self, event: HITLEvent) -> bool:\n        \"\"\"\n        Publish an HITL event to Kafka.\n\n        Args:\n            event: The event to publish\n\n        Returns:\n            True if published successfully\n\n        Raises:\n            KafkaPublishError: If publishing fails\n        \"\"\"\n        if not self._running or not self._producer:\n            raise KafkaPublishError(\"Kafka producer not started\")\n\n        try:\n            # Use request_id as partition key for ordering within a request\n            key = event.request_id\n\n            await self._producer.send_and_wait(\n                topic=event.topic.value,\n                value=event.to_dict(),\n                key=key,\n            )\n\n            logger.debug(\n                f\"Published event {event.event_id} to {event.topic.value} \"\n                f\"(type={event.event_type.value}, request={event.request_id})\"\n            )\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to publish event {event.event_id}: {self._sanitize_error(e)}\")\n            raise KafkaPublishError(f\"Failed to publish event: {self._sanitize_error(e)}\") from e\n\n    async def publish_approval_requested(\n        self,\n        request_id: str,\n        chain_id: str,\n        priority: str,\n        decision_type: str,\n        impact_level: str,\n        requester_id: Optional[str] = None,\n        metadata: Optional[JSONDict] = None,\n    ) -> HITLEvent:\n        \"\"\"\n        Publish an approval requested event.\n\n        Args:\n            request_id: Unique approval request ID\n            chain_id: Approval chain ID\n            priority: Request priority (critical/high/medium/low)\n            decision_type: Type of decision requiring approval\n            impact_level: Impact level of the decision\n            requester_id: ID of the user/system requesting approval\n            metadata: Additional metadata\n\n        Returns:\n            The published event\n        \"\"\"\n        event = HITLEvent.create(\n            event_type=HITLEventType.APPROVAL_REQUESTED,\n            request_id=request_id,\n            chain_id=chain_id,\n            priority=priority,\n            payload={\n                \"decision_type\": decision_type,\n                \"impact_level\": impact_level,\n                \"requester_id\": requester_id,\n                **(metadata or {}),\n            },\n        )\n        await self.publish_event(event)\n        return event\n\n    async def publish_escalation(\n        self,\n        request_id: str,\n        reason: str,\n        from_level: int,\n        to_level: int,\n        timeout_minutes: Optional[int] = None,\n        metadata: Optional[JSONDict] = None,\n    ) -> HITLEvent:\n        \"\"\"\n        Publish an escalation event.\n\n        Args:\n            request_id: Approval request ID\n            reason: Reason for escalation (timeout/manual/sla_breach)\n            from_level: Previous escalation level\n            to_level: New escalation level\n            timeout_minutes: Timeout that triggered escalation (if applicable)\n            metadata: Additional metadata\n\n        Returns:\n            The published event\n        \"\"\"\n        event = HITLEvent.create(\n            event_type=HITLEventType.ESCALATION_TRIGGERED,\n            request_id=request_id,\n            escalation_level=to_level,\n            payload={\n                \"reason\": reason,\n                \"from_level\": from_level,\n                \"to_level\": to_level,\n                \"timeout_minutes\": timeout_minutes,\n                **(metadata or {}),\n            },\n        )\n        await self.publish_event(event)\n        return event\n\n    async def publish_approval_completed(\n        self,\n        request_id: str,\n        decision: str,  # approved/rejected\n        approver_id: str,\n        reasoning: Optional[str] = None,\n        chain_id: Optional[str] = None,\n        metadata: Optional[JSONDict] = None,\n    ) -> HITLEvent:\n        \"\"\"\n        Publish an approval completed event.\n\n        Args:\n            request_id: Approval request ID\n            decision: Decision made (approved/rejected)\n            approver_id: ID of the approver\n            reasoning: Reasoning for the decision\n            chain_id: Approval chain ID\n            metadata: Additional metadata\n\n        Returns:\n            The published event\n        \"\"\"\n        event_type = (\n            HITLEventType.APPROVAL_APPROVED\n            if decision == \"approved\"\n            else HITLEventType.APPROVAL_REJECTED\n        )\n\n        event = HITLEvent.create(\n            event_type=event_type,\n            request_id=request_id,\n            user_id=approver_id,\n            chain_id=chain_id,\n            payload={\n                \"decision\": decision,\n                \"approver_id\": approver_id,\n                \"reasoning\": reasoning,\n                **(metadata or {}),\n            },\n        )\n        await self.publish_event(event)\n        return event\n\n    async def publish_sla_breach(\n        self,\n        request_id: str,\n        priority: str,\n        sla_timeout_minutes: int,\n        actual_time_minutes: float,\n        escalation_level: int,\n        metadata: Optional[JSONDict] = None,\n    ) -> HITLEvent:\n        \"\"\"\n        Publish an SLA breach event.\n\n        Args:\n            request_id: Approval request ID\n            priority: Request priority\n            sla_timeout_minutes: Configured SLA timeout\n            actual_time_minutes: Actual time elapsed\n            escalation_level: Current escalation level\n            metadata: Additional metadata\n\n        Returns:\n            The published event\n        \"\"\"\n        event = HITLEvent.create(\n            event_type=HITLEventType.SLA_BREACH,\n            request_id=request_id,\n            priority=priority,\n            escalation_level=escalation_level,\n            payload={\n                \"sla_timeout_minutes\": sla_timeout_minutes,\n                \"actual_time_minutes\": actual_time_minutes,\n                \"overage_minutes\": actual_time_minutes - sla_timeout_minutes,\n                \"overage_percent\": (\n                    ((actual_time_minutes - sla_timeout_minutes) / sla_timeout_minutes) * 100\n                    if sla_timeout_minutes > 0\n                    else 0\n                ),\n                **(metadata or {}),\n            },\n        )\n        await self.publish_event(event)\n        return event\n\n    # =========================================================================\n    # Subscribing\n    # =========================================================================\n\n    def register_handler(\n        self,\n        topic: HITLTopic,\n        handler: EventHandler,\n    ) -> None:\n        \"\"\"\n        Register an event handler for a topic.\n\n        Args:\n            topic: Topic to handle events from\n            handler: Async function to call for each event\n        \"\"\"\n        self._handlers[topic].append(handler)\n        logger.info(f\"Registered handler for {topic.value}: {handler.__name__}\")\n\n    def unregister_handler(\n        self,\n        topic: HITLTopic,\n        handler: EventHandler,\n    ) -> bool:\n        \"\"\"\n        Unregister an event handler.\n\n        Args:\n            topic: Topic the handler was registered for\n            handler: Handler to unregister\n\n        Returns:\n            True if handler was found and removed\n        \"\"\"\n        try:\n            self._handlers[topic].remove(handler)\n            logger.info(f\"Unregistered handler for {topic.value}: {handler.__name__}\")\n            return True\n        except ValueError:\n            return False\n\n    async def start_consuming(\n        self,\n        topics: Optional[List[HITLTopic]] = None,\n        group_id: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Start consuming events from topics.\n\n        Args:\n            topics: Topics to consume (all if None)\n            group_id: Consumer group ID (auto-generated if None)\n        \"\"\"\n        if not self._running:\n            raise KafkaClientError(\"Kafka client not started\")\n\n        topics = topics or list(HITLTopic)\n        topic_names = [t.value for t in topics]\n        group_id = group_id or f\"{self._client_id}-consumer-group\"\n\n        consumer = AIOKafkaConsumer(\n            *topic_names,\n            bootstrap_servers=self._bootstrap_servers,\n            group_id=group_id,\n            value_deserializer=self._deserialize_event,\n            auto_offset_reset=\"latest\",\n            enable_auto_commit=True,\n        )\n\n        consumer_id = f\"consumer-{uuid.uuid4().hex[:8]}\"\n        self._consumers[consumer_id] = consumer\n\n        await consumer.start()\n\n        async def consume_loop():\n            \"\"\"Process messages from consumer.\"\"\"\n            try:\n                async for msg in consumer:\n                    if not self._running:\n                        break\n\n                    try:\n                        event = msg.value\n                        if event:\n                            topic = HITLTopic(msg.topic)\n                            for handler in self._handlers[topic]:\n                                try:\n                                    await handler(event)\n                                except Exception as e:\n                                    logger.error(\n                                        f\"Handler error for {event.event_id}: \"\n                                        f\"{self._sanitize_error(e)}\"\n                                    )\n\n                    except Exception as e:\n                        logger.error(f\"Error processing Kafka message: {self._sanitize_error(e)}\")\n\n            except asyncio.CancelledError:\n                pass\n            finally:\n                await consumer.stop()\n                self._consumers.pop(consumer_id, None)\n\n        task = asyncio.create_task(consume_loop())\n        self._consumer_tasks.append(task)\n\n        logger.info(f\"Started consuming from topics: {topic_names}\")\n\n    # =========================================================================\n    # Serialization\n    # =========================================================================\n\n    def _serialize_event(self, event_dict: JSONDict) -> bytes:\n        \"\"\"Serialize event to JSON bytes.\"\"\"\n        return json.dumps(event_dict, default=str).encode(\"utf-8\")\n\n    def _deserialize_event(self, data: bytes) -> Optional[HITLEvent]:\n        \"\"\"Deserialize event from JSON bytes.\"\"\"\n        try:\n            event_dict = json.loads(data.decode(\"utf-8\"))\n            return HITLEvent.from_dict(event_dict)\n        except Exception as e:\n            logger.error(f\"Failed to deserialize event: {self._sanitize_error(e)}\")\n            return None\n\n    # =========================================================================\n    # Utilities\n    # =========================================================================\n\n    def _sanitize_url(self, url: str) -> str:\n        \"\"\"Sanitize URL for logging (remove passwords).\"\"\"\n        if \"@\" in url:\n            parts = url.split(\"@\")\n            return f\"***@{parts[-1]}\"\n        return url\n\n    def _sanitize_error(self, error: Exception) -> str:\n        \"\"\"Sanitize error message for logging (remove sensitive data).\"\"\"\n        error_msg = str(error)\n        error_msg = re.sub(r\"bootstrap_servers='[^']+'\", \"bootstrap_servers='REDACTED'\", error_msg)\n        error_msg = re.sub(r\"password='[^']+'\", \"password='REDACTED'\", error_msg)\n        error_msg = re.sub(r\"sasl_plain_password='[^']+'\", \"password='REDACTED'\", error_msg)\n        return error_msg\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Check if client is running.\"\"\"\n        return self._running\n\n    @property\n    def topics(self) -> List[str]:\n        \"\"\"Get list of HITL topic names.\"\"\"\n        return HITLTopic.all_topics()\n\n\n# =============================================================================\n# Singleton Instance Management\n# =============================================================================\n\n_kafka_client: Optional[HITLKafkaClient] = None\n\n\ndef get_kafka_client() -> HITLKafkaClient:\n    \"\"\"\n    Get the global HITLKafkaClient instance.\n\n    Returns:\n        The singleton HITLKafkaClient instance\n\n    Raises:\n        KafkaNotAvailableError: If aiokafka is not installed\n    \"\"\"\n    global _kafka_client\n    if _kafka_client is None:\n        _kafka_client = HITLKafkaClient()\n    return _kafka_client\n\n\nasync def initialize_kafka_client(\n    bootstrap_servers: Optional[str] = None,\n    client_id: str = \"hitl-approvals\",\n    auto_create_topics: bool = True,\n    start: bool = True,\n) -> HITLKafkaClient:\n    \"\"\"\n    Initialize and optionally start the global Kafka client.\n\n    Args:\n        bootstrap_servers: Kafka bootstrap servers\n        client_id: Client identifier\n        auto_create_topics: Whether to create topics on startup\n        start: Whether to start the client\n\n    Returns:\n        The initialized HITLKafkaClient\n    \"\"\"\n    global _kafka_client\n\n    _kafka_client = HITLKafkaClient(\n        bootstrap_servers=bootstrap_servers,\n        client_id=client_id,\n        auto_create_topics=auto_create_topics,\n    )\n\n    if start:\n        await _kafka_client.start()\n\n    return _kafka_client\n\n\nasync def close_kafka_client() -> None:\n    \"\"\"Close and cleanup the global Kafka client.\"\"\"\n    global _kafka_client\n\n    if _kafka_client:\n        await _kafka_client.stop()\n        _kafka_client = None\n\n\ndef reset_kafka_client() -> None:\n    \"\"\"\n    Reset the global HITLKafkaClient instance.\n\n    Used primarily for test isolation.\n    \"\"\"\n    global _kafka_client\n    _kafka_client = None\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.120482",
  "last_updated": "2026-01-04T05:35:58.746583"
}