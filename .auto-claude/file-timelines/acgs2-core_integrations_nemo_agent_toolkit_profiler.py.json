{
  "file_path": "src/core/integrations/nemo_agent_toolkit/profiler.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Constitutional Profiler for NeMo-Agent-Toolkit\nConstitutional Hash: cdd01ef066bc6cf2\n\nProvides profiling and metrics collection for AI agents\nwith constitutional governance tracking.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport statistics\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import UTC, datetime\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Any, Callable\n\nif TYPE_CHECKING:\n    pass\n\nCONSTITUTIONAL_HASH: str = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\n\nclass MetricType(str, Enum):\n    \"\"\"Types of metrics collected.\"\"\"\n\n    LATENCY = \"latency\"\n    THROUGHPUT = \"throughput\"\n    COMPLIANCE = \"compliance\"\n    VIOLATION = \"violation\"\n    TOKEN_USAGE = \"token_usage\"\n    COST = \"cost\"\n    GUARDRAIL_CHECK = \"guardrail_check\"\n\n\n@dataclass\nclass GovernanceMetrics:\n    \"\"\"Governance-specific metrics for AI agents.\"\"\"\n\n    # Compliance metrics\n    total_requests: int = 0\n    compliant_requests: int = 0\n    blocked_requests: int = 0\n    modified_requests: int = 0\n\n    # Violation metrics\n    privacy_violations: int = 0\n    safety_violations: int = 0\n    ethics_violations: int = 0\n    compliance_violations: int = 0\n\n    # Guardrail metrics\n    input_checks: int = 0\n    output_checks: int = 0\n    input_blocks: int = 0\n    output_blocks: int = 0\n    pii_redactions: int = 0\n\n    # Performance metrics\n    average_check_latency_ms: float = 0.0\n    p50_check_latency_ms: float = 0.0\n    p95_check_latency_ms: float = 0.0\n    p99_check_latency_ms: float = 0.0\n\n    # Constitutional tracking\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n    collection_start: datetime = field(default_factory=lambda: datetime.now(UTC))\n    collection_end: datetime | None = None\n\n    @property\n    def compliance_rate(self) -> float:\n        \"\"\"Calculate compliance rate.\"\"\"\n        if self.total_requests == 0:\n            return 1.0\n        return self.compliant_requests / self.total_requests\n\n    @property\n    def block_rate(self) -> float:\n        \"\"\"Calculate block rate.\"\"\"\n        if self.total_requests == 0:\n            return 0.0\n        return self.blocked_requests / self.total_requests\n\n    @property\n    def violation_rate(self) -> float:\n        \"\"\"Calculate violation rate.\"\"\"\n        total_violations = (\n            self.privacy_violations\n            + self.safety_violations\n            + self.ethics_violations\n            + self.compliance_violations\n        )\n        if self.total_requests == 0:\n            return 0.0\n        return total_violations / self.total_requests\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"total_requests\": self.total_requests,\n            \"compliant_requests\": self.compliant_requests,\n            \"blocked_requests\": self.blocked_requests,\n            \"modified_requests\": self.modified_requests,\n            \"compliance_rate\": self.compliance_rate,\n            \"block_rate\": self.block_rate,\n            \"violation_rate\": self.violation_rate,\n            \"violations\": {\n                \"privacy\": self.privacy_violations,\n                \"safety\": self.safety_violations,\n                \"ethics\": self.ethics_violations,\n                \"compliance\": self.compliance_violations,\n            },\n            \"guardrails\": {\n                \"input_checks\": self.input_checks,\n                \"output_checks\": self.output_checks,\n                \"input_blocks\": self.input_blocks,\n                \"output_blocks\": self.output_blocks,\n                \"pii_redactions\": self.pii_redactions,\n            },\n            \"latency\": {\n                \"average_ms\": self.average_check_latency_ms,\n                \"p50_ms\": self.p50_check_latency_ms,\n                \"p95_ms\": self.p95_check_latency_ms,\n                \"p99_ms\": self.p99_check_latency_ms,\n            },\n            \"constitutional_hash\": self.constitutional_hash,\n            \"collection_start\": self.collection_start.isoformat(),\n            \"collection_end\": self.collection_end.isoformat() if self.collection_end else None,\n        }\n\n\n@dataclass\nclass ProfilerEvent:\n    \"\"\"A profiler event.\"\"\"\n\n    event_type: str\n    name: str\n    duration_ms: float\n    metadata: dict[str, Any] = field(default_factory=dict)\n    timestamp: datetime = field(default_factory=lambda: datetime.now(UTC))\n    constitutional_hash: str = CONSTITUTIONAL_HASH\n\n\nclass ConstitutionalProfiler:\n    \"\"\"\n    Profiler for AI agents with constitutional governance tracking.\n\n    Integrates with NeMo-Agent-Toolkit's profiling capabilities\n    while adding constitutional compliance metrics.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"default\",\n        enable_detailed_logging: bool = False,\n        export_interval_seconds: float = 60.0,\n    ) -> None:\n        \"\"\"\n        Initialize the profiler.\n\n        Args:\n            name: Profiler instance name\n            enable_detailed_logging: Enable detailed event logging\n            export_interval_seconds: Interval for metric exports\n        \"\"\"\n        self._name = name\n        self._detailed_logging = enable_detailed_logging\n        self._export_interval = export_interval_seconds\n\n        self._metrics = GovernanceMetrics()\n        self._events: list[ProfilerEvent] = []\n        self._latencies: list[float] = []\n        self._callbacks: list[Callable[[dict[str, Any]], None]] = []\n\n        self._running = False\n        self._export_task: asyncio.Task | None = None\n\n    @property\n    def name(self) -> str:\n        \"\"\"Get profiler name.\"\"\"\n        return self._name\n\n    @property\n    def metrics(self) -> GovernanceMetrics:\n        \"\"\"Get current metrics.\"\"\"\n        return self._metrics\n\n    def start(self) -> None:\n        \"\"\"Start the profiler.\"\"\"\n        if self._running:\n            return\n\n        self._running = True\n        self._metrics = GovernanceMetrics()\n        self._events.clear()\n        self._latencies.clear()\n\n        logger.info(f\"Profiler '{self._name}' started\")\n\n    def stop(self) -> GovernanceMetrics:\n        \"\"\"Stop the profiler and return final metrics.\"\"\"\n        self._running = False\n        self._metrics.collection_end = datetime.now(UTC)\n\n        if self._export_task:\n            self._export_task.cancel()\n            self._export_task = None\n\n        # Calculate final latency percentiles\n        self._update_latency_stats()\n\n        logger.info(f\"Profiler '{self._name}' stopped\")\n        return self._metrics\n\n    def record_request(\n        self,\n        compliant: bool,\n        blocked: bool = False,\n        modified: bool = False,\n    ) -> None:\n        \"\"\"Record a request.\"\"\"\n        self._metrics.total_requests += 1\n        if compliant:\n            self._metrics.compliant_requests += 1\n        if blocked:\n            self._metrics.blocked_requests += 1\n        if modified:\n            self._metrics.modified_requests += 1\n\n    def record_violation(\n        self,\n        violation_type: str,\n        details: dict[str, Any] | None = None,\n    ) -> None:\n        \"\"\"Record a violation.\"\"\"\n        if violation_type == \"privacy\":\n            self._metrics.privacy_violations += 1\n        elif violation_type == \"safety\":\n            self._metrics.safety_violations += 1\n        elif violation_type == \"ethics\":\n            self._metrics.ethics_violations += 1\n        elif violation_type == \"compliance\":\n            self._metrics.compliance_violations += 1\n\n        if self._detailed_logging:\n            self._record_event(\"violation\", violation_type, 0, details or {})\n\n    def record_guardrail_check(\n        self,\n        direction: str,\n        blocked: bool,\n        pii_redacted: bool = False,\n        latency_ms: float = 0.0,\n    ) -> None:\n        \"\"\"Record a guardrail check.\"\"\"\n        if direction == \"input\":\n            self._metrics.input_checks += 1\n            if blocked:\n                self._metrics.input_blocks += 1\n        elif direction == \"output\":\n            self._metrics.output_checks += 1\n            if blocked:\n                self._metrics.output_blocks += 1\n\n        if pii_redacted:\n            self._metrics.pii_redactions += 1\n\n        if latency_ms > 0:\n            self._latencies.append(latency_ms)\n\n        if self._detailed_logging:\n            self._record_event(\n                \"guardrail_check\",\n                direction,\n                latency_ms,\n                {\n                    \"blocked\": blocked,\n                    \"pii_redacted\": pii_redacted,\n                },\n            )\n\n    def record_latency(self, latency_ms: float) -> None:\n        \"\"\"Record a latency measurement.\"\"\"\n        self._latencies.append(latency_ms)\n\n    def _record_event(\n        self,\n        event_type: str,\n        name: str,\n        duration_ms: float,\n        metadata: dict[str, Any],\n    ) -> None:\n        \"\"\"Record a detailed event.\"\"\"\n        event = ProfilerEvent(\n            event_type=event_type,\n            name=name,\n            duration_ms=duration_ms,\n            metadata=metadata,\n        )\n        self._events.append(event)\n\n        # Notify callbacks\n        for callback in self._callbacks:\n            try:\n                callback(event.__dict__)\n            except Exception as e:\n                logger.error(f\"Callback error: {e}\")\n\n    def _update_latency_stats(self) -> None:\n        \"\"\"Update latency statistics.\"\"\"\n        if not self._latencies:\n            return\n\n        sorted_latencies = sorted(self._latencies)\n        n = len(sorted_latencies)\n\n        self._metrics.average_check_latency_ms = statistics.mean(sorted_latencies)\n        self._metrics.p50_check_latency_ms = sorted_latencies[int(n * 0.50)]\n        self._metrics.p95_check_latency_ms = (\n            sorted_latencies[int(n * 0.95)] if n >= 20 else sorted_latencies[-1]\n        )\n        self._metrics.p99_check_latency_ms = (\n            sorted_latencies[int(n * 0.99)] if n >= 100 else sorted_latencies[-1]\n        )\n\n    def add_callback(self, callback: Callable[[dict[str, Any]], None]) -> None:\n        \"\"\"Add a callback for profiler events.\"\"\"\n        self._callbacks.append(callback)\n\n    def remove_callback(self, callback: Callable[[dict[str, Any]], None]) -> None:\n        \"\"\"Remove a callback.\"\"\"\n        if callback in self._callbacks:\n            self._callbacks.remove(callback)\n\n    def get_events(self) -> list[ProfilerEvent]:\n        \"\"\"Get all recorded events.\"\"\"\n        return self._events.copy()\n\n    async def export_metrics(self) -> dict[str, Any]:\n        \"\"\"Export current metrics.\"\"\"\n        self._update_latency_stats()\n        return self._metrics.to_dict()\n\n    def create_context_manager(self, operation_name: str) -> ProfilerContext:\n        \"\"\"Create a context manager for timing operations.\"\"\"\n        return ProfilerContext(self, operation_name)\n\n    def time_operation(self, name: str) -> Callable[[Callable[..., Any]], Callable[..., Any]]:\n        \"\"\"Decorator to time an operation.\"\"\"\n\n        def decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n            if asyncio.iscoroutinefunction(func):\n\n                async def async_wrapper(*args: Any, **kwargs: Any) -> Any:\n                    start = time.perf_counter()\n                    try:\n                        return await func(*args, **kwargs)\n                    finally:\n                        elapsed = (time.perf_counter() - start) * 1000\n                        self.record_latency(elapsed)\n                        if self._detailed_logging:\n                            self._record_event(\"operation\", name, elapsed, {})\n\n                return async_wrapper\n            else:\n\n                def sync_wrapper(*args: Any, **kwargs: Any) -> Any:\n                    start = time.perf_counter()\n                    try:\n                        return func(*args, **kwargs)\n                    finally:\n                        elapsed = (time.perf_counter() - start) * 1000\n                        self.record_latency(elapsed)\n                        if self._detailed_logging:\n                            self._record_event(\"operation\", name, elapsed, {})\n\n                return sync_wrapper\n\n        return decorator\n\n    def get_summary(self) -> str:\n        \"\"\"Get a human-readable summary of metrics.\"\"\"\n        m = self._metrics\n        return f\"\"\"\nConstitutional Profiler Summary: {self._name}\n============================================\nTotal Requests: {m.total_requests}\nCompliance Rate: {m.compliance_rate:.2%}\nBlock Rate: {m.block_rate:.2%}\nViolation Rate: {m.violation_rate:.2%}\n\nViolations:\n  - Privacy: {m.privacy_violations}\n  - Safety: {m.safety_violations}\n  - Ethics: {m.ethics_violations}\n  - Compliance: {m.compliance_violations}\n\nGuardrail Checks:\n  - Input Checks: {m.input_checks} (Blocked: {m.input_blocks})\n  - Output Checks: {m.output_checks} (Blocked: {m.output_blocks})\n  - PII Redactions: {m.pii_redactions}\n\nLatency:\n  - Average: {m.average_check_latency_ms:.2f}ms\n  - P50: {m.p50_check_latency_ms:.2f}ms\n  - P95: {m.p95_check_latency_ms:.2f}ms\n  - P99: {m.p99_check_latency_ms:.2f}ms\n\nConstitutional Hash: {m.constitutional_hash}\nCollection Period: {m.collection_start.isoformat()} - {m.collection_end.isoformat() if m.collection_end else \"ongoing\"}\n\"\"\"\n\n\nclass ProfilerContext:\n    \"\"\"Context manager for timing operations.\"\"\"\n\n    def __init__(self, profiler: ConstitutionalProfiler, operation_name: str) -> None:\n        \"\"\"Initialize context.\"\"\"\n        self._profiler = profiler\n        self._operation_name = operation_name\n        self._start_time: float = 0\n\n    def __enter__(self) -> ProfilerContext:\n        \"\"\"Enter context.\"\"\"\n        self._start_time = time.perf_counter()\n        return self\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:\n        \"\"\"Exit context.\"\"\"\n        elapsed = (time.perf_counter() - self._start_time) * 1000\n        self._profiler.record_latency(elapsed)\n        if self._profiler._detailed_logging:\n            self._profiler._record_event(\n                \"operation\",\n                self._operation_name,\n                elapsed,\n                {\"error\": str(exc_val) if exc_val else None},\n            )\n\n    async def __aenter__(self) -> ProfilerContext:\n        \"\"\"Async enter context.\"\"\"\n        self._start_time = time.perf_counter()\n        return self\n\n    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:\n        \"\"\"Async exit context.\"\"\"\n        elapsed = (time.perf_counter() - self._start_time) * 1000\n        self._profiler.record_latency(elapsed)\n        if self._profiler._detailed_logging:\n            self._profiler._record_event(\n                \"operation\",\n                self._operation_name,\n                elapsed,\n                {\"error\": str(exc_val) if exc_val else None},\n            )\n\n\nclass NeMoProfilerBridge:\n    \"\"\"\n    Bridge between ACGS-2 profiler and NeMo-Agent-Toolkit profiler.\n\n    Allows seamless integration with NeMo's built-in profiling\n    while adding constitutional governance metrics.\n    \"\"\"\n\n    def __init__(\n        self,\n        constitutional_profiler: ConstitutionalProfiler,\n    ) -> None:\n        \"\"\"\n        Initialize the bridge.\n\n        Args:\n            constitutional_profiler: ACGS-2 constitutional profiler\n        \"\"\"\n        self._profiler = constitutional_profiler\n        self._nemo_profiler: Any = None\n\n    def connect_nemo_profiler(self, nemo_profiler: Any) -> None:\n        \"\"\"\n        Connect to NeMo-Agent-Toolkit profiler.\n\n        Args:\n            nemo_profiler: NeMo profiler instance\n        \"\"\"\n        self._nemo_profiler = nemo_profiler\n\n        # Add callback to sync events\n        if hasattr(nemo_profiler, \"add_callback\"):\n            nemo_profiler.add_callback(self._on_nemo_event)\n\n    def _on_nemo_event(self, event: dict[str, Any]) -> None:\n        \"\"\"Handle events from NeMo profiler.\"\"\"\n        # Map NeMo events to constitutional profiler\n        event_type = event.get(\"type\", \"unknown\")\n\n        if event_type == \"inference\":\n            latency = event.get(\"duration_ms\", 0)\n            self._profiler.record_latency(latency)\n\n        elif event_type == \"tool_call\":\n            # Track tool calls for governance\n            self._profiler._record_event(\n                \"nemo_tool_call\",\n                event.get(\"tool_name\", \"unknown\"),\n                event.get(\"duration_ms\", 0),\n                event,\n            )\n\n    def get_combined_metrics(self) -> dict[str, Any]:\n        \"\"\"Get combined metrics from both profilers.\"\"\"\n        metrics = self._profiler.metrics.to_dict()\n\n        if self._nemo_profiler and hasattr(self._nemo_profiler, \"get_metrics\"):\n            nemo_metrics = self._nemo_profiler.get_metrics()\n            metrics[\"nemo_metrics\"] = nemo_metrics\n\n        return metrics\n\n    def export_for_nemo(self) -> dict[str, Any]:\n        \"\"\"Export metrics in NeMo-compatible format.\"\"\"\n        metrics = self._profiler.metrics\n\n        return {\n            \"governance\": {\n                \"compliance_rate\": metrics.compliance_rate,\n                \"violation_rate\": metrics.violation_rate,\n                \"block_rate\": metrics.block_rate,\n            },\n            \"performance\": {\n                \"avg_latency_ms\": metrics.average_check_latency_ms,\n                \"p50_latency_ms\": metrics.p50_check_latency_ms,\n                \"p95_latency_ms\": metrics.p95_check_latency_ms,\n                \"p99_latency_ms\": metrics.p99_check_latency_ms,\n            },\n            \"guardrails\": {\n                \"total_checks\": metrics.input_checks + metrics.output_checks,\n                \"total_blocks\": metrics.input_blocks + metrics.output_blocks,\n                \"pii_redactions\": metrics.pii_redactions,\n            },\n            \"constitutional_hash\": CONSTITUTIONAL_HASH,\n        }\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.116053",
  "last_updated": "2026-01-04T05:35:58.433461"
}