{
  "file_path": "acgs2-core/enhanced_agent_bus/kafka_bus.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 Kafka Event Bus Implementation\nConstitutional Hash: cdd01ef066bc6cf2\nProvides high-throughput, multi-tenant isolated messaging.\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport re\nimport ssl\nfrom typing import Any, Callable, Dict, List, Optional\n\ntry:\n    from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n\n    KAFKA_AVAILABLE = True\nexcept ImportError:\n    KAFKA_AVAILABLE = False\n\ntry:\n    from .exceptions import MessageDeliveryError\n    from .models import AgentMessage, MessageType\n    from .shared.config import settings\nexcept ImportError:\n    from exceptions import MessageDeliveryError  # type: ignore\n    from models import AgentMessage, MessageType  # type: ignore\n\n    from shared.config import settings  # type: ignore\n\nlogger = logging.getLogger(__name__)\n\n\nclass KafkaEventBus:\n    \"\"\"\n    Kafka-based event bus for high-performance multi-agent orchestration.\n    Supports topic-level multi-tenant isolation.\n    \"\"\"\n\n    def __init__(self, bootstrap_servers: str = \"localhost:9092\", client_id: str = \"acgs2-bus\"):\n        self.bootstrap_servers = bootstrap_servers\n        self.client_id = client_id\n        self.producer: Optional[AIOKafkaProducer] = None\n        self._consumers: Dict[str, AIOKafkaConsumer] = {}\n        self._running = False\n        self._ssl_context: Optional[ssl.SSLContext] = self._create_ssl_context()\n\n    async def start(self):\n        \"\"\"Start the Kafka producer.\"\"\"\n        if not KAFKA_AVAILABLE:\n            logger.error(\"aiokafka not installed. KafkaEventBus unavailable.\")\n            return\n\n        self.producer = AIOKafkaProducer(\n            bootstrap_servers=self.bootstrap_servers,\n            client_id=self.client_id,\n            value_serializer=lambda v: json.dumps(v, default=str).encode(\"utf-8\"),\n            acks=\"all\",  # Ensure durability for production\n            enable_idempotence=True,  # Prevent duplicate votes\n            retry_backoff_ms=500,\n            security_protocol=settings.kafka.get(\"security_protocol\", \"PLAINTEXT\"),\n            ssl_context=self._ssl_context,\n        )\n        await self.producer.start()\n        self._running = True\n        logger.info(f\"KafkaEventBus started on {self.bootstrap_servers}\")\n\n    async def stop(self):\n        \"\"\"Stop the Kafka producer and all consumers.\"\"\"\n        self._running = False\n        if self.producer:\n            await self.producer.flush()  # Ensure all messages are sent\n            await self.producer.stop()\n        for consumer in self._consumers.values():\n            await consumer.stop()\n        logger.info(\"KafkaEventBus stopped\")\n\n    def _create_ssl_context(self) -> Optional[ssl.SSLContext]:\n        \"\"\"Create SSL context for Kafka if security protocol is SSL.\"\"\"\n        security_protocol = settings.kafka.get(\"security_protocol\", \"PLAINTEXT\")\n        if security_protocol != \"SSL\":\n            return None\n\n        context = ssl.create_default_context(cafile=settings.kafka.get(\"ssl_ca_location\"))\n\n        cert_file = settings.kafka.get(\"ssl_certificate_location\")\n        key_file = settings.kafka.get(\"ssl_key_location\")\n        password = settings.kafka.get(\"ssl_password\")\n\n        if cert_file and key_file:\n            context.load_cert_chain(certfile=cert_file, keyfile=key_file, password=password)\n\n        return context\n\n    def _get_topic_name(self, tenant_id: str, message_type: str) -> str:\n        \"\"\"\n        Generate a multi-tenant isolated topic name.\n        Format: acgs.tenant.{tenant_id}.{message_type}\n        \"\"\"\n        safe_tenant = tenant_id.replace(\".\", \"_\") if tenant_id else \"default\"\n        return f\"acgs.tenant.{safe_tenant}.{message_type.lower()}\"\n\n    def _get_vote_topic(self, tenant_id: str) -> str:\n        \"\"\"\n        Get vote topic name for a tenant.\n        Format: acgs.tenant.{tenant_id}.votes\n        \"\"\"\n        safe_tenant = tenant_id.replace(\".\", \"_\") if tenant_id else \"default\"\n        pattern = settings.voting.vote_topic_pattern\n        return pattern.format(tenant_id=safe_tenant)\n\n    def _get_audit_topic(self, tenant_id: str) -> str:\n        \"\"\"\n        Get audit topic name for a tenant.\n        Format: acgs.tenant.{tenant_id}.audit.votes\n        \"\"\"\n        safe_tenant = tenant_id.replace(\".\", \"_\") if tenant_id else \"default\"\n        pattern = settings.voting.audit_topic_pattern\n        return pattern.format(tenant_id=safe_tenant)\n\n    async def send_message(self, message: AgentMessage) -> bool:\n        \"\"\"Send a message to the appropriate Kafka topic.\"\"\"\n        if not self.producer or not self._running:\n            raise MessageDeliveryError(\n                message_id=message.message_id,\n                target_agent=message.to_agent or \"unknown\",\n                reason=\"Kafka producer not started\",\n            )\n\n        topic = self._get_topic_name(message.tenant_id, message.message_type.name)\n\n        # Use conversation_id as partition key to ensure ordering within a session\n        key = message.conversation_id.encode(\"utf-8\") if message.conversation_id else None\n\n        try:\n            # Re-convert to dict ensuring all fields are present\n            msg_dict = message.to_dict_raw()\n\n            await self.producer.send_and_wait(topic, value=msg_dict, key=key)\n            logger.debug(f\"Message {message.message_id} sent to topic {topic}\")\n            return True\n        except Exception as e:\n            logger.error(\n                f\"Failed to send message to Kafka (topic={topic}): {self._sanitize_error(e)}\"\n            )\n            return False\n\n    async def subscribe(self, tenant_id: str, message_types: List[MessageType], handler: Callable):\n        \"\"\"Subscribe to topics and process messages.\"\"\"\n        if not KAFKA_AVAILABLE:\n            return\n\n        topics = [self._get_topic_name(tenant_id, mt.name) for mt in message_types]\n        consumer_id = f\"{tenant_id}-{''.join([mt.name for mt in message_types])}\"\n\n        consumer = AIOKafkaConsumer(\n            *topics,\n            bootstrap_servers=self.bootstrap_servers,\n            group_id=f\"{self.client_id}-group-{tenant_id}\",\n            value_deserializer=lambda v: json.loads(v.decode(\"utf-8\")),\n            security_protocol=settings.kafka.get(\"security_protocol\", \"PLAINTEXT\"),\n            ssl_context=self._ssl_context,\n        )\n\n        self._consumers[consumer_id] = consumer\n        await consumer.start()\n\n        async def consume_loop():\n            try:\n                async for msg in consumer:\n                    if not self._running:\n                        break\n                    try:\n                        # Reconstruct AgentMessage\n                        message_data = msg.value\n                        # In a real implementation, we'd have a from_dict method\n                        # For now, we'll assume the handler can take the dict or we wrap it\n                        await handler(message_data)\n                    except Exception as e:\n                        logger.error(f\"Error in Kafka message handler: {self._sanitize_error(e)}\")\n            finally:\n                await consumer.stop()\n\n        asyncio.create_task(consume_loop())\n        logger.info(f\"Subscribed to topics: {topics}\")\n\n    def _sanitize_error(self, error: Exception) -> str:\n        \"\"\"Strip sensitive metadata from error messages (VULN-008).\"\"\"\n        error_msg = str(error)\n        # Remove potential bootstrap server details if they contain secrets, etc.\n        error_msg = re.sub(r\"bootstrap_servers='[^']+'\", \"bootstrap_servers='REDACTED'\", error_msg)\n        error_msg = re.sub(r\"password='[^']+'\", \"password='REDACTED'\", error_msg)\n        return error_msg\n\n    async def publish_vote_event(self, tenant_id: str, vote_event: Dict[str, Any]) -> bool:\n        \"\"\"\n        Publish a vote event to Kafka vote topic with guaranteed delivery.\n\n        Args:\n            tenant_id: Tenant identifier for topic isolation\n            vote_event: VoteEvent dictionary (must include election_id)\n\n        Returns:\n            True if published successfully, False otherwise\n        \"\"\"\n        if not self.producer or not self._running:\n            logger.error(\"Kafka producer not started, cannot publish vote event\")\n            return False\n\n        topic = self._get_vote_topic(tenant_id)\n        election_id = vote_event.get(\"election_id\", \"\")\n\n        # Use election_id as partition key to ensure all votes for one election\n        # go to the same partition (maintains ordering)\n        key = election_id.encode(\"utf-8\") if election_id else None\n\n        try:\n            await self.producer.send_and_wait(topic, value=vote_event, key=key)\n            logger.debug(f\"Vote event published to topic {topic} for election {election_id}\")\n            return True\n        except Exception as e:\n            logger.error(\n                f\"Failed to publish vote event to Kafka (topic={topic}): {self._sanitize_error(e)}\"\n            )\n            return False\n\n    async def publish_audit_record(self, tenant_id: str, audit_record: Dict[str, Any]) -> bool:\n        \"\"\"\n        Publish an audit record to Kafka audit topic (compacted).\n\n        Args:\n            tenant_id: Tenant identifier for topic isolation\n            audit_record: AuditRecord dictionary with signature\n\n        Returns:\n            True if published successfully, False otherwise\n        \"\"\"\n        if not self.producer or not self._running:\n            logger.error(\"Kafka producer not started, cannot publish audit record\")\n            return False\n\n        topic = self._get_audit_topic(tenant_id)\n        election_id = audit_record.get(\"election_id\", \"\")\n\n        # Use election_id as partition key for audit records too\n        key = election_id.encode(\"utf-8\") if election_id else None\n\n        try:\n            await self.producer.send_and_wait(topic, value=audit_record, key=key)\n            logger.debug(f\"Audit record published to topic {topic} for election {election_id}\")\n            return True\n        except Exception as e:\n            logger.error(\n                f\"Failed to publish audit record to Kafka (topic={topic}): {self._sanitize_error(e)}\"\n            )\n            return False\n\n\nclass Orchestrator:\n    \"\"\"\n    Base class for Orchestrator-Worker pattern.\n    \"\"\"\n\n    def __init__(self, bus: KafkaEventBus, tenant_id: str):\n        self.bus = bus\n        self.tenant_id = tenant_id\n\n    async def dispatch_task(self, task_data: Dict[str, Any], worker_type: str):\n        \"\"\"Dispatch a task to a specific worker type.\"\"\"\n        message = AgentMessage(\n            message_type=MessageType.TASK_REQUEST,\n            content=task_data,\n            tenant_id=self.tenant_id,\n            to_agent=f\"worker-{worker_type}\",\n        )\n        await self.bus.send_message(message)\n\n\nclass Blackboard:\n    \"\"\"\n    Implementation of the Blackboard pattern using Kafka Compacted Topics.\n    \"\"\"\n\n    def __init__(self, bus: KafkaEventBus, tenant_id: str, board_name: str):\n        self.bus = bus\n        self.tenant_id = tenant_id\n        self.topic = f\"acgs.blackboard.{tenant_id}.{board_name}\"\n        self.state: Dict[str, Any] = {}\n\n    async def update(self, key: str, value: Any):\n        \"\"\"Update a value on the blackboard.\"\"\"\n        message = AgentMessage(\n            message_type=MessageType.EVENT,\n            content={\"key\": key, \"value\": value},\n            tenant_id=self.tenant_id,\n            payload={\"blackboard_update\": True},\n        )\n        # In a real implementation, we'd use a specific Kafka key for compaction\n        await self.bus.send_message(message)\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), acgs2-core/breakthrough (79 occurrences), and acgs2-core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.295117",
  "last_updated": "2026-01-04T05:35:58.551429"
}