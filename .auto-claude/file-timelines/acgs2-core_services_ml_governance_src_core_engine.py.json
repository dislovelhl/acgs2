{
  "file_path": "src/core/services/ml_governance/src/core/engine.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"Constitutional Hash: cdd01ef066bc6cf2\nML Governance Engine - Random Forest scoring with online learning and feedback loops\n\"\"\"\n\nimport json\nimport pickle\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Tuple\n\nimport mlflow\nimport mlflow.sklearn\nimport numpy as np\nimport pandas as pd\nimport structlog\nfrom evidently import ColumnMapping\nfrom evidently.metrics import DataDriftTable\nfrom evidently.report import Report\nfrom river import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom .models import (\n    ABTest,\n    DriftDetectionResult,\n    FeatureVector,\n    FeedbackSubmission,\n    GovernanceDecision,\n    GovernanceRequest,\n    GovernanceResponse,\n    ModelStatus,\n    ModelType,\n    ModelVersion,\n)\n\nlogger = structlog.get_logger()\n\n\nclass MLGovernanceEngine:\n    \"\"\"Core ML engine for adaptive governance\"\"\"\n\n    def __init__(\n        self,\n        redis_client=None,\n        mlflow_tracking_uri: str = \"sqlite:///mlflow.db\",\n        model_dir: str = \"/tmp/ml_models\"\n    ):\n        \"\"\"\n        Initialize the ML governance engine\n\n        Args:\n            redis_client: Redis client for caching and persistence\n            mlflow_tracking_uri: MLflow tracking URI\n            model_dir: Directory to store trained models\n        \"\"\"\n        self.redis = redis_client\n        self.model_dir = Path(model_dir)\n        self.model_dir.mkdir(exist_ok=True)\n\n        # MLflow setup\n        mlflow.set_tracking_uri(mlflow_tracking_uri)\n\n        # Model storage\n        self.models: Dict[str, Any] = {}\n        self.online_learners: Dict[str, Any] = {}\n        self.active_versions: Dict[ModelType, str] = {}\n        self.ab_tests: Dict[str, ABTest] = {}\n\n        # Metrics tracking\n        self.metrics = {\n            \"predictions\": 0,\n            \"feedback_received\": 0,\n            \"drift_checks\": 0\n        }\n\n        # Initialize with baseline models\n        self._initialize_baseline_models()\n\n    def _initialize_baseline_models(self):\n        \"\"\"Initialize baseline models for cold start\"\"\"\n        logger.info(\"Initializing baseline ML models\")\n\n        # Create baseline Random Forest model\n        baseline_model = RandomForestClassifier(\n            n_estimators=100,\n            max_depth=10,\n            random_state=42,\n            n_jobs=-1\n        )\n\n        # Create baseline online learner (River)\n        online_learner = tree.HoeffdingTreeClassifier()\n\n        # Generate synthetic training data for initial model\n        synthetic_data = self._generate_synthetic_training_data(1000)\n        if synthetic_data:\n            X, y = synthetic_data\n            baseline_model.fit(X, y)\n\n            # Save baseline model\n            model_version = ModelVersion(\n                version_id=\"baseline-v1.0\",\n                model_type=ModelType.RANDOM_FOREST,\n                status=ModelStatus.ACTIVE,\n                training_samples=len(X),\n                created_at=datetime.now(timezone.utc)\n            )\n\n            self._save_model(\"baseline-v1.0\", baseline_model, model_version)\n            self.active_versions[ModelType.RANDOM_FOREST] = \"baseline-v1.0\"\n\n        # Initialize online learner\n        self.online_learners[\"online-v1.0\"] = online_learner\n        self.active_versions[ModelType.ONLINE_LEARNER] = \"online-v1.0\"\n\n    def _generate_synthetic_training_data(self, n_samples: int) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Generate synthetic training data for model initialization\"\"\"\n        try:\n            np.random.seed(42)\n\n            # Generate synthetic features\n            features = []\n            labels = []\n\n            for _ in range(n_samples):\n                # Create realistic feature vector\n                intent_confidence = np.random.beta(2, 2)  # Beta distribution for confidence\n                intent_class = np.random.choice([\"helpful\", \"harmful\", \"neutral\"])\n                content_length = np.random.poisson(200)  # Poisson for content length\n                time_of_day = np.random.randint(0, 24)\n                day_of_week = np.random.randint(0, 7)\n                risk_score = np.random.beta(1, 3)  # Low risk bias\n\n                # Determine label based on features\n                is_helpful = intent_class == \"helpful\" and intent_confidence > 0.7\n                is_harmful = intent_class == \"harmful\" or risk_score > 0.8\n                is_business_hours = 9 <= time_of_day <= 17 and day_of_week < 5\n\n                if is_harmful:\n                    decision = GovernanceDecision.DENY\n                elif is_helpful and is_business_hours and risk_score < 0.3:\n                    decision = GovernanceDecision.ALLOW\n                elif intent_confidence > 0.8:\n                    decision = GovernanceDecision.ALLOW\n                else:\n                    decision = GovernanceDecision.MONITOR\n\n                # Convert to feature vector\n                feature_vector = np.array([\n                    intent_confidence,\n                    1.0 if intent_class == \"helpful\" else 0.0,\n                    1.0 if intent_class == \"harmful\" else 0.0,\n                    content_length / 1000.0,\n                    np.random.random(),  # has_urls\n                    np.random.random(),  # has_email\n                    np.random.random(),  # has_code\n                    risk_score,  # toxicity_score\n                    0.5,  # user_history_score\n                    time_of_day / 24.0,\n                    day_of_week / 7.0,\n                    1.0 if is_business_hours else 0.0,\n                    np.random.randint(0, 5),  # policy_match_count\n                    np.random.randint(0, 2),  # policy_deny_count\n                    np.random.randint(2, 8),  # policy_allow_count\n                    risk_score,  # risk_level\n                    np.random.randint(0, 3),  # compliance_flags\n                    risk_score  # sensitivity_score\n                ])\n\n                features.append(feature_vector)\n                labels.append(decision.value)\n\n            X = np.array(features)\n            y = np.array(labels)\n\n            logger.info(f\"Generated {n_samples} synthetic training samples\")\n            return X, y\n\n        except Exception as e:\n            logger.error(\"Failed to generate synthetic training data\", error=str(e))\n            return None\n\n    async def predict(\n        self,\n        request: GovernanceRequest,\n        use_ab_test: bool = False\n    ) -> GovernanceResponse:\n        \"\"\"\n        Make governance prediction using ML models\n\n        Args:\n            request: Governance request\n            use_ab_test: Whether to use A/B testing\n\n        Returns:\n            Governance response with decision and confidence\n        \"\"\"\n        start_time = datetime.now(timezone.utc)\n\n        try:\n            # Extract features from request\n            features = self._extract_features(request)\n\n            # Determine which model to use\n            model_version, is_ab_test = self._select_model(use_ab_test)\n\n            # Make prediction\n            decision, confidence, reasoning = await self._make_prediction(\n                features, model_version, request\n            )\n\n            # Track metrics\n            self.metrics[\"predictions\"] += 1\n\n            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000\n\n            response = GovernanceResponse(\n                request_id=request.request_id,\n                decision=decision,\n                confidence=confidence,\n                reasoning=reasoning,\n                model_version=model_version,\n                features=features,\n                processing_time_ms=processing_time\n            )\n\n            # Log prediction for potential feedback\n            await self._log_prediction(request, response, is_ab_test)\n\n            logger.info(\n                \"Governance prediction made\",\n                request_id=request.request_id,\n                decision=decision.value,\n                confidence=confidence,\n                model_version=model_version,\n                processing_time_ms=processing_time\n            )\n\n            return response\n\n        except Exception as e:\n            logger.error(\n                \"Prediction failed\",\n                request_id=request.request_id,\n                error=str(e)\n            )\n            # Fallback to conservative decision\n            return GovernanceResponse(\n                request_id=request.request_id,\n                decision=GovernanceDecision.MONITOR,\n                confidence=0.5,\n                reasoning=\"Prediction failed, using conservative fallback\",\n                model_version=\"fallback\",\n                features=self._extract_features(request),\n                processing_time_ms=(datetime.now(timezone.utc) - start_time).total_seconds() * 1000\n            )\n\n    async def submit_feedback(self, feedback: FeedbackSubmission) -> bool:\n        \"\"\"\n        Submit user feedback for model improvement\n\n        Args:\n            feedback: User feedback submission\n\n        Returns:\n            True if feedback was processed successfully\n        \"\"\"\n        try:\n            # Store feedback for batch retraining\n            await self._store_feedback(feedback)\n\n            # Update online learners immediately\n            await self._update_online_learners(feedback)\n\n            # Track metrics\n            self.metrics[\"feedback_received\"] += 1\n\n            logger.info(\n                \"Feedback submitted\",\n                request_id=feedback.request_id,\n                feedback_type=feedback.feedback_type.value,\n                user_id=feedback.user_id\n            )\n\n            return True\n\n        except Exception as e:\n            logger.error(\n                \"Feedback submission failed\",\n                request_id=feedback.request_id,\n                error=str(e)\n            )\n            return False\n\n    async def check_drift(self, model_version: str) -> Optional[DriftDetectionResult]:\n        \"\"\"Check for model drift using recent data\"\"\"\n        try:\n            # Get recent predictions and feedback\n            recent_data = await self._get_recent_data(hours=24)\n\n            if not recent_data:\n                return None\n\n            # Use Evidently for drift detection\n            reference_data = recent_data.get(\"reference\", pd.DataFrame())\n            current_data = recent_data.get(\"current\", pd.DataFrame())\n\n            if reference_data.empty or current_data.empty:\n                return DriftDetectionResult(\n                    check_id=f\"drift-{datetime.now(timezone.utc).isoformat()}\",\n                    model_version=model_version,\n                    drift_detected=False,\n                    drift_score=0.0,\n                    threshold=0.1,\n                    details={\"reason\": \"Insufficient data for drift detection\"}\n                )\n\n            # Configure drift detection\n            column_mapping = ColumnMapping(\n                target=\"decision\",\n                prediction=\"prediction\",\n                numerical_features=[\"confidence\", \"feature_0\", \"feature_1\"],\n                categorical_features=[]\n            )\n\n            report = Report(metrics=[DataDriftTable()])\n            report.run(reference_data=reference_data, current_data=current_data, column_mapping=column_mapping)\n\n            # Extract drift score (simplified)\n            drift_score = 0.0  # Would extract from report\n\n            drift_detected = drift_score > 0.1  # Configurable threshold\n\n            result = DriftDetectionResult(\n                check_id=f\"drift-{datetime.now(timezone.utc).isoformat()}\",\n                model_version=model_version,\n                drift_detected=drift_detected,\n                drift_score=drift_score,\n                threshold=0.1,\n                details={\"report\": \"drift_analysis_report\"}  # Would include actual report\n            )\n\n            self.metrics[\"drift_checks\"] += 1\n\n            if drift_detected:\n                logger.warning(\n                    \"Model drift detected\",\n                    model_version=model_version,\n                    drift_score=drift_score\n                )\n\n            return result\n\n        except Exception as e:\n            logger.error(\"Drift detection failed\", error=str(e))\n            return None\n\n    def _extract_features(self, request: GovernanceRequest) -> FeatureVector:\n        \"\"\"Extract feature vector from governance request\"\"\"\n        # This would integrate with intent classification and content analysis\n        # For now, return a basic feature vector\n        content = request.content\n        context = request.context\n\n        return FeatureVector(\n            intent_confidence=context.get(\"intent_confidence\", 0.5),\n            intent_class=context.get(\"intent_class\", \"neutral\"),\n            intent_is_helpful=context.get(\"intent_class\") == \"helpful\",\n            intent_is_harmful=context.get(\"intent_class\") == \"harmful\",\n            content_length=len(content),\n            content_has_urls=\"http\" in content.lower(),\n            content_has_email=\"@\" in content,\n            content_has_code=\"```\" in content or \"def \" in content,\n            content_toxicity_score=context.get(\"toxicity_score\", 0.0),\n            user_history_score=context.get(\"user_history_score\", 0.5),\n            time_of_day=datetime.now().hour,\n            day_of_week=datetime.now().weekday(),\n            is_business_hours=9 <= datetime.now().hour <= 17,\n            policy_match_count=context.get(\"policy_matches\", 0),\n            policy_deny_count=context.get(\"policy_denies\", 0),\n            policy_allow_count=context.get(\"policy_allows\", 0),\n            risk_level=context.get(\"risk_level\", \"medium\"),\n            compliance_flags=context.get(\"compliance_flags\", []),\n            sensitivity_score=context.get(\"sensitivity_score\", 0.0)\n        )\n\n    def _select_model(self, use_ab_test: bool = False) -> Tuple[str, bool]:\n        \"\"\"Select which model version to use\"\"\"\n        if use_ab_test:\n            # Check if request should use A/B test\n            for ab_test in self.ab_tests.values():\n                if ab_test.status == \"active\":\n                    # Simple random selection based on traffic split\n                    if np.random.random() < ab_test.traffic_split:\n                        return ab_test.candidate_version, True\n                    else:\n                        return ab_test.champion_version, True\n\n        # Use active model\n        model_version = self.active_versions.get(ModelType.RANDOM_FOREST, \"baseline-v1.0\")\n        return model_version, False\n\n    async def _make_prediction(\n        self,\n        features: FeatureVector,\n        model_version: str,\n        request: GovernanceRequest\n    ) -> Tuple[GovernanceDecision, float, str]:\n        \"\"\"Make prediction using specified model version\"\"\"\n        try:\n            # Get model\n            model = self.models.get(model_version)\n            if not model:\n                logger.warning(f\"Model {model_version} not found, using baseline\")\n                model = self.models.get(\"baseline-v1.0\")\n\n            if not model:\n                # Fallback decision\n                return GovernanceDecision.MONITOR, 0.5, \"Using conservative fallback due to model unavailability\"\n\n            # Convert features to array\n            X = features.to_numpy_array().reshape(1, -1)\n\n            # Make prediction\n            prediction_proba = model.predict_proba(X)[0]\n            prediction = model.predict(X)[0]\n\n            # Convert to GovernanceDecision\n            decision = GovernanceDecision(prediction)\n\n            # Get confidence (max probability)\n            confidence = float(np.max(prediction_proba))\n\n            # Generate reasoning\n            reasoning = self._generate_reasoning(features, decision, confidence)\n\n            return decision, confidence, reasoning\n\n        except Exception as e:\n            logger.error(f\"Prediction failed for model {model_version}\", error=str(e))\n            return GovernanceDecision.MONITOR, 0.5, f\"Prediction error: {str(e)}\"\n\n    def _generate_reasoning(\n        self,\n        features: FeatureVector,\n        decision: GovernanceDecision,\n        confidence: float\n    ) -> str:\n        \"\"\"Generate human-readable reasoning for the decision\"\"\"\n        reasons = []\n\n        if features.intent_is_harmful:\n            reasons.append(\"Content classified as potentially harmful\")\n        elif features.intent_is_helpful and features.intent_confidence > 0.8:\n            reasons.append(\"High-confidence helpful intent detected\")\n\n        if features.content_toxicity_score > 0.7:\n            reasons.append(\"High toxicity score detected\")\n\n        if not features.is_business_hours:\n            reasons.append(\"Request made outside business hours\")\n\n        if features.risk_level == \"high\":\n            reasons.append(\"High risk level assessment\")\n\n        if not reasons:\n            reasons.append(\"Decision based on ML model analysis\")\n\n        reasoning = f\"{decision.value.title()} decision with {confidence:.1%} confidence. \"\n        reasoning += \"Reasons: \" + \"; \".join(reasons)\n\n        return reasoning\n\n    async def _store_feedback(self, feedback: FeedbackSubmission):\n        \"\"\"Store feedback for batch retraining\"\"\"\n        # Store in Redis for immediate access\n        if self.redis:\n            feedback_key = f\"feedback:{feedback.request_id}\"\n            await self.redis.set(\n                feedback_key,\n                feedback.json(),\n                ex=86400 * 30  # 30 days\n            )\n\n        # Would also store in database for long-term analysis\n\n    async def _update_online_learners(self, feedback: FeedbackSubmission):\n        \"\"\"Update online learners with new feedback\"\"\"\n        try:\n            # Get the original prediction\n            prediction_data = await self._get_prediction(feedback.request_id)\n            if not prediction_data:\n                return\n\n            # Extract features and correct label\n            features = prediction_data[\"features\"]\n            correct_decision = feedback.correct_decision or prediction_data[\"decision\"]\n\n            # Update online learners\n            for learner_name, learner in self.online_learners.items():\n                if hasattr(learner, 'learn_one'):\n                    # River-style online learning\n                    learner.learn_one(features, correct_decision.value)\n\n            logger.info(\"Updated online learners with feedback\", request_id=feedback.request_id)\n\n        except Exception as e:\n            logger.error(\"Failed to update online learners\", error=str(e))\n\n    async def _get_prediction(self, request_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get stored prediction data\"\"\"\n        if self.redis:\n            prediction_key = f\"prediction:{request_id}\"\n            prediction_data = await self.redis.get(prediction_key)\n            if prediction_data:\n                return json.loads(prediction_data)\n        return None\n\n    async def _log_prediction(\n        self,\n        request: GovernanceRequest,\n        response: GovernanceResponse,\n        is_ab_test: bool\n    ):\n        \"\"\"Log prediction for feedback and analysis\"\"\"\n        prediction_data = {\n            \"request_id\": request.request_id,\n            \"features\": response.features.dict(),\n            \"decision\": response.decision.value,\n            \"confidence\": response.confidence,\n            \"model_version\": response.model_version,\n            \"ab_test\": is_ab_test,\n            \"timestamp\": response.timestamp.isoformat()\n        }\n\n        if self.redis:\n            prediction_key = f\"prediction:{request.request_id}\"\n            await self.redis.set(\n                prediction_key,\n                json.dumps(prediction_data),\n                ex=86400 * 7  # 7 days\n            )\n\n    def _save_model(self, version_id: str, model: Any, metadata: ModelVersion):\n        \"\"\"Save trained model\"\"\"\n        try:\n            model_path = self.model_dir / f\"{version_id}.pkl\"\n            with open(model_path, 'wb') as f:\n                pickle.dump(model, f)\n\n            self.models[version_id] = model\n\n            # Log to MLflow\n            with mlflow.start_run(run_name=f\"model_{version_id}\"):\n                mlflow.sklearn.log_model(model, \"model\")\n                mlflow.log_param(\"version_id\", version_id)\n                mlflow.log_param(\"model_type\", metadata.model_type.value)\n                mlflow.log_metric(\"accuracy\", metadata.accuracy)\n                mlflow.log_metric(\"training_samples\", metadata.training_samples)\n\n            logger.info(\"Model saved\", version_id=version_id, path=str(model_path))\n\n        except Exception as e:\n            logger.error(\"Failed to save model\", version_id=version_id, error=str(e))\n\n    async def _get_recent_data(self, hours: int) -> Optional[Dict[str, pd.DataFrame]]:\n        \"\"\"Get recent prediction data for drift detection\"\"\"\n        # This would query recent predictions from database/cache\n        # For now, return None (drift detection disabled)\n        return None\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.245563",
  "last_updated": "2026-01-04T05:35:58.518556"
}