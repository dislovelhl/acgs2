{
  "file_path": "src/core/shared/l1_cache.py",
  "main_branch_history": [],
  "task_views": {
    "056-reduce-excessive-any-type-usage-in-python-codebase": {
      "task_id": "056-reduce-excessive-any-type-usage-in-python-codebase",
      "branch_point": {
        "commit_hash": "fc6e42927298263034acf989773f3200e422ad17",
        "content": "\"\"\"\nACGS-2 L1 In-Process Cache Module\nConstitutional Hash: cdd01ef066bc6cf2\n\nProvides thread-safe in-memory caching using cachetools.TTLCache for ultra-hot data\nwith sub-millisecond latency (<0.1ms target).\n\nUsage:\n    from shared.l1_cache import L1Cache, get_l1_cache\n\n    # Direct usage\n    cache = L1Cache(maxsize=1024, ttl=600)\n    cache.set('key', 'value')\n    value = cache.get('key')\n\n    # Singleton pattern\n    cache = get_l1_cache()\n\nExample:\n    cache = L1Cache(maxsize=100, ttl=300)\n    cache.set('user:123', {'name': 'Alice'})\n    user = cache.get('user:123')  # Returns {'name': 'Alice'}\n\"\"\"\n\nimport json\nimport logging\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, List, Optional, TypeVar\n\ntry:\n    from cachetools import TTLCache\nexcept ImportError:\n    TTLCache = None\n\n# Constitutional Hash for governance validation\nCONSTITUTIONAL_HASH = \"cdd01ef066bc6cf2\"\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass L1CacheConfig:\n    \"\"\"Configuration for L1 in-process cache.\"\"\"\n\n    maxsize: int = 1024  # Maximum number of items in cache\n    ttl: int = 600  # Time-to-live in seconds (default: 10 minutes)\n    serialize: bool = False  # Whether to JSON serialize values for type consistency\n\n\n@dataclass\nclass L1CacheStats:\n    \"\"\"Statistics for L1 cache operations.\"\"\"\n\n    hits: int = 0\n    misses: int = 0\n    sets: int = 0\n    deletes: int = 0\n    evictions: int = 0\n\n    @property\n    def hit_ratio(self) -> float:\n        \"\"\"Calculate hit ratio.\"\"\"\n        total = self.hits + self.misses\n        return self.hits / total if total > 0 else 0.0\n\n\nclass L1Cache:\n    \"\"\"\n    Thread-safe in-process cache using cachetools.TTLCache.\n\n    Provides ultra-fast local caching with configurable TTL and size limits.\n    Thread safety is ensured via threading.Lock for all cache operations.\n\n    Attributes:\n        maxsize: Maximum number of items the cache can hold\n        ttl: Time-to-live for cache entries in seconds\n    \"\"\"\n\n    def __init__(\n        self,\n        maxsize: int = 1024,\n        ttl: int = 600,\n        serialize: bool = False,\n        on_evict: Optional[Callable[[str, Any], None]] = None,\n    ):\n        \"\"\"\n        Initialize L1 cache.\n\n        Args:\n            maxsize: Maximum number of items in cache (default: 1024)\n            ttl: Time-to-live in seconds (default: 600 = 10 minutes)\n            serialize: Whether to JSON serialize values for type consistency\n            on_evict: Optional callback when items are evicted\n        \"\"\"\n        if TTLCache is None:\n            raise ImportError(\n                \"cachetools is required for L1Cache. Install with: pip install cachetools\"\n            )\n\n        self.maxsize = maxsize\n        self.ttl = ttl\n        self.serialize = serialize\n        self.on_evict = on_evict\n\n        # Core cache with TTL support\n        self._cache: TTLCache = TTLCache(maxsize=maxsize, ttl=ttl)\n\n        # Thread safety lock - CRITICAL for concurrent FastAPI access\n        self._lock = threading.Lock()\n\n        # Statistics tracking\n        self._stats = L1CacheStats()\n\n        # Access tracking for promotion decisions (frequency per key)\n        self._access_counts: Dict[str, int] = {}\n        self._access_window_start: float = time.time()\n        self._access_window_seconds: int = 60  # Track access per minute\n\n        logger.debug(f\"[{CONSTITUTIONAL_HASH}] L1Cache initialized: maxsize={maxsize}, ttl={ttl}s\")\n\n    def get(self, key: str, default: Optional[T] = None) -> Optional[T]:\n        \"\"\"\n        Get a value from the cache.\n\n        Args:\n            key: Cache key\n            default: Default value if key not found\n\n        Returns:\n            Cached value or default\n        \"\"\"\n        with self._lock:\n            self._track_access(key)\n            try:\n                value = self._cache[key]\n                self._stats.hits += 1\n\n                # Deserialize if needed\n                if self.serialize and value is not None:\n                    value = json.loads(value)\n\n                return value\n            except KeyError:\n                self._stats.misses += 1\n                return default\n\n    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n        \"\"\"\n        Set a value in the cache.\n\n        Args:\n            key: Cache key\n            value: Value to cache\n            ttl: Optional per-item TTL override (not supported by TTLCache, ignored)\n        \"\"\"\n        with self._lock:\n            # Serialize if configured\n            if self.serialize:\n                value = json.dumps(value)\n\n            # Track evictions\n            old_size = len(self._cache)\n            self._cache[key] = value\n            new_size = len(self._cache)\n\n            # If size didn't increase but we added a new key, eviction occurred\n            if new_size <= old_size and key not in self._cache:\n                self._stats.evictions += 1\n\n            self._stats.sets += 1\n            self._track_access(key)\n\n    def delete(self, key: str) -> bool:\n        \"\"\"\n        Delete a key from the cache.\n\n        Args:\n            key: Cache key to delete\n\n        Returns:\n            True if key was deleted, False if not found\n        \"\"\"\n        with self._lock:\n            try:\n                # Call eviction callback if set\n                if self.on_evict and key in self._cache:\n                    self.on_evict(key, self._cache[key])\n\n                del self._cache[key]\n                self._stats.deletes += 1\n                return True\n            except KeyError:\n                return False\n\n    def exists(self, key: str) -> bool:\n        \"\"\"\n        Check if a key exists in the cache.\n\n        Args:\n            key: Cache key\n\n        Returns:\n            True if key exists and not expired\n        \"\"\"\n        with self._lock:\n            return key in self._cache\n\n    def clear(self) -> None:\n        \"\"\"Clear all items from the cache.\"\"\"\n        with self._lock:\n            self._cache.clear()\n            self._access_counts.clear()\n            logger.debug(f\"[{CONSTITUTIONAL_HASH}] L1Cache cleared\")\n\n    def get_many(self, keys: List[str]) -> Dict[str, Any]:\n        \"\"\"\n        Get multiple values from the cache.\n\n        Args:\n            keys: List of cache keys\n\n        Returns:\n            Dictionary of found key-value pairs\n        \"\"\"\n        result = {}\n        with self._lock:\n            for key in keys:\n                self._track_access(key)\n                try:\n                    value = self._cache[key]\n                    self._stats.hits += 1\n\n                    if self.serialize and value is not None:\n                        value = json.loads(value)\n\n                    result[key] = value\n                except KeyError:\n                    self._stats.misses += 1\n\n        return result\n\n    def set_many(self, items: Dict[str, Any]) -> None:\n        \"\"\"\n        Set multiple values in the cache.\n\n        Args:\n            items: Dictionary of key-value pairs to cache\n        \"\"\"\n        with self._lock:\n            for key, value in items.items():\n                if self.serialize:\n                    value = json.dumps(value)\n\n                self._cache[key] = value\n                self._stats.sets += 1\n                self._track_access(key)\n\n    def _track_access(self, key: str) -> None:\n        \"\"\"\n        Track access frequency for promotion decisions.\n        Called within lock context.\n\n        Args:\n            key: Cache key being accessed\n        \"\"\"\n        current_time = time.time()\n\n        # Reset window if expired\n        if current_time - self._access_window_start >= self._access_window_seconds:\n            self._access_counts.clear()\n            self._access_window_start = current_time\n\n        # Increment access count\n        self._access_counts[key] = self._access_counts.get(key, 0) + 1\n\n    def get_access_frequency(self, key: str) -> int:\n        \"\"\"\n        Get access frequency for a key in the current window.\n\n        Args:\n            key: Cache key\n\n        Returns:\n            Number of accesses in current minute window\n        \"\"\"\n        with self._lock:\n            current_time = time.time()\n\n            # Check if window expired\n            if current_time - self._access_window_start >= self._access_window_seconds:\n                return 0\n\n            return self._access_counts.get(key, 0)\n\n    def get_hot_keys(self, threshold: int = 10) -> List[str]:\n        \"\"\"\n        Get keys with access frequency above threshold.\n\n        Args:\n            threshold: Minimum access count to be considered \"hot\"\n\n        Returns:\n            List of hot keys\n        \"\"\"\n        with self._lock:\n            current_time = time.time()\n\n            # Check if window expired\n            if current_time - self._access_window_start >= self._access_window_seconds:\n                return []\n\n            return [key for key, count in self._access_counts.items() if count >= threshold]\n\n    @property\n    def stats(self) -> L1CacheStats:\n        \"\"\"Get cache statistics.\"\"\"\n        return self._stats\n\n    @property\n    def size(self) -> int:\n        \"\"\"Get current number of items in cache.\"\"\"\n        with self._lock:\n            return len(self._cache)\n\n    @property\n    def currsize(self) -> int:\n        \"\"\"Alias for size property (cachetools compatibility).\"\"\"\n        return self.size\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get comprehensive cache statistics.\n\n        Returns:\n            Dictionary with cache stats\n        \"\"\"\n        with self._lock:\n            return {\n                \"tier\": \"L1\",\n                \"constitutional_hash\": CONSTITUTIONAL_HASH,\n                \"maxsize\": self.maxsize,\n                \"ttl\": self.ttl,\n                \"current_size\": len(self._cache),\n                \"hits\": self._stats.hits,\n                \"misses\": self._stats.misses,\n                \"sets\": self._stats.sets,\n                \"deletes\": self._stats.deletes,\n                \"evictions\": self._stats.evictions,\n                \"hit_ratio\": self._stats.hit_ratio,\n                \"hot_keys_count\": len([k for k, v in self._access_counts.items() if v >= 10]),\n            }\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"Support 'in' operator.\"\"\"\n        return self.exists(key)\n\n    def __len__(self) -> int:\n        \"\"\"Support len() function.\"\"\"\n        return self.size\n\n    def __repr__(self) -> str:\n        \"\"\"String representation.\"\"\"\n        return (\n            f\"L1Cache(maxsize={self.maxsize}, ttl={self.ttl}, \"\n            f\"size={self.size}, hit_ratio={self._stats.hit_ratio:.2%})\"\n        )\n\n\n# Singleton instance for shared usage\n_default_cache: Optional[L1Cache] = None\n_singleton_lock = threading.Lock()\n\n\ndef get_l1_cache(\n    maxsize: int = 1024,\n    ttl: int = 600,\n    serialize: bool = False,\n) -> L1Cache:\n    \"\"\"\n    Get or create the singleton L1 cache instance.\n\n    Thread-safe singleton pattern for shared cache access across the application.\n\n    Args:\n        maxsize: Maximum number of items (used only on first call)\n        ttl: Time-to-live in seconds (used only on first call)\n        serialize: Whether to JSON serialize values (used only on first call)\n\n    Returns:\n        L1Cache singleton instance\n    \"\"\"\n    global _default_cache\n\n    if _default_cache is None:\n        with _singleton_lock:\n            if _default_cache is None:\n                _default_cache = L1Cache(maxsize=maxsize, ttl=ttl, serialize=serialize)\n                logger.info(\n                    f\"[{CONSTITUTIONAL_HASH}] L1Cache singleton created: \"\n                    f\"maxsize={maxsize}, ttl={ttl}\"\n                )\n\n    return _default_cache\n\n\ndef reset_l1_cache() -> None:\n    \"\"\"\n    Reset the singleton L1 cache instance.\n\n    Useful for testing or when cache configuration needs to change.\n    \"\"\"\n    global _default_cache\n\n    with _singleton_lock:\n        if _default_cache is not None:\n            _default_cache.clear()\n            _default_cache = None\n            logger.info(f\"[{CONSTITUTIONAL_HASH}] L1Cache singleton reset\")\n\n\n__all__ = [\n    # Constants\n    \"CONSTITUTIONAL_HASH\",\n    # Classes\n    \"L1Cache\",\n    \"L1CacheConfig\",\n    \"L1CacheStats\",\n    # Functions\n    \"get_l1_cache\",\n    \"reset_l1_cache\",\n]\n",
        "timestamp": "2026-01-04T05:35:58.374613"
      },
      "worktree_state": null,
      "task_intent": {
        "title": "056-reduce-excessive-any-type-usage-in-python-codebase",
        "description": "Found 373 occurrences of ': Any' type annotations across 120+ Python files, with high concentrations in integration-service (16 occurrences), src/core/breakthrough (79 occurrences), and src/core/enhanced_agent_bus (50+ occurrences). Excessive use of 'Any' defeats the purpose of type hints and can mask type-related bugs.",
        "from_plan": true
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-03T19:36:18.262248",
  "last_updated": "2026-01-04T05:35:58.954156"
}