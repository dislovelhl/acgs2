"""
ACGS-2 A/B Testing Framework Tests
Constitutional Hash: cdd01ef066bc6cf2

Unit tests for A/B testing framework with traffic routing, metrics tracking,
and model promotion validation. Tests verify traffic split within ±2% variance.
"""

import logging
import os
import sys
from datetime import datetime, timedelta, timezone
from unittest.mock import MagicMock, Mock, patch

import pytest

# Add parent directory to path for module imports
enhanced_agent_bus_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if enhanced_agent_bus_dir not in sys.path:
    sys.path.insert(0, enhanced_agent_bus_dir)

from ab_testing import (
    AB_TEST_CONFIDENCE_LEVEL,
    AB_TEST_MIN_IMPROVEMENT,
    AB_TEST_MIN_SAMPLES,
    AB_TEST_SPLIT,
    CANDIDATE_ALIAS,
    CHAMPION_ALIAS,
    MODEL_REGISTRY_NAME,
    NUMPY_AVAILABLE,
    ABTestRouter,
    CohortMetrics,
    CohortType,
    ComparisonResult,
    MetricsComparison,
    PredictionResult,
    PromotionResult,
    PromotionStatus,
    RoutingResult,
    compare_models,
    get_ab_test_metrics,
    get_ab_test_router,
    promote_candidate_model,
    route_and_predict,
    route_request,
)

logger = logging.getLogger(__name__)


class TestCohortType:
    """Tests for CohortType enum."""

    def test_cohort_type_values(self):
        """Test all CohortType enum values."""
        assert CohortType.CHAMPION.value == "champion"
        assert CohortType.CANDIDATE.value == "candidate"

    def test_cohort_type_is_string_enum(self):
        """Test that CohortType inherits from str."""
        assert isinstance(CohortType.CHAMPION, str)
        assert CohortType.CHAMPION == "champion"
        assert CohortType.CANDIDATE == "candidate"

    def test_cohort_type_count(self):
        """Test all expected cohort types exist."""
        cohort_values = [c.value for c in CohortType]
        assert len(cohort_values) == 2
        assert "champion" in cohort_values
        assert "candidate" in cohort_values


class TestPromotionStatus:
    """Tests for PromotionStatus enum."""

    def test_promotion_status_values(self):
        """Test all PromotionStatus enum values."""
        assert PromotionStatus.READY.value == "ready"
        assert PromotionStatus.NOT_READY.value == "not_ready"
        assert PromotionStatus.BLOCKED.value == "blocked"
        assert PromotionStatus.PROMOTED.value == "promoted"
        assert PromotionStatus.ERROR.value == "error"

    def test_promotion_status_is_string_enum(self):
        """Test that PromotionStatus inherits from str."""
        assert isinstance(PromotionStatus.READY, str)
        assert PromotionStatus.PROMOTED == "promoted"

    def test_promotion_status_count(self):
        """Test all expected promotion status values exist."""
        status_values = [s.value for s in PromotionStatus]
        assert len(status_values) == 5
        assert "ready" in status_values
        assert "error" in status_values


class TestComparisonResult:
    """Tests for ComparisonResult enum."""

    def test_comparison_result_values(self):
        """Test all ComparisonResult enum values."""
        assert ComparisonResult.CANDIDATE_BETTER.value == "candidate_better"
        assert ComparisonResult.CHAMPION_BETTER.value == "champion_better"
        assert ComparisonResult.NO_DIFFERENCE.value == "no_difference"
        assert ComparisonResult.INSUFFICIENT_DATA.value == "insufficient_data"

    def test_comparison_result_is_string_enum(self):
        """Test that ComparisonResult inherits from str."""
        assert isinstance(ComparisonResult.CANDIDATE_BETTER, str)
        assert ComparisonResult.NO_DIFFERENCE == "no_difference"

    def test_comparison_result_count(self):
        """Test all expected comparison results exist."""
        result_values = [r.value for r in ComparisonResult]
        assert len(result_values) == 4
        assert "candidate_better" in result_values
        assert "insufficient_data" in result_values


class TestCohortMetrics:
    """Tests for CohortMetrics dataclass."""

    def test_cohort_metrics_defaults(self):
        """Test CohortMetrics with default values."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)

        assert metrics.cohort == CohortType.CHAMPION
        assert metrics.request_count == 0
        assert metrics.correct_predictions == 0
        assert metrics.total_predictions == 0
        assert metrics.accuracy == 0.0
        assert metrics.total_latency_ms == 0.0
        assert metrics.min_latency_ms == float("inf")
        assert metrics.max_latency_ms == 0.0
        assert metrics.p50_latency_ms == 0.0
        assert metrics.p95_latency_ms == 0.0
        assert metrics.p99_latency_ms == 0.0
        assert metrics.errors == 0
        assert metrics.first_request_at is None
        assert metrics.last_request_at is None
        assert metrics.latencies == []

    def test_cohort_metrics_record_request(self):
        """Test recording a request updates metrics correctly."""
        metrics = CohortMetrics(cohort=CohortType.CANDIDATE)

        metrics.record_request(latency_ms=50.0, prediction=1, actual=1)

        assert metrics.request_count == 1
        assert metrics.total_latency_ms == 50.0
        assert metrics.min_latency_ms == 50.0
        assert metrics.max_latency_ms == 50.0
        assert metrics.correct_predictions == 1
        assert metrics.total_predictions == 1
        assert metrics.accuracy == 1.0
        assert metrics.first_request_at is not None
        assert metrics.last_request_at is not None
        assert len(metrics.latencies) == 1

    def test_cohort_metrics_record_multiple_requests(self):
        """Test recording multiple requests."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)

        metrics.record_request(latency_ms=10.0, prediction=1, actual=1)
        metrics.record_request(latency_ms=20.0, prediction=0, actual=1)
        metrics.record_request(latency_ms=30.0, prediction=1, actual=1)

        assert metrics.request_count == 3
        assert metrics.total_latency_ms == 60.0
        assert metrics.min_latency_ms == 10.0
        assert metrics.max_latency_ms == 30.0
        assert metrics.correct_predictions == 2
        assert metrics.total_predictions == 3
        assert metrics.accuracy == pytest.approx(2 / 3, rel=1e-6)

    def test_cohort_metrics_record_error(self):
        """Test recording an error request."""
        metrics = CohortMetrics(cohort=CohortType.CANDIDATE)

        metrics.record_request(latency_ms=100.0, is_error=True)

        assert metrics.request_count == 1
        assert metrics.errors == 1
        assert metrics.total_predictions == 0  # Errors don't count as predictions
        assert metrics.total_latency_ms == 100.0

    def test_cohort_metrics_avg_latency(self):
        """Test average latency calculation."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)

        metrics.record_request(latency_ms=10.0)
        metrics.record_request(latency_ms=20.0)
        metrics.record_request(latency_ms=30.0)

        assert metrics.avg_latency_ms == pytest.approx(20.0, rel=1e-6)

    def test_cohort_metrics_avg_latency_zero_requests(self):
        """Test average latency with no requests."""
        metrics = CohortMetrics(cohort=CohortType.CANDIDATE)

        assert metrics.avg_latency_ms == 0.0

    def test_cohort_metrics_calculate_percentiles(self):
        """Test latency percentile calculation."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)

        # Record 100 requests with increasing latency
        for i in range(100):
            metrics.record_request(latency_ms=float(i + 1))

        metrics.calculate_percentiles()

        assert metrics.p50_latency_ms == pytest.approx(50.0, rel=0.1)
        assert metrics.p95_latency_ms == pytest.approx(95.0, rel=0.1)
        assert metrics.p99_latency_ms == pytest.approx(99.0, rel=0.1)

    def test_cohort_metrics_calculate_percentiles_empty(self):
        """Test percentile calculation with no latencies."""
        metrics = CohortMetrics(cohort=CohortType.CANDIDATE)

        metrics.calculate_percentiles()

        assert metrics.p50_latency_ms == 0.0
        assert metrics.p95_latency_ms == 0.0
        assert metrics.p99_latency_ms == 0.0

    def test_cohort_metrics_to_dict(self):
        """Test serialization to dictionary."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)
        metrics.record_request(latency_ms=50.0, prediction=1, actual=1)
        metrics.record_request(latency_ms=75.0, prediction=0, actual=0)

        result = metrics.to_dict()

        assert result["cohort"] == "champion"
        assert result["request_count"] == 2
        assert result["accuracy"] == 1.0
        assert result["avg_latency_ms"] == pytest.approx(62.5, rel=1e-6)
        assert result["min_latency_ms"] == 50.0
        assert result["max_latency_ms"] == 75.0
        assert "first_request_at" in result
        assert "last_request_at" in result
        assert "error_rate" in result


class TestRoutingResult:
    """Tests for RoutingResult dataclass."""

    def test_routing_result_creation(self):
        """Test creating RoutingResult."""
        result = RoutingResult(
            cohort=CohortType.CHAMPION,
            request_id="req-123",
            model_version=5,
        )

        assert result.cohort == CohortType.CHAMPION
        assert result.request_id == "req-123"
        assert result.model_version == 5
        assert result.routed_at is not None

    def test_routing_result_default_timestamp(self):
        """Test RoutingResult has default timestamp."""
        result = RoutingResult(
            cohort=CohortType.CANDIDATE,
            request_id="req-456",
        )

        assert isinstance(result.routed_at, datetime)
        assert result.model_version is None


class TestPredictionResult:
    """Tests for PredictionResult dataclass."""

    def test_prediction_result_minimal(self):
        """Test PredictionResult with minimal fields."""
        result = PredictionResult(
            prediction=1,
            cohort=CohortType.CHAMPION,
            request_id="req-123",
            latency_ms=50.0,
        )

        assert result.prediction == 1
        assert result.cohort == CohortType.CHAMPION
        assert result.request_id == "req-123"
        assert result.latency_ms == 50.0
        assert result.model_version is None
        assert result.confidence is None
        assert result.probabilities is None
        assert result.error is None

    def test_prediction_result_full(self):
        """Test PredictionResult with all fields."""
        result = PredictionResult(
            prediction=0,
            cohort=CohortType.CANDIDATE,
            request_id="req-789",
            latency_ms=75.0,
            model_version=3,
            confidence=0.95,
            probabilities={0: 0.95, 1: 0.05},
            error=None,
        )

        assert result.prediction == 0
        assert result.model_version == 3
        assert result.confidence == 0.95
        assert result.probabilities[0] == 0.95

    def test_prediction_result_with_error(self):
        """Test PredictionResult with error."""
        result = PredictionResult(
            prediction=None,
            cohort=CohortType.CHAMPION,
            request_id="req-err",
            latency_ms=100.0,
            error="Model not loaded",
        )

        assert result.prediction is None
        assert result.error == "Model not loaded"


class TestMetricsComparison:
    """Tests for MetricsComparison dataclass."""

    @pytest.fixture
    def champion_metrics(self):
        """Create champion cohort metrics."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)
        for i in range(100):
            metrics.record_request(
                latency_ms=50.0 + i * 0.1,
                prediction=i % 2,
                actual=i % 2,  # 100% accuracy
            )
        return metrics

    @pytest.fixture
    def candidate_metrics(self):
        """Create candidate cohort metrics."""
        metrics = CohortMetrics(cohort=CohortType.CANDIDATE)
        for i in range(50):
            metrics.record_request(
                latency_ms=40.0 + i * 0.1,
                prediction=i % 2,
                actual=i % 2,  # 100% accuracy
            )
        return metrics

    def test_metrics_comparison_creation(self, champion_metrics, candidate_metrics):
        """Test MetricsComparison creation."""
        comparison = MetricsComparison(
            champion_metrics=champion_metrics,
            candidate_metrics=candidate_metrics,
            result=ComparisonResult.NO_DIFFERENCE,
            accuracy_delta=0.0,
            latency_delta_ms=-10.0,
            sample_size_champion=100,
            sample_size_candidate=50,
            is_significant=False,
            candidate_is_better=False,
            recommendation="Continue testing",
        )

        assert comparison.result == ComparisonResult.NO_DIFFERENCE
        assert comparison.accuracy_delta == 0.0
        assert comparison.latency_delta_ms == -10.0
        assert comparison.sample_size_champion == 100
        assert comparison.sample_size_candidate == 50
        assert comparison.is_significant is False
        assert comparison.candidate_is_better is False

    def test_metrics_comparison_to_dict(self, champion_metrics, candidate_metrics):
        """Test MetricsComparison serialization."""
        comparison = MetricsComparison(
            champion_metrics=champion_metrics,
            candidate_metrics=candidate_metrics,
            result=ComparisonResult.CANDIDATE_BETTER,
            accuracy_delta=0.05,
            latency_delta_ms=-5.0,
            sample_size_champion=100,
            sample_size_candidate=50,
            is_significant=True,
            candidate_is_better=True,
            recommendation="Promote candidate",
        )

        result = comparison.to_dict()

        assert result["result"] == "candidate_better"
        assert result["accuracy_delta"] == 0.05
        assert result["latency_delta_ms"] == -5.0
        assert result["is_significant"] is True
        assert result["candidate_is_better"] is True
        assert "champion_metrics" in result
        assert "candidate_metrics" in result
        assert "compared_at" in result


class TestPromotionResult:
    """Tests for PromotionResult dataclass."""

    def test_promotion_result_success(self):
        """Test PromotionResult for successful promotion."""
        result = PromotionResult(
            status=PromotionStatus.PROMOTED,
            previous_champion_version=2,
            new_champion_version=3,
            promoted_at=datetime.now(timezone.utc),
        )

        assert result.status == PromotionStatus.PROMOTED
        assert result.previous_champion_version == 2
        assert result.new_champion_version == 3
        assert result.promoted_at is not None
        assert result.error_message is None

    def test_promotion_result_blocked(self):
        """Test PromotionResult for blocked promotion."""
        result = PromotionResult(
            status=PromotionStatus.BLOCKED,
            error_message="Candidate performance is worse",
        )

        assert result.status == PromotionStatus.BLOCKED
        assert result.error_message is not None
        assert result.new_champion_version is None

    def test_promotion_result_not_ready(self):
        """Test PromotionResult for not ready state."""
        result = PromotionResult(
            status=PromotionStatus.NOT_READY,
            error_message="Insufficient samples",
        )

        assert result.status == PromotionStatus.NOT_READY


class TestABTestRouter:
    """Tests for ABTestRouter class."""

    @pytest.fixture
    def router(self):
        """Create an ABTestRouter instance for testing."""
        return ABTestRouter(
            candidate_split=0.1,
            min_samples=100,
            confidence_level=0.95,
            min_improvement=0.01,
        )

    @pytest.fixture
    def mock_sklearn_model(self):
        """Create a mock sklearn model."""
        model = MagicMock()
        model.predict = MagicMock(return_value=[1])
        model.predict_proba = MagicMock(return_value=[[0.2, 0.8]])
        model.classes_ = [0, 1]
        return model

    def test_router_initialization(self):
        """Test ABTestRouter initialization with default values."""
        router = ABTestRouter()

        assert router.candidate_split == AB_TEST_SPLIT
        assert router.champion_alias == CHAMPION_ALIAS
        assert router.candidate_alias == CANDIDATE_ALIAS
        assert router.min_samples == AB_TEST_MIN_SAMPLES
        assert router.confidence_level == AB_TEST_CONFIDENCE_LEVEL
        assert router.min_improvement == AB_TEST_MIN_IMPROVEMENT

    def test_router_initialization_custom(self, router):
        """Test ABTestRouter initialization with custom values."""
        assert router.candidate_split == 0.1
        assert router.min_samples == 100
        assert router.confidence_level == 0.95
        assert router.min_improvement == 0.01

    def test_router_invalid_split_raises(self):
        """Test that invalid candidate_split raises ValueError."""
        with pytest.raises(ValueError) as exc_info:
            ABTestRouter(candidate_split=1.5)
        assert "candidate_split must be between 0 and 1" in str(exc_info.value)

        with pytest.raises(ValueError):
            ABTestRouter(candidate_split=-0.1)

    def test_router_route_deterministic(self, router):
        """Test that routing is deterministic for same request_id."""
        result1 = router.route("test-request-123")
        result2 = router.route("test-request-123")

        assert result1.cohort == result2.cohort
        assert result1.request_id == result2.request_id

    def test_router_route_returns_valid_cohort(self, router):
        """Test that routing returns valid cohort types."""
        result = router.route("test-request")

        assert result.cohort in [CohortType.CHAMPION, CohortType.CANDIDATE]
        assert result.request_id == "test-request"
        assert isinstance(result, RoutingResult)

    def test_router_compute_hash_value_range(self, router):
        """Test hash value is in valid range [0, 1)."""
        for i in range(100):
            hash_value = router._compute_hash_value(f"request-{i}")
            assert 0.0 <= hash_value < 1.0

    def test_router_compute_hash_value_deterministic(self, router):
        """Test hash value is deterministic."""
        value1 = router._compute_hash_value("same-request")
        value2 = router._compute_hash_value("same-request")

        assert value1 == value2

    def test_router_compute_hash_value_different_inputs(self, router):
        """Test different inputs produce different hash values."""
        value1 = router._compute_hash_value("request-a")
        value2 = router._compute_hash_value("request-b")

        # While collisions are possible, these specific strings should differ
        assert value1 != value2

    def test_router_set_champion_model(self, router, mock_sklearn_model):
        """Test setting champion model directly."""
        router.set_champion_model(mock_sklearn_model, version=5)

        assert router._champion_model is mock_sklearn_model
        assert router._champion_version == 5

    def test_router_set_candidate_model(self, router, mock_sklearn_model):
        """Test setting candidate model directly."""
        router.set_candidate_model(mock_sklearn_model, version=3)

        assert router._candidate_model is mock_sklearn_model
        assert router._candidate_version == 3

    def test_router_set_ab_test_active(self, router):
        """Test enabling/disabling A/B testing."""
        router.set_ab_test_active(False)
        assert router._ab_test_active is False

        router.set_ab_test_active(True)
        assert router._ab_test_active is True

    def test_router_ab_test_disabled_routes_to_champion(self, router):
        """Test all traffic goes to champion when A/B testing disabled."""
        router.set_ab_test_active(False)

        champion_count = 0
        for i in range(100):
            result = router.route(f"request-{i}")
            if result.cohort == CohortType.CHAMPION:
                champion_count += 1

        assert champion_count == 100

    def test_router_get_champion_metrics(self, router):
        """Test getting champion metrics."""
        metrics = router.get_champion_metrics()

        assert isinstance(metrics, CohortMetrics)
        assert metrics.cohort == CohortType.CHAMPION

    def test_router_get_candidate_metrics(self, router):
        """Test getting candidate metrics."""
        metrics = router.get_candidate_metrics()

        assert isinstance(metrics, CohortMetrics)
        assert metrics.cohort == CohortType.CANDIDATE

    def test_router_get_metrics_summary(self, router):
        """Test getting full metrics summary."""
        summary = router.get_metrics_summary()

        assert "ab_test_active" in summary
        assert "candidate_split" in summary
        assert "champion" in summary
        assert "candidate" in summary
        assert "champion_version" in summary
        assert "candidate_version" in summary
        assert "has_champion_model" in summary
        assert "has_candidate_model" in summary

    def test_router_record_outcome(self, router):
        """Test recording outcome for a routed request."""
        # First route a request
        result = router.route("test-outcome")

        # Record outcome
        success = router.record_outcome(
            request_id="test-outcome",
            predicted=1,
            actual=1,
        )

        assert success is True

    def test_router_record_outcome_unknown_request(self, router):
        """Test recording outcome for unknown request returns False."""
        success = router.record_outcome(
            request_id="unknown-request",
            predicted=1,
            actual=1,
        )

        assert success is False


class TestABTestRouterTrafficSplit:
    """
    Critical tests for traffic split verification.

    Per spec: Traffic split within ±2% variance over 1000 requests.
    """

    @pytest.fixture
    def router_10_percent_split(self):
        """Create router with 10% candidate split."""
        return ABTestRouter(candidate_split=0.1)

    @pytest.fixture
    def router_20_percent_split(self):
        """Create router with 20% candidate split."""
        return ABTestRouter(candidate_split=0.2)

    @pytest.fixture
    def router_50_percent_split(self):
        """Create router with 50% candidate split."""
        return ABTestRouter(candidate_split=0.5)

    def test_traffic_split_10_percent_within_tolerance(self, router_10_percent_split):
        """
        CRITICAL TEST: Verify 10% candidate split within ±2% variance.

        Expected: ~10% requests to candidate (80-120 out of 1000)
        Tolerance: ±2% means 8-12% acceptable (80-120 out of 1000)
        """
        distribution = router_10_percent_split.get_traffic_distribution(n_requests=1000)

        expected_candidate_split = 0.1
        actual_candidate_split = distribution["actual_candidate_split"]
        variance = distribution["variance"]

        # Verify within tolerance
        assert variance < 0.02, (
            f"Traffic split variance {variance:.4f} exceeds ±2% tolerance. "
            f"Expected ~{expected_candidate_split:.0%}, "
            f"got {actual_candidate_split:.2%}"
        )

        # Additional sanity checks
        assert distribution["n_requests"] == 1000
        assert distribution["champion_count"] + distribution["candidate_count"] == 1000
        assert distribution["within_tolerance"] is True

    def test_traffic_split_10_percent_extended(self, router_10_percent_split):
        """Test 10% split with more requests for statistical confidence."""
        distribution = router_10_percent_split.get_traffic_distribution(n_requests=5000)

        variance = distribution["variance"]

        # With more samples, variance should be even smaller
        assert variance < 0.015, f"Extended test variance {variance:.4f} too high"
        assert distribution["within_tolerance"] is True

    def test_traffic_split_20_percent_within_tolerance(self, router_20_percent_split):
        """Verify 20% candidate split within ±2% variance."""
        distribution = router_20_percent_split.get_traffic_distribution(n_requests=1000)

        variance = distribution["variance"]
        actual_split = distribution["actual_candidate_split"]

        assert variance < 0.02, (
            f"Traffic split variance {variance:.4f} exceeds ±2% tolerance for 20% split. "
            f"Got {actual_split:.2%}"
        )

    def test_traffic_split_50_percent_within_tolerance(self, router_50_percent_split):
        """Verify 50% candidate split within ±2% variance."""
        distribution = router_50_percent_split.get_traffic_distribution(n_requests=1000)

        variance = distribution["variance"]
        actual_split = distribution["actual_candidate_split"]

        assert variance < 0.02, (
            f"Traffic split variance {variance:.4f} exceeds ±2% tolerance for 50% split. "
            f"Got {actual_split:.2%}"
        )

    def test_traffic_distribution_structure(self, router_10_percent_split):
        """Test traffic distribution returns complete structure."""
        distribution = router_10_percent_split.get_traffic_distribution(n_requests=100)

        assert "n_requests" in distribution
        assert "champion_count" in distribution
        assert "candidate_count" in distribution
        assert "actual_champion_split" in distribution
        assert "actual_candidate_split" in distribution
        assert "expected_candidate_split" in distribution
        assert "variance" in distribution
        assert "within_tolerance" in distribution

    def test_traffic_split_reproducible(self, router_10_percent_split):
        """Test traffic split is reproducible with same request IDs."""
        dist1 = router_10_percent_split.get_traffic_distribution(n_requests=100)
        dist2 = router_10_percent_split.get_traffic_distribution(n_requests=100)

        # Same request IDs should produce same distribution
        assert dist1["champion_count"] == dist2["champion_count"]
        assert dist1["candidate_count"] == dist2["candidate_count"]

    def test_traffic_split_manual_verification(self):
        """Manually verify traffic split by routing individual requests."""
        router = ABTestRouter(candidate_split=0.1)

        champion_count = 0
        candidate_count = 0

        for i in range(1000):
            result = router.route(f"manual-test-{i}")
            if result.cohort == CohortType.CHAMPION:
                champion_count += 1
            else:
                candidate_count += 1

        actual_candidate_split = candidate_count / 1000
        variance = abs(actual_candidate_split - 0.1)

        assert variance < 0.02, (
            f"Manual traffic split variance {variance:.4f} exceeds ±2% tolerance. "
            f"Champion: {champion_count}, Candidate: {candidate_count}"
        )

    def test_different_request_ids_vary(self):
        """Test that different request patterns produce varying but consistent routing."""
        router = ABTestRouter(candidate_split=0.1)

        # Route requests with different ID patterns
        patterns = [
            [f"alpha-{i}" for i in range(1000)],
            [f"beta-{i}" for i in range(1000)],
            [f"gamma-{i}" for i in range(1000)],
        ]

        variances = []
        for pattern in patterns:
            candidate_count = sum(
                1 for req_id in pattern if router.route(req_id).cohort == CohortType.CANDIDATE
            )
            variance = abs(candidate_count / 1000 - 0.1)
            variances.append(variance)
            # Clean up tracking
            for req_id in pattern:
                router._request_cohorts.pop(req_id, None)

        # All patterns should be within tolerance
        for i, variance in enumerate(variances):
            assert variance < 0.02, f"Pattern {i} variance {variance:.4f} exceeds ±2% tolerance"


class TestABTestRouterComparison:
    """Tests for model comparison functionality."""

    @pytest.fixture
    def router_with_data(self):
        """Create router with sample data in both cohorts."""
        router = ABTestRouter(candidate_split=0.1, min_samples=50)

        # Simulate champion requests (90% accuracy)
        for i in range(100):
            router._champion_metrics.record_request(
                latency_ms=50.0 + i * 0.1,
                prediction=1 if i < 90 else 0,
                actual=1,
            )

        # Simulate candidate requests (85% accuracy)
        for i in range(100):
            router._candidate_metrics.record_request(
                latency_ms=45.0 + i * 0.1,
                prediction=1 if i < 85 else 0,
                actual=1,
            )

        return router

    def test_compare_metrics_insufficient_data(self):
        """Test comparison with insufficient data."""
        router = ABTestRouter(min_samples=1000)

        # Add just a few samples
        for _ in range(10):
            router._champion_metrics.record_request(latency_ms=50.0, prediction=1, actual=1)
            router._candidate_metrics.record_request(latency_ms=45.0, prediction=1, actual=1)

        comparison = router.compare_metrics()

        assert comparison.result == ComparisonResult.INSUFFICIENT_DATA
        assert comparison.is_significant is False
        assert "Need more samples" in comparison.recommendation

    def test_compare_metrics_champion_better(self, router_with_data):
        """Test comparison when champion is better."""
        comparison = router_with_data.compare_metrics()

        # Champion has 90% accuracy, candidate has 85%
        assert comparison.accuracy_delta == pytest.approx(-0.05, rel=0.01)
        assert comparison.champion_metrics.accuracy == pytest.approx(0.90, rel=0.01)
        assert comparison.candidate_metrics.accuracy == pytest.approx(0.85, rel=0.01)

    def test_compare_metrics_candidate_better(self):
        """Test comparison when candidate is better."""
        router = ABTestRouter(candidate_split=0.1, min_samples=50)

        # Champion: 80% accuracy
        for i in range(100):
            router._champion_metrics.record_request(
                latency_ms=50.0,
                prediction=1 if i < 80 else 0,
                actual=1,
            )

        # Candidate: 95% accuracy
        for i in range(100):
            router._candidate_metrics.record_request(
                latency_ms=45.0,
                prediction=1 if i < 95 else 0,
                actual=1,
            )

        comparison = router.compare_metrics()

        assert comparison.accuracy_delta > 0
        assert comparison.candidate_metrics.accuracy > comparison.champion_metrics.accuracy

    def test_compare_metrics_no_difference(self):
        """Test comparison with similar performance."""
        router = ABTestRouter(candidate_split=0.1, min_samples=50)

        # Both at ~90% accuracy
        for i in range(100):
            router._champion_metrics.record_request(
                latency_ms=50.0,
                prediction=1 if i < 90 else 0,
                actual=1,
            )
            router._candidate_metrics.record_request(
                latency_ms=50.0,
                prediction=1 if i < 90 else 0,
                actual=1,
            )

        comparison = router.compare_metrics()

        assert abs(comparison.accuracy_delta) < router.min_improvement


class TestABTestRouterPromotion:
    """Tests for model promotion functionality."""

    @pytest.fixture
    def promotable_router(self):
        """Create router where candidate is ready for promotion."""
        router = ABTestRouter(candidate_split=0.1, min_samples=50)

        # Mock version manager
        router._version_manager = MagicMock()
        router._version_manager.promote_candidate_to_champion = MagicMock(return_value=True)

        # Set models
        router._champion_model = MagicMock()
        router._champion_version = 1
        router._candidate_model = MagicMock()
        router._candidate_version = 2

        # Champion: 80% accuracy
        for i in range(200):
            router._champion_metrics.record_request(
                latency_ms=50.0,
                prediction=1 if i < 160 else 0,
                actual=1,
            )

        # Candidate: 95% accuracy (clearly better)
        for i in range(200):
            router._candidate_metrics.record_request(
                latency_ms=45.0,
                prediction=1 if i < 190 else 0,
                actual=1,
            )

        return router

    def test_promote_candidate_insufficient_data(self):
        """Test promotion blocked due to insufficient data."""
        router = ABTestRouter(min_samples=1000)

        result = router.promote_candidate()

        assert result.status == PromotionStatus.NOT_READY
        assert "Insufficient data" in result.error_message

    def test_promote_candidate_blocked_worse_performance(self):
        """Test promotion blocked when candidate is worse."""
        router = ABTestRouter(min_samples=50)

        # Champion: 95% accuracy
        for i in range(200):
            router._champion_metrics.record_request(
                latency_ms=50.0,
                prediction=1 if i < 190 else 0,
                actual=1,
            )

        # Candidate: 70% accuracy (worse)
        for i in range(200):
            router._candidate_metrics.record_request(
                latency_ms=45.0,
                prediction=1 if i < 140 else 0,
                actual=1,
            )

        result = router.promote_candidate()

        assert result.status == PromotionStatus.BLOCKED
        assert "not better" in result.error_message

    def test_promote_candidate_no_version_manager(self, promotable_router):
        """Test promotion fails without version manager."""
        promotable_router._version_manager = None

        result = promotable_router.promote_candidate()

        assert result.status == PromotionStatus.ERROR
        assert "Version manager not available" in result.error_message

    def test_promote_candidate_force(self):
        """Test forced promotion bypasses validation."""
        router = ABTestRouter(min_samples=1000)

        # Mock version manager
        router._version_manager = MagicMock()
        router._version_manager.promote_candidate_to_champion = MagicMock(return_value=True)
        router._candidate_model = MagicMock()
        router._candidate_version = 2

        result = router.promote_candidate(force=True)

        assert result.status == PromotionStatus.PROMOTED

    def test_promote_candidate_success(self, promotable_router):
        """Test successful promotion."""
        result = promotable_router.promote_candidate()

        assert result.status == PromotionStatus.PROMOTED
        assert result.previous_champion_version == 1
        assert result.new_champion_version == 2
        assert result.promoted_at is not None

        # Verify models swapped
        assert promotable_router._champion_version == 2
        assert promotable_router._candidate_model is None

    def test_reset_metrics_after_promotion(self, promotable_router):
        """Test metrics are reset after promotion."""
        # Record initial state
        initial_champion_requests = promotable_router._champion_metrics.request_count

        assert initial_champion_requests > 0

        promotable_router.promote_candidate()

        # Metrics should be reset
        assert promotable_router._champion_metrics.request_count == 0
        assert promotable_router._candidate_metrics.request_count == 0


class TestABTestRouterPrediction:
    """Tests for prediction functionality."""

    @pytest.fixture
    def router_with_models(self):
        """Create router with mock models."""
        router = ABTestRouter(candidate_split=0.1)

        # Create mock champion model
        champion = MagicMock()
        champion.predict = MagicMock(return_value=[1])
        champion.predict_proba = MagicMock(return_value=[[0.2, 0.8]])
        champion.classes_ = [0, 1]

        # Create mock candidate model
        candidate = MagicMock()
        candidate.predict = MagicMock(return_value=[0])
        candidate.predict_proba = MagicMock(return_value=[[0.9, 0.1]])
        candidate.classes_ = [0, 1]

        router.set_champion_model(champion, version=1)
        router.set_candidate_model(candidate, version=2)
        router._initialized = True

        return router

    def test_predict_with_routing(self, router_with_models):
        """Test prediction with routing result."""
        routing = router_with_models.route("test-predict")
        result = router_with_models.predict(routing, [1.0, 2.0, 3.0])

        assert result.request_id == "test-predict"
        assert result.cohort in [CohortType.CHAMPION, CohortType.CANDIDATE]
        assert result.latency_ms > 0
        assert result.error is None

    def test_route_and_predict_convenience(self, router_with_models):
        """Test route_and_predict convenience method."""
        result = router_with_models.route_and_predict(
            request_id="test-convenience",
            features=[1.0, 2.0, 3.0],
        )

        assert result.request_id == "test-convenience"
        assert result.prediction is not None
        assert result.latency_ms > 0

    def test_predict_records_latency_metrics(self, router_with_models):
        """Test prediction records latency to cohort metrics."""
        initial_champion = router_with_models._champion_metrics.request_count
        initial_candidate = router_with_models._candidate_metrics.request_count

        # Make several predictions
        for i in range(10):
            router_with_models.route_and_predict(f"latency-test-{i}", [1.0, 2.0])

        total_requests = (
            router_with_models._champion_metrics.request_count
            + router_with_models._candidate_metrics.request_count
        )
        initial_total = initial_champion + initial_candidate

        assert total_requests == initial_total + 10

    @pytest.mark.skipif(not NUMPY_AVAILABLE, reason="NumPy not installed")
    def test_predict_with_dict_features(self, router_with_models):
        """Test prediction with dictionary features."""
        result = router_with_models.route_and_predict(
            request_id="dict-features",
            features={"f1": 1.0, "f2": 2.0, "f3": 3.0},
        )

        assert result.prediction is not None

    def test_predict_no_model_returns_none_prediction(self):
        """Test prediction returns None when no model available."""
        router = ABTestRouter(candidate_split=0.1)
        router._initialized = True  # Skip model loading

        routing = router.route("no-model-test")
        result = router.predict(routing, [1.0, 2.0])

        assert result.prediction is None


class TestModuleLevelFunctions:
    """Tests for module-level convenience functions."""

    @pytest.fixture(autouse=True)
    def reset_global_router(self):
        """Reset the global A/B test router before each test."""
        import ab_testing

        ab_testing._ab_test_router = None
        yield
        ab_testing._ab_test_router = None

    def test_get_ab_test_router_creates_singleton(self):
        """Test get_ab_test_router creates a singleton."""
        router1 = get_ab_test_router()
        router2 = get_ab_test_router()

        assert router1 is router2

    def test_get_ab_test_router_with_custom_params(self):
        """Test get_ab_test_router with custom parameters."""
        router = get_ab_test_router(
            candidate_split=0.2,
            min_samples=500,
        )

        assert router.candidate_split == 0.2
        assert router.min_samples == 500

    def test_route_request_function(self):
        """Test module-level route_request function."""
        result = route_request("module-level-test")

        assert isinstance(result, RoutingResult)
        assert result.request_id == "module-level-test"

    def test_get_ab_test_metrics_function(self):
        """Test module-level get_ab_test_metrics function."""
        metrics = get_ab_test_metrics()

        assert "ab_test_active" in metrics
        assert "champion" in metrics
        assert "candidate" in metrics

    def test_compare_models_function(self):
        """Test module-level compare_models function."""
        comparison = compare_models()

        assert isinstance(comparison, MetricsComparison)
        assert comparison.result in list(ComparisonResult)

    def test_promote_candidate_model_function(self):
        """Test module-level promote_candidate_model function."""
        result = promote_candidate_model()

        assert isinstance(result, PromotionResult)
        # Should return NOT_READY or ERROR since we don't have setup
        assert result.status in [
            PromotionStatus.NOT_READY,
            PromotionStatus.ERROR,
        ]


class TestConfigurationConstants:
    """Tests for configuration constants."""

    def test_ab_test_split_from_env(self):
        """Test AB_TEST_SPLIT uses environment variable."""
        expected = float(os.getenv("AB_TEST_SPLIT", "0.1"))
        assert AB_TEST_SPLIT == expected

    def test_champion_alias_from_env(self):
        """Test CHAMPION_ALIAS uses environment variable."""
        expected = os.getenv("CHAMPION_ALIAS", "champion")
        assert CHAMPION_ALIAS == expected

    def test_candidate_alias_from_env(self):
        """Test CANDIDATE_ALIAS uses environment variable."""
        expected = os.getenv("CANDIDATE_ALIAS", "candidate")
        assert CANDIDATE_ALIAS == expected

    def test_ab_test_min_samples_from_env(self):
        """Test AB_TEST_MIN_SAMPLES uses environment variable."""
        expected = int(os.getenv("AB_TEST_MIN_SAMPLES", "1000"))
        assert AB_TEST_MIN_SAMPLES == expected

    def test_ab_test_confidence_level_from_env(self):
        """Test AB_TEST_CONFIDENCE_LEVEL uses environment variable."""
        expected = float(os.getenv("AB_TEST_CONFIDENCE_LEVEL", "0.95"))
        assert AB_TEST_CONFIDENCE_LEVEL == expected

    def test_ab_test_min_improvement_from_env(self):
        """Test AB_TEST_MIN_IMPROVEMENT uses environment variable."""
        expected = float(os.getenv("AB_TEST_MIN_IMPROVEMENT", "0.01"))
        assert AB_TEST_MIN_IMPROVEMENT == expected

    def test_model_registry_name_from_env(self):
        """Test MODEL_REGISTRY_NAME uses environment variable."""
        expected = os.getenv("MODEL_REGISTRY_NAME", "governance_impact_scorer")
        assert MODEL_REGISTRY_NAME == expected


class TestAvailabilityFlags:
    """Tests for availability flag exports."""

    def test_numpy_available_flag(self):
        """Test NUMPY_AVAILABLE flag is exported."""
        assert isinstance(NUMPY_AVAILABLE, bool)


class TestEdgeCases:
    """Tests for edge cases and error handling."""

    def test_router_with_zero_candidate_split(self):
        """Test router with 0% candidate split."""
        router = ABTestRouter(candidate_split=0.0)

        # All requests should go to champion
        for i in range(100):
            result = router.route(f"zero-split-{i}")
            assert result.cohort == CohortType.CHAMPION

    def test_router_with_full_candidate_split(self):
        """Test router with 100% candidate split."""
        router = ABTestRouter(candidate_split=1.0)

        # All requests should go to candidate
        for i in range(100):
            result = router.route(f"full-split-{i}")
            assert result.cohort == CohortType.CANDIDATE

    def test_cohort_metrics_very_high_latency(self):
        """Test metrics handle very high latency values."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)

        metrics.record_request(latency_ms=100000.0)  # 100 seconds

        assert metrics.max_latency_ms == 100000.0
        assert metrics.avg_latency_ms == 100000.0

    def test_cohort_metrics_very_low_latency(self):
        """Test metrics handle very low latency values."""
        metrics = CohortMetrics(cohort=CohortType.CHAMPION)

        metrics.record_request(latency_ms=0.001)  # 1 microsecond

        assert metrics.min_latency_ms == 0.001

    def test_router_prediction_error_handling(self):
        """Test router handles prediction errors gracefully."""
        router = ABTestRouter(candidate_split=0.1)
        router._initialized = True

        # Create model that raises error
        bad_model = MagicMock()
        bad_model.predict = MagicMock(side_effect=ValueError("Model failed"))

        router.set_champion_model(bad_model, version=1)

        routing = router.route("error-test")
        result = router.predict(routing, [1.0, 2.0])

        assert result.error is not None
        assert "Model failed" in result.error


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
